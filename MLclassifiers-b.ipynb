{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies\n",
    "import os\n",
    "import re\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import hamming_loss, confusion_matrix, multilabel_confusion_matrix, classification_report\n",
    "from sklearn.metrics import coverage_error, label_ranking_average_precision_score, label_ranking_loss\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n",
    "from skmultilearn.problem_transform import ClassifierChain, BinaryRelevance, LabelPowerset\n",
    "\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from yellowbrick.text import FreqDistVisualizer, TSNEVisualizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation PEP8\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametres graphiques\n",
    "%matplotlib inline\n",
    "rc = {\n",
    "    'font.size': 14,\n",
    "    'font.family': 'Arial',\n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'figure.max_open_warning': 30}\n",
    "\n",
    "sns.set(font='Arial', rc=rc)\n",
    "sns.set_style(\n",
    "    \"whitegrid\", {\n",
    "        'axes.edgecolor': 'k',\n",
    "        'axes.linewidth': 1,\n",
    "        'axes.grid': True,\n",
    "        'xtick.major.width': 1,\n",
    "        'ytick.major.width': 1\n",
    "        })\n",
    "sns.set_context(\n",
    "    \"notebook\",\n",
    "    font_scale=1.1,\n",
    "    rc={\"lines.linewidth\": 1.5})\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path\n",
    "data_path = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv(os.path.join(data_path, \"working_data_rameau_preprocessed_datav1.csv\"), converters={\"rameau_list_unstack\": eval}, index_col=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "def encoding(df, corpus, col_label):\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    # define X and y\n",
    "    X = df_encoded[corpus]\n",
    "    y = df_encoded[col_label]\n",
    "\n",
    "    # encode labels\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_encoded = mlb.fit_transform(y)\n",
    "    classes = mlb.classes_\n",
    "    return X, y_encoded, classes, mlb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Splitting pour multilabel\n",
    "def iterative_train_test_split_dataframe(df, corpus, col_label, test_size):\n",
    "\n",
    "    # encode labels\n",
    "    print(\"Encoding labels\")\n",
    "    X, y, classes, mlb = encoding(\n",
    "        df,\n",
    "        corpus=corpus,\n",
    "        col_label=col_label)\n",
    "    print(\"Labels encoded\")\n",
    "\n",
    "    # split data\n",
    "    print(\"splitting data\")\n",
    "    df_index = X.index.to_numpy().reshape(-1, 1)\n",
    "    df_index_train, y_train, df_index_test, y_test = iterative_train_test_split(\n",
    "        df_index, y, test_size=test_size)\n",
    "    print(\"Data splitted\")\n",
    "    print(\"Finalizing X_train and X_test\")\n",
    "    X_train = X.loc[df_index_train[:, 0]]\n",
    "    X_test = X.loc[df_index_test[:, 0]]\n",
    "    return (\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        df_index_train, df_index_test,\n",
    "        classes, mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, y_train, X_test, y_test, index_train, index_test, classes, mlb = iterative_train_test_split_dataframe(\n",
    "    df,\n",
    "    corpus=\"DESCR_processed\",\n",
    "    col_label=\"rameau_list_unstack\",\n",
    "    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check classes\n",
    "print(\"nombre de labels diff√©rents: \", len(classes))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of test and train datasets\n",
    "print(f\"train dataset size : {len(y_train)}\")\n",
    "print(f\"test dataset size : {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check splitting balance\n",
    "from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n",
    "order = 3\n",
    "X, y, classes, _ = encoding(\n",
    "    df,\n",
    "    corpus=\"DESCR_processed\",\n",
    "    col_label=\"rameau_list_unstack\")\n",
    "Counter(combination for row in get_combination_wise_output_matrix(y, order=order) for combination in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance following data split\n",
    "pd.DataFrame({\n",
    "    'train': Counter(\n",
    "        str(combination) for row in get_combination_wise_output_matrix(\n",
    "            y_train, order=order) for combination in row),\n",
    "    'test': Counter(\n",
    "        str(combination) for row in get_combination_wise_output_matrix(\n",
    "            y_test, order=order) for combination in row)\n",
    "}).T.fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize dataset (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDf vectorization\n",
    "def vectorizer_tfidf(\n",
    "    X_train, X_test, max_df, min_df,\n",
    "    max_features, n_gram=(1, 2),\n",
    "    save=True):\n",
    "    regex_pattern = r'\\w{3,}'\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=max_df,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        ngram_range=n_gram,\n",
    "        token_pattern=regex_pattern)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    if save:\n",
    "        pickle.dump(\n",
    "            vectorizer,\n",
    "            open(os.path.join(output_path, \"tfidf.pickle\"), \"wb\"))\n",
    "    return X_train, X_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for tf-idf\n",
    "max_df = 0.5\n",
    "min_df = 5\n",
    "max_features = 500\n",
    "n_gram = (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize corpus\n",
    "X_train_vect, X_test_vect, features = vectorizer_tfidf(\n",
    "    X_train, X_test, max_df, min_df,\n",
    "    max_features, n_gram=n_gram, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Word Frequency Distribution\n",
    "plt.figure(figsize=(20, 10))\n",
    "visualizer = FreqDistVisualizer(features=features, n=50, orient=\"v\")\n",
    "visualizer.fit(X_train_vect)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding TEF labels\n",
    "lab_encod = LabelEncoder()\n",
    "TEF_test_encoded = lab_encod.fit_transform(\n",
    "    df.loc[index_test[:, 0], 'TEF_LABEL'].values)\n",
    "TEF_train_encoded = lab_encod.fit_transform(\n",
    "    df.loc[index_train[:, 0], 'TEF_LABEL'].values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset with T-SNE\n",
    "tsne = TSNEVisualizer()\n",
    "tsne.fit(X_test_vect, TEF_test_encoded)\n",
    "tsne.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "def save_model_pkl(model, modelname):\n",
    "    joblib.dump(model, os.path.join(output_path, modelname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of transformed classifier for multi-label purpose\n",
    "def classification_model(X_train, y_train, algo, transf=\"\", save=True):\n",
    "    if algo == 'lr':\n",
    "        model = LogisticRegression(\n",
    "            solver='saga',\n",
    "            max_iter=500,\n",
    "            class_weight='balanced')\n",
    "        require_dense = [False, True]\n",
    "    elif algo == 'svc':\n",
    "        model = LinearSVC(\n",
    "            max_iter=500,\n",
    "            class_weight='balanced')\n",
    "        require_dense = [False, True]\n",
    "    elif algo == 'MultinomialNB':\n",
    "        model = MultinomialNB()\n",
    "        require_dense = [False, True]\n",
    "    elif algo == 'GaussianNB':\n",
    "        model = GaussianNB()\n",
    "        require_dense = [True, True]\n",
    "    elif algo == 'knn':\n",
    "        model = KNeighborsClassifier(n_neighbors=3)\n",
    "        require_dense = [False, False]\n",
    "    elif algo == 'MLkNN':\n",
    "        model = MLkNN(k=3)\n",
    "    elif algo == 'rf':\n",
    "        model = RandomForestClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight='balanced',\n",
    "            max_depth=20,\n",
    "            n_jobs=-1)\n",
    "        require_dense = [False, True]\n",
    "    elif algo == 'tree':\n",
    "        model = DecisionTreeClassifier()\n",
    "        require_dense = [False, True]\n",
    "    elif algo == \"bagging\":\n",
    "        model = BaggingClassifier(n_jobs=-1)\n",
    "        require_dense = [False, True]\n",
    "    elif algo == \"boosting\":\n",
    "        model = GradientBoostingClassifier()\n",
    "        require_dense = [False, True]\n",
    "    else:\n",
    "        print('The algo ' + algo + ' is not defined!')\n",
    "    if transf == 'BR':\n",
    "        clf = BinaryRelevance(model, require_dense=require_dense)\n",
    "    elif transf == 'CC':\n",
    "        clf = ClassifierChain(model, require_dense=require_dense)\n",
    "    elif transf == 'LP':\n",
    "        clf = LabelPowerset(model, require_dense=require_dense)\n",
    "    elif transf == 'OneVsRest':\n",
    "        clf = OneVsRestClassifier(model)\n",
    "    else:\n",
    "        clf = model\n",
    "    get_ipython().run_line_magic('time', 'clf.fit(X_train, y_train)  # Training the model on dataset')\n",
    "    if save:\n",
    "        save_model_pkl(clf, str(algo + \".pickle\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "def metricsReport(modelName, test_labels, predictions, print_metrics=False):\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    macro_precision = precision_score(test_labels, predictions, average='macro')\n",
    "    macro_recall = recall_score(test_labels, predictions, average='macro')\n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "    micro_precision = precision_score(test_labels, predictions, average='micro')\n",
    "    micro_recall = recall_score(test_labels, predictions, average='micro')\n",
    "    micro_f1 = f1_score(test_labels, predictions, average='micro')\n",
    "    hamLoss = hamming_loss(test_labels, predictions)\n",
    "    lrap = label_ranking_average_precision_score(test_labels, predictions.toarray())\n",
    "    lrl = label_ranking_loss(test_labels, predictions.toarray())\n",
    "    cov_error = coverage_error(test_labels, predictions.toarray())\n",
    "    EMR = np.all(test_labels == predictions, axis=1).mean()\n",
    "    if print_metrics:\n",
    "        # Print result\n",
    "        print(\"------\" + modelName + \" Model Metrics-----\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\\nHamming Loss: {hamLoss:.4f}\\nCoverage Error: {cov_error:.4f}\")\n",
    "        print(f\"Exact Match Ratio: {EMR:.4f}\\nRanking Loss: {lrl:.4f}\\nLabel Ranking avarge precision (LRAP): {lrap:.4f}\")\n",
    "        print(f\"Precision:\\n  - Macro: {macro_precision:.4f}\\n  - Micro: {micro_precision:.4f}\")\n",
    "        print(f\"Recall:\\n  - Macro: {macro_recall:.4f}\\n  - Micro: {micro_recall:.4f}\")\n",
    "        print(f\"F1-measure:\\n  - Macro: {macro_f1:.4f}\\n  - Micro: {micro_f1:.4f}\")\n",
    "    return {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Hamming Loss\": hamLoss,\n",
    "        \"Coverage Error\": cov_error,\n",
    "        \"Exact Match Ratio\": EMR,\n",
    "        \"Ranking Loss\": lrl,\n",
    "        \"Label Ranking avarge precision (LRAP)\": lrap,\n",
    "        \"Precision\": {\n",
    "            \"Macro\": macro_precision,\n",
    "            \"Micro\": micro_precision},\n",
    "        \"Recall\": {\n",
    "            \"Macro\": macro_recall,\n",
    "            \"Micro\": micro_recall},\n",
    "        \"F1-measure\": {\n",
    "            \"Macro\": macro_f1,\n",
    "            \"Micro\": micro_f1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and transformations\n",
    "model_list = [\n",
    "    'svc', 'MultinomialNB', 'lr',\n",
    "    'knn', 'GaussianNB', 'tree', 'rf',\n",
    "    'bagging', 'boosting']\n",
    "transf_list = [\"LP\"]\n",
    "ModelsPerformance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "for model in model_list:\n",
    "    for transf in transf_list:\n",
    "        print(f\"Treating model: {model} adapted with {transf}\")\n",
    "        model_name = str(model + \"_\" + transf)\n",
    "        clf = classification_model(\n",
    "            X_train=X_train_vect,\n",
    "            y_train=y_train,\n",
    "            algo=model,\n",
    "            transf=transf,\n",
    "            save=False)\n",
    "        print(\"model fitted\")\n",
    "        predictions = clf.predict(X_test_vect)\n",
    "        print(\"predictions done\")\n",
    "        print(\"Computing metrics\")\n",
    "        ModelsPerformance[model_name] = metricsReport(model, y_test, predictions, print_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show performances of a specific model\n",
    "model_name = \"svc_LP\"\n",
    "ModelsPerformance[model_name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models adapted to multi-label classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLKNN\n",
    "model = \"MLkNN\"\n",
    "clf = classification_model(\n",
    "        X_train=X_train_vect,\n",
    "        y_train=y_train,\n",
    "        algo=model,\n",
    "        save=False)\n",
    "predictions = clf.predict(X_test_vect)\n",
    "ModelsPerformance[model] = metricsReport(model, y_test, predictions, print_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEKA\n",
    "from skmultilearn.ext import download_meka\n",
    "from skmultilearn.ext import Meka\n",
    "\n",
    "meka_classpath = download_meka()\n",
    "meka_classpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = \"Meka\"\n",
    "meka = Meka(\n",
    "        meka_classifier = \"meka.classifiers.multilabel.BR\", # Binary Relevance\n",
    "        weka_classifier = \"weka.classifiers.bayes.NaiveBayesMultinomial\", # with Naive Bayes single-label classifier\n",
    "        meka_classpath = meka_classpath, #obtained via download_meka\n",
    "        java_command = '/usr/bin/java' # path to java executable\n",
    ")\n",
    "meka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and performance\n",
    "meka.fit(X_train, y_train)\n",
    "predictions = meka.predict(X_test)\n",
    "ModelsPerformance[model] = metricsReport(model, y_test, predictions, print_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Keras classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U skorch torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "model= \"neural_network\"\n",
    "nodes = 8\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = int(input_dim/nodes)\n",
    "output_dim = len(np.unique(y_train.rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "class MultiClassClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(MultiClassClassifierModule, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = F.relu(self.hidden(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.softmax(self.output(X), dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    MultiClassClassifierModule,\n",
    "    max_epochs=20,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict\n",
    "clf = LabelPowerset(classifier=net, require_dense=[True,True])\n",
    "clf.fit(X_train.astype(np.float32),y_train)\n",
    "y_pred = clf.predict(X_test.astype(np.float32))\n",
    "ModelsPerformance[model] = metricsReport(model, y_test, y_pred, print_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performances\n",
    "pd.DataFrame(ModelsPerformance).to_csv(os.path.join(output_path, \"ML_metrics_13042023.csv\"), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on subsamples for metrics understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on N samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of samples\n",
    "n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 20 samples\n",
    "X_test_vect_sample = X_test_vect[:n]\n",
    "y_test_sample = y_test[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate labels (vedettes)\n",
    "y_test_labels_sample = mlb.inverse_transform(y_test_sample)\n",
    "y_test_labels_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "ModelsPerformance = {}\n",
    "for model in model_list:\n",
    "    print(\"Treating model: \", model)\n",
    "    clf = classification_model(\n",
    "        X_train=X_train_vect,\n",
    "        y_train=y_train,\n",
    "        algo=model,\n",
    "        save=False)\n",
    "    predictions20 = clf.predict(X_test_vect_20)\n",
    "    ModelsPerformance[model] = metricsReport(model, y_test_20, predictions20, print_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_test\n",
    "index_test[:n].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refind raw data\n",
    "df.loc[index_test.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "pd.DataFrame([X_test[:n].values, y_test_labels_sample, labels])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

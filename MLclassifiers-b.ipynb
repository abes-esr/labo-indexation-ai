{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 21:06:38.529164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-17 21:06:39.228172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-05-17 21:06:39.832389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-17 21:06:39.832805: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "[nltk_data] Downloading package words to /home/aurelie/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aurelie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aurelie/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import librairies\n",
    "import os\n",
    "import re\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import hamming_loss, confusion_matrix, multilabel_confusion_matrix, classification_report\n",
    "from sklearn.metrics import coverage_error, label_ranking_average_precision_score, label_ranking_loss\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n",
    "from skmultilearn.problem_transform import ClassifierChain, BinaryRelevance, LabelPowerset\n",
    "\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from yellowbrick.text import FreqDistVisualizer, TSNEVisualizer\n",
    "\n",
    "from utils_text_processing import *\n",
    "from utils_visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametres graphiques\n",
    "%matplotlib inline\n",
    "rc = {\n",
    "    'font.size': 14,\n",
    "    'font.family': 'Arial',\n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'figure.max_open_warning': 30}\n",
    "\n",
    "sns.set(font='Arial', rc=rc)\n",
    "sns.set_style(\n",
    "    \"whitegrid\", {\n",
    "        'axes.edgecolor': 'k',\n",
    "        'axes.linewidth': 1,\n",
    "        'axes.grid': False,\n",
    "        'xtick.major.width': 1,\n",
    "        'ytick.major.width': 1\n",
    "        })\n",
    "sns.set_context(\n",
    "    \"notebook\",\n",
    "    font_scale=1.1,\n",
    "    rc={\"lines.linewidth\": 1.5})\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "path = \".\"\n",
    "os.chdir(path)\n",
    "data_path = path + \"/data\"\n",
    "output_path = path + \"/outputs\"\n",
    "fig_path = path + \"/figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "working_data_filename = \"working_data_sans_dewey_processed.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le Fichier de données contient 154508 lignes et  10 colonnes\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "df = pd.read_pickle(os.path.join(data_path, working_data_filename))\n",
    "print(f\"le Fichier de données contient {df.shape[0]} lignes et  {df.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "def encoding(df, corpus, col_label):\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    # define X and y\n",
    "    X = df_encoded[corpus]\n",
    "    y = df_encoded[col_label]\n",
    "\n",
    "    # encode labels\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_encoded = mlb.fit_transform(y)\n",
    "    classes = mlb.classes_\n",
    "    return X, y_encoded, classes, mlb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Splitting pour multilabel\n",
    "def iterative_train_test_split_dataframe(df, corpus, col_label, test_size):\n",
    "\n",
    "    # encode labels\n",
    "    print(\"Encoding labels\")\n",
    "    X, y, classes, mlb = encoding(\n",
    "        df,\n",
    "        corpus=corpus,\n",
    "        col_label=col_label)\n",
    "    print(\"Labels encoded\")\n",
    "\n",
    "    # split data\n",
    "    print(\"splitting data\")\n",
    "    df_index = X.index.to_numpy().reshape(-1, 1)\n",
    "    df_index_train, y_train, df_index_test, y_test = iterative_train_test_split(\n",
    "        df_index, y, test_size=test_size)\n",
    "    print(\"Data splitted\")\n",
    "    print(\"Finalizing X_train and X_test\")\n",
    "    X_train = X.loc[df_index_train[:, 0]]\n",
    "    X_test = X.loc[df_index_test[:, 0]]\n",
    "    return (\n",
    "        X_train, y_train,\n",
    "        X_test, y_test,\n",
    "        df_index_train, df_index_test,\n",
    "        classes, mlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding labels\n",
      "Labels encoded\n",
      "splitting data\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, y_train, X_test, y_test, index_train, index_test, classes, mlb = iterative_train_test_split_dataframe(\n",
    "    df,\n",
    "    corpus=\"DESCR_processed\",\n",
    "    col_label=\"rameau_concepts\",\n",
    "    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check classes\n",
    "print(\"nombre de labels différents: \", len(classes))\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of test and train datasets\n",
    "print(f\"train dataset size : {len(y_train)}\")\n",
    "print(f\"test dataset size : {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check splitting balance\n",
    "from skmultilearn.model_selection.measures import get_combination_wise_output_matrix\n",
    "order = 3\n",
    "X, y, classes, _ = encoding(\n",
    "    df,\n",
    "    corpus=\"DESCR_processed\",\n",
    "    col_label=\"rameau_list_unstack\")\n",
    "Counter(combination for row in get_combination_wise_output_matrix(y, order=order) for combination in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance following data split\n",
    "pd.DataFrame({\n",
    "    'train': Counter(\n",
    "        str(combination) for row in get_combination_wise_output_matrix(\n",
    "            y_train, order=order) for combination in row),\n",
    "    'test': Counter(\n",
    "        str(combination) for row in get_combination_wise_output_matrix(\n",
    "            y_test, order=order) for combination in row)\n",
    "}).T.fillna(0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize dataset (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDf vectorization\n",
    "def vectorizer_tfidf(\n",
    "    X_train, X_test, max_df, min_df,\n",
    "    max_features, n_gram=(1, 2),\n",
    "    save=True):\n",
    "    regex_pattern = r'\\w{3,}'\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_df=max_df,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        ngram_range=n_gram,\n",
    "        token_pattern=regex_pattern)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    if save:\n",
    "        pickle.dump(\n",
    "            vectorizer,\n",
    "            open(os.path.join(output_path, \"tfidf.pickle\"), \"wb\"))\n",
    "    return X_train, X_test, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for tf-idf\n",
    "max_df = 0.5\n",
    "min_df = 5\n",
    "max_features = 500\n",
    "n_gram = (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize corpus\n",
    "X_train_vect, X_test_vect, features = vectorizer_tfidf(\n",
    "    X_train, X_test, max_df, min_df,\n",
    "    max_features, n_gram=n_gram, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Word Frequency Distribution\n",
    "plt.figure(figsize=(20, 10))\n",
    "visualizer = FreqDistVisualizer(features=features, n=50, orient=\"v\")\n",
    "visualizer.fit(X_train_vect)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding TEF labels\n",
    "lab_encod = LabelEncoder()\n",
    "TEF_test_encoded = lab_encod.fit_transform(\n",
    "    df.loc[index_test[:, 0], 'TEF_LABEL'].values)\n",
    "TEF_train_encoded = lab_encod.fit_transform(\n",
    "    df.loc[index_train[:, 0], 'TEF_LABEL'].values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset with T-SNE\n",
    "tsne = TSNEVisualizer()\n",
    "tsne.fit(X_test_vect, TEF_test_encoded)\n",
    "tsne.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "def save_model_pkl(model, modelname):\n",
    "    joblib.dump(model, os.path.join(output_path, modelname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of transformed classifier for multi-label purpose\n",
    "def classification_model(X_train, y_train, algo, transf=\"\", save=True):\n",
    "    if algo == 'lr':\n",
    "        model = LogisticRegression(\n",
    "            solver='saga',\n",
    "            max_iter=500,\n",
    "            class_weight='balanced')\n",
    "        require_dense = [False, True]\n",
    "    elif algo == 'svc':\n",
    "        model = LinearSVC(\n",
    "            max_iter=500,\n",
    "            class_weight='balanced')\n",
    "        require_dense = [False, True]\n",
    "    elif algo == 'MultinomialNB':\n",
    "        model = MultinomialNB()\n",
    "        require_dense = [False, True]\n",
    "    elif algo == 'GaussianNB':\n",
    "        model = GaussianNB()\n",
    "        require_dense = [True, True]\n",
    "    elif algo == 'knn':\n",
    "        model = KNeighborsClassifier(n_neighbors=3)\n",
    "        require_dense = [False, False]\n",
    "    elif algo == 'MLkNN':\n",
    "        model = MLkNN(k=3)\n",
    "    elif algo == 'rf':\n",
    "        model = RandomForestClassifier(\n",
    "            random_state=RANDOM_STATE,\n",
    "            class_weight='balanced',\n",
    "            max_depth=20,\n",
    "            n_jobs=-1)\n",
    "        require_dense = [False, True]\n",
    "    elif algo == 'tree':\n",
    "        model = DecisionTreeClassifier()\n",
    "        require_dense = [False, True]\n",
    "    elif algo == \"bagging\":\n",
    "        model = BaggingClassifier(n_jobs=-1)\n",
    "        require_dense = [False, True]\n",
    "    elif algo == \"boosting\":\n",
    "        model = GradientBoostingClassifier()\n",
    "        require_dense = [False, True]\n",
    "    else:\n",
    "        print('The algo ' + algo + ' is not defined!')\n",
    "    if transf == 'BR':\n",
    "        clf = BinaryRelevance(model, require_dense=require_dense)\n",
    "    elif transf == 'CC':\n",
    "        clf = ClassifierChain(model, require_dense=require_dense)\n",
    "    elif transf == 'LP':\n",
    "        clf = LabelPowerset(model, require_dense=require_dense)\n",
    "    elif transf == 'OneVsRest':\n",
    "        clf = OneVsRestClassifier(model)\n",
    "    else:\n",
    "        clf = model\n",
    "    clf.fit(X_train, y_train)  # Training the model on dataset')\n",
    "    if save:\n",
    "        save_model_pkl(clf, str(algo + \".pickle\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models and transformations\n",
    "model_list = [\n",
    "    'svc', 'MultinomialNB', 'lr',\n",
    "    'knn', 'GaussianNB', 'tree', 'rf',\n",
    "    'bagging', 'boosting']\n",
    "transf_list = [\"LP\"]\n",
    "ModelsPerformance = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "for model in model_list:\n",
    "    for transf in transf_list:\n",
    "        print(f\"Treating model: {model} adapted with {transf}\")\n",
    "        model_name = str(model + \"_\" + transf)\n",
    "        clf = classification_model(\n",
    "            X_train=X_train_vect,\n",
    "            y_train=y_train,\n",
    "            algo=model,\n",
    "            transf=transf,\n",
    "            save=False)\n",
    "        print(\"model fitted\")\n",
    "        predictions = clf.predict(X_test_vect)\n",
    "        print(\"predictions done\")\n",
    "        print(\"Computing metrics\")\n",
    "        ModelsPerformance[model_name] = label_metrics_report(model, y_test, predictions, zero_division=\"warn\", print_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show performances of a specific model\n",
    "model_name = \"svc_LP\"\n",
    "ModelsPerformance[model_name]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models adapted to multi-label classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLKNN\n",
    "model = \"MLkNN\"\n",
    "clf = classification_model(\n",
    "        X_train=X_train_vect,\n",
    "        y_train=y_train,\n",
    "        algo=model,\n",
    "        save=False)\n",
    "predictions = clf.predict(X_test_vect)\n",
    "ModelsPerformance[model] = metricsReport(model, y_test, predictions, print_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEKA\n",
    "from skmultilearn.ext import download_meka\n",
    "from skmultilearn.ext import Meka\n",
    "\n",
    "meka_classpath = download_meka()\n",
    "meka_classpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = \"Meka\"\n",
    "meka = Meka(\n",
    "        meka_classifier = \"meka.classifiers.multilabel.BR\", # Binary Relevance\n",
    "        weka_classifier = \"weka.classifiers.bayes.NaiveBayesMultinomial\", # with Naive Bayes single-label classifier\n",
    "        meka_classpath = meka_classpath, #obtained via download_meka\n",
    "        java_command = '/usr/bin/java' # path to java executable\n",
    ")\n",
    "meka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and performance\n",
    "meka.fit(X_train, y_train)\n",
    "predictions = meka.predict(X_test)\n",
    "ModelsPerformance[model] = metricsReport(model, y_test, predictions, print_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Keras classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U skorch torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "model= \"neural_network\"\n",
    "nodes = 8\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = int(input_dim/nodes)\n",
    "output_dim = len(np.unique(y_train.rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "class MultiClassClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=input_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=output_dim,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(MultiClassClassifierModule, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = F.relu(self.hidden(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.softmax(self.output(X), dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetClassifier(\n",
    "    MultiClassClassifierModule,\n",
    "    max_epochs=20,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict\n",
    "clf = LabelPowerset(classifier=net, require_dense=[True,True])\n",
    "clf.fit(X_train.astype(np.float32),y_train)\n",
    "y_pred = clf.predict(X_test.astype(np.float32))\n",
    "ModelsPerformance[model] = metricsReport(model, y_test, y_pred, print_metrics=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ModelsPerformance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performances\n",
    "pd.DataFrame(ModelsPerformance).to_csv(os.path.join(output_path, \"ML_metrics_13042023.csv\"), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on subsamples for metrics understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on N samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of samples\n",
    "n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 20 samples\n",
    "X_test_vect_sample = X_test_vect[:n]\n",
    "y_test_sample = y_test[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate labels (vedettes)\n",
    "y_test_labels_sample = mlb.inverse_transform(y_test_sample)\n",
    "y_test_labels_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "ModelsPerformance = {}\n",
    "for model in model_list:\n",
    "    print(\"Treating model: \", model)\n",
    "    clf = classification_model(\n",
    "        X_train=X_train_vect,\n",
    "        y_train=y_train,\n",
    "        algo=model,\n",
    "        save=False)\n",
    "    predictions20 = clf.predict(X_test_vect_20)\n",
    "    ModelsPerformance[model] = metricsReport(model, y_test_20, predictions20, print_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_test\n",
    "index_test[:n].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refind raw data\n",
    "df.loc[index_test.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "pd.DataFrame([X_test[:n].values, y_test_labels_sample, labels])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

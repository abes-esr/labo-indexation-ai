{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import hamming_loss, confusion_matrix, multilabel_confusion_matrix, classification_report\n",
    "from sklearn.metrics import coverage_error, label_ranking_average_precision_score, label_ranking_loss\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "def label_metrics_report(modelName, y_true, y_pred, print_metrics=False):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    macro_precision = precision_score(y_true, y_pred, average='macro')\n",
    "    macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    micro_precision = precision_score(y_true, y_pred, average='micro')\n",
    "    micro_recall = recall_score(y_true, y_pred, average='micro')\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    hamLoss = hamming_loss(y_true, y_pred)\n",
    "    if print_metrics:\n",
    "        # Print result\n",
    "        print(\"------\" + modelName + \" Model Metrics-----\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\\nHamming Loss: {hamLoss:.4f}\")\n",
    "        print(f\"Precision:\\n  - Macro: {macro_precision:.4f}\\n  - Micro: {micro_precision:.4f}\")\n",
    "        print(f\"Recall:\\n  - Macro: {macro_recall:.4f}\\n  - Micro: {micro_recall:.4f}\")\n",
    "        print(f\"F1-measure:\\n  - Macro: {macro_f1:.4f}\\n  - Micro: {micro_f1:.4f}\")\n",
    "    return {\n",
    "        \"Hamming Loss\": hamLoss,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": {\n",
    "            \"Macro\": macro_precision,\n",
    "            \"Micro\": micro_precision},\n",
    "        \"Recall\": {\n",
    "            \"Macro\": macro_recall,\n",
    "            \"Micro\": micro_recall},\n",
    "        \"F1-measure\": {\n",
    "            \"Macro\": macro_f1,\n",
    "            \"Micro\": micro_f1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------random_sample Model Metrics-----\n",
      "Accuracy: 0.4400\n",
      "Hamming Loss: 0.5600\n",
      "Precision:\n",
      "  - Macro: 0.4380\n",
      "  - Micro: 0.4400\n",
      "Recall:\n",
      "  - Macro: 0.4383\n",
      "  - Micro: 0.4400\n",
      "F1-measure:\n",
      "  - Macro: 0.4380\n",
      "  - Micro: 0.4400\n"
     ]
    }
   ],
   "source": [
    "# Test use\n",
    "import numpy as np\n",
    "model_name = \"random_sample\"\n",
    "y_pred = np.random.randint(0, 2, 100)  # Binary data\n",
    "y_true = np.random.randint(0, 2, 100)  # Binary data\n",
    "\n",
    "report = label_metrics_report(model_name, y_true, y_pred, print_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "def notice_metrics_report(modelName, y_true, y_pred, print_metrics=False):   \n",
    "    lrap = label_ranking_average_precision_score(y_true, y_pred)\n",
    "    lrl = label_ranking_loss(y_true, y_pred)\n",
    "    cov_error = coverage_error(y_true, y_pred)\n",
    "    if print_metrics:\n",
    "        # Print result\n",
    "        print(\"------\" + modelName + \" Model Metrics-----\")\n",
    "        print(f\"Coverage Error: {cov_error:.4f}\")\n",
    "        print(f\"Ranking Loss: {lrl:.4f}\\nLabel Ranking avarge precision (LRAP): {lrap:.4f}\")\n",
    "    return {\n",
    "        \"Coverage Error\": cov_error,\n",
    "        \"Ranking Loss\": lrl,\n",
    "        \"Label Ranking avarge precision (LRAP)\": lrap,\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------random_sample Model Metrics-----\n",
      "Coverage Error: 2.8200\n",
      "Ranking Loss: 0.3708\n",
      "Label Ranking avarge precision (LRAP): 0.7625\n"
     ]
    }
   ],
   "source": [
    "# Simulate data\n",
    "y_true = [np.random.randint(0,2,4) for i in range(100)]\n",
    "y_pred= [np.random.uniform(0,1,4) for i in range(100)]\n",
    "report_prob = notice_metrics_report(model_name, y_true, y_pred, print_metrics=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abes_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Machine learning algorithms - No hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import librairies\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import hamming_loss\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "from utils_text_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"words\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nlp = spacy.load(\"fr_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "path = \".\"\n",
    "os.chdir(path)\n",
    "data_path = path + \"/data\"\n",
    "output_path = path + \"/outputs\"\n",
    "fig_path = path + \"/figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation PEP8\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres graphiques\n",
    "%matplotlib inline\n",
    "rc = {\n",
    "    'font.size': 14,\n",
    "    'font.family': 'Arial',\n",
    "    'axes.labelsize': 14,\n",
    "    'legend.fontsize': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'figure.max_open_warning': 30}\n",
    "\n",
    "sns.set(font='Arial', rc=rc)\n",
    "sns.set_style(\n",
    "    \"whitegrid\", {\n",
    "        'axes.edgecolor': 'k',\n",
    "        'axes.linewidth': 1,\n",
    "        'axes.grid': True,\n",
    "        'xtick.major.width': 1,\n",
    "        'ytick.major.width': 1\n",
    "        })\n",
    "sns.set_context(\n",
    "    \"notebook\",\n",
    "    font_scale=1.1,\n",
    "    rc={\"lines.linewidth\": 1.5})\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv(os.path.join(data_path, 'working_data_sans_dewey.csv'), index_col=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add words\n",
    "add_words = [\n",
    "        \"la\",\n",
    "        \"de\",\n",
    "        \"le\",\n",
    "        \"les\",\n",
    "        \"l\",\n",
    "        \"au\",\n",
    "        \"du\",\n",
    "        \"ouvrage\",\n",
    "        \"auteur\",\n",
    "        \"livre\",\n",
    "        \"quatrieme\",\n",
    "        \"couv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select sample of data\n",
    "n_sample = 50000\n",
    "df_sample = df.sample(n=n_sample)\n",
    "df_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preproces des résumés\n",
    "df_sample['DESCR_processed'] = df_sample['DESCR'].apply(\n",
    "    lambda x: preprocess_text(\n",
    "        x,\n",
    "        add_words=add_words,\n",
    "        numeric=False,\n",
    "        stopw=True,\n",
    "        stem=False,\n",
    "        lem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "y = df_sample[\"rameau_concepts\"]\n",
    "X = df_sample[\"DESCR_processed\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size\n",
    "print(f\"train dataset size : {len(y_train)}\")\n",
    "print(f\"test dataset size : {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the categorical labels to Multi Label Encodings\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(y_train)\n",
    "test_labels = mlb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorised_train_documents = vectorizer.fit_transform(X_train)\n",
    "vectorised_test_documents = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Word Frequency Distribution\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "plt.figure(figsize=(20, 10))\n",
    "features = vectorizer.get_feature_names_out()\n",
    "visualizer = FreqDistVisualizer(features=features, n=100, orient=\"v\")\n",
    "visualizer.fit(vectorised_train_documents)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dataset with T-SNE\n",
    "tsne = TSNEVisualizer()\n",
    "tsne.fit(vectorised_train_documents)\n",
    "tsne.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate Classifiers\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, hamming_loss, coverage_error, brier_score_loss,\n",
    "    label_ranking_average_precision_score, label_ranking_loss)\n",
    "\n",
    "\n",
    "ModelsPerformance = {}\n",
    "\n",
    "def metricsReport(modelName, test_labels, predictions, compare=\"macro_f1\"):\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    macro_precision = precision_score(test_labels, predictions, average='macro')\n",
    "    macro_recall = recall_score(test_labels, predictions, average='macro')\n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "\n",
    "    micro_precision = precision_score(test_labels, predictions, average='micro')\n",
    "    micro_recall = recall_score(test_labels, predictions, average='micro')\n",
    "    micro_f1 = f1_score(test_labels, predictions, average='micro')\n",
    "\n",
    "    hamLoss = hamming_loss(test_labels, predictions)\n",
    "    lrap = label_ranking_average_precision_score(test_labels, predictions)\n",
    "    lrl = label_ranking_loss(test_labels, predictions)\n",
    "    cov_error = coverage_error(test_labels, predictions)\n",
    "    EMR = np.all(test_labels == predictions, axis=1).mean()\n",
    "    #brier = brier_score_loss(test_labels, predictions)\n",
    "\n",
    "    # Print result\n",
    "    print(\"------\" + modelName + \" Model Metrics-----\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\\nHamming Loss: {hamLoss:.4f}\\nCoverage Error: {cov_error:.4f}\")\n",
    "    #print(f\"Brier Score: {brier:.4f}\")\n",
    "    print(f\"Exact Match Ratio: {EMR:.4f}\\nRanking Loss: {lrl:.4f}\\nLabel Ranking avarge precision (LRAP): {lrap:.4f}\")\n",
    "    print(f\"Precision:\\n  - Macro: {macro_precision:.4f}\\n  - Micro: {micro_precision:.4f}\")\n",
    "    print(f\"Recall:\\n  - Macro: {macro_recall:.4f}\\n  - Micro: {micro_recall:.4f}\")\n",
    "    print(f\"F1-measure:\\n  - Macro: {macro_f1:.4f}\\n  - Micro: {micro_f1:.4f}\")\n",
    "\n",
    "    # Store F1\n",
    "    ModelsPerformance[modelName] = eval(compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "knnClf = KNeighborsClassifier()\n",
    "\n",
    "knnClf.fit(vectorised_train_documents, train_labels)\n",
    "knnPredictions = knnClf.predict(vectorised_test_documents)\n",
    "metricsReport(\"knn\", test_labels, knnPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtClassifier = DecisionTreeClassifier()\n",
    "dtClassifier.fit(vectorised_train_documents, train_labels)\n",
    "dtPreds = dtClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Decision Tree\", test_labels, dtPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfClassifier = RandomForestClassifier(n_jobs=-1)\n",
    "rfClassifier.fit(vectorised_train_documents, train_labels)\n",
    "rfPreds = rfClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Random Forest\", test_labels, rfPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bagClassifier = OneVsRestClassifier(BaggingClassifier(n_jobs=-1))\n",
    "bagClassifier.fit(vectorised_train_documents, train_labels)\n",
    "bagPreds = bagClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Bagging\", test_labels, bagPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "boostClassifier = OneVsRestClassifier(GradientBoostingClassifier())\n",
    "boostClassifier.fit(vectorised_train_documents, train_labels)\n",
    "boostPreds = boostClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Boosting\", test_labels, boostPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifierf\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nbClassifier = OneVsRestClassifier(MultinomialNB())\n",
    "nbClassifier.fit(vectorised_train_documents, train_labels)\n",
    "nbPreds = nbClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"Multinomial NB\", test_labels, nbPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine (Linear SVC)\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svmClassifier = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\n",
    "svmClassifier.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "svmPreds = svmClassifier.predict(vectorised_test_documents)\n",
    "metricsReport(\"SVC Sq. Hinge Loss\", test_labels, svmPreds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary relevance\n",
    "from sklearn.svm import LinearSVC\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "\n",
    "BinaryRelSVC = BinaryRelevance(LinearSVC())\n",
    "BinaryRelSVC.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "BinaryRelSVCPreds = BinaryRelSVC.predict(vectorised_test_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label powerset\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "powerSetSVC = LabelPowerset(LinearSVC())\n",
    "powerSetSVC.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "powerSetSVCPreds = powerSetSVC.predict(vectorised_test_documents)\n",
    "metricsReport(\"Power Set SVC\", test_labels, powerSetSVCPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison on different models based on their Micro-F1 score\n",
    "print(\"  Model Name \" + \" \"*10 + \"| Micro-F1 Score\")\n",
    "print(\"-------------------------------------------\")\n",
    "for key, value in ModelsPerformance.items():\n",
    "    print(\"  \" + key, \" \"*(20-len(key)) + \"|\", value)\n",
    "    print(\"-------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abes_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "79d7ff32004ac4c5bc1812f118fca289ef6cc0cea24529fb05e42e57e2fccd5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

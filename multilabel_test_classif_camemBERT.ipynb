{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://camembert-model.fr/posts/tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://colab.research.google.com/drive/1ejBYmu0P5urzghoTTDB-GBUxpbUFX0Gz?usp=sharing#scrollTo=aN6M-KaApoRo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/theartificialguy/NLP-with-Deep-Learning/blob/master/BERT/Multi%20Label%20Text%20Classification%20using%20BERT%20PyTorch/bert_multilabel_pytorch_standard.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 16:21:54.203045: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 16:21:55.125491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/aurelie/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Wed Jul 12 16:21:56 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:0A:00.0 Off |                  Off |\n",
      "|  0%   43C    P8    20W / 450W |   1451MiB / 24564MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://curiousily.com/posts/multi-label-text-classification-with-bert-and-pytorch-lightning/\n",
    "https://www.youtube.com/watch?v=vNKIg8rXK6w&ab_channel=rupertai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Setup torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "path = \".\"\n",
    "os.chdir(path)\n",
    "data_path = path + \"/data\"\n",
    "output_path = path + \"/outputs\"\n",
    "fig_path = path + \"/figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des FutureWarnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:  (125220, 103022)\n",
      "Test dataset:  (29227, 103022)\n",
      "Validation dataset:  (100, 103022)\n"
     ]
    }
   ],
   "source": [
    "## load data (takes around 1min30s)\n",
    "df_train = pd.read_pickle(os.path.join(data_path, \"train_dataset_for_DL.pkl\")).reset_index(drop=True)\n",
    "print(\"Train dataset: \", df_train.shape)\n",
    "df_test = pd.read_pickle(os.path.join(data_path, \"test_dataset_for_DL.pkl\")).reset_index(drop=True)\n",
    "print(\"Test dataset: \", df_test.shape)\n",
    "df_valid100 = pd.read_pickle(os.path.join(data_path, \"valid100_dataset_for_DL.pkl\"))\n",
    "print(\"Validation dataset: \", df_valid100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125220 entries, 0 to 125219\n",
      "Columns: 103022 entries, !Xóõ (langue) to descr\n",
      "dtypes: Sparse[int64, 0](103021), object(1)\n",
      "memory usage: 4.8+ MB\n",
      "train dataset memory usage:  None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29227 entries, 0 to 29226\n",
      "Columns: 103022 entries, !Xóõ (langue) to descr\n",
      "dtypes: Sparse[int64, 0](103021), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "ntest dataset memory usage:  None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Columns: 103022 entries, !Xóõ (langue) to descr\n",
      "dtypes: Sparse[int64, 0](103021), object(1)\n",
      "memory usage: 4.1+ KB\n",
      "validation dataset memory usage:  None\n"
     ]
    }
   ],
   "source": [
    "# Check memory space\n",
    "print(\"train dataset memory usage: \", df_train.info())\n",
    "print()\n",
    "print(\"ntest dataset memory usage: \", df_test.info())\n",
    "print()\n",
    "print(\"validation dataset memory usage: \", df_valid100.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:  La bataille mondiale des matières premières Dans le débat sur un nouvel ordre économique international, les marchés mondiaux des matières premières constituent un enjeu de première importance. Ils conditionnent largement les moyens de financement du développement de pays pauvres et sont un des lieux stratégiques où se joue l'indépendance des pays. L'auteur analyse d'abord les mécanismes et les acteurs des marchés libres, mettant en lumière les limites du jeu libéral de l'offre et de la demande. Son examen des divers systèmes de régulation qui ont été expérimentés l'amènent ensuite à émettre de sérieuses réserves sur l'efficacité des stocks régulateurs. De même, les accords compensatoires (type prêts du FMI) se heurtent-ils à des difficultés théoriques et concrètes de mise en place. La régulation de l'offre n'a véritablement réussi que dans le cas du pétrole. Des solutions plus radicales existent en dehors d'un fonctionnement aménagé du marché : ouverture unilatérale des frontières pour certains produits ; indexation des prix, maîtrise des circuits de transformation et commercialisation. Toutes solutions qui supposent une modification profonde des règles des échanges internationaux. [4e de couv.]\n",
      "Concepts:  {'Matières premières': 1, 'Relations économiques internationales': 1}\n"
     ]
    }
   ],
   "source": [
    "# get one row\n",
    "row_id = 64\n",
    "label_cols = df_train.columns[:-1]\n",
    "sample_row = df_train.iloc[row_id]\n",
    "sample_descr = sample_row.descr\n",
    "sample_labels = sample_row[label_cols]\n",
    "\n",
    "print(\"Description: \", sample_descr)\n",
    "print(\"Concepts: \", sample_labels[sample_labels != 0].to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Sample datasets\\ndf_train = df_train.sample(n=10000, random_state=42)\\nprint(\"Train dataset: \", df_train.shape)\\ndf_test = df_test.sample(n=3000, random_state=42)\\nprint(\"Test dataset: \", df_test.shape) '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Sample datasets\n",
    "df_train = df_train.sample(n=10000, random_state=42)\n",
    "print(\"Train dataset: \", df_train.shape)\n",
    "df_test = df_test.sample(n=3000, random_state=42)\n",
    "print(\"Test dataset: \", df_test.shape) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Remove useless columns (labels not used in train dataset - Takes > 5min)\\ncols_to_remove = df_train.columns[df_train.sum() == 0]\\nlen(cols_to_remove) '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Remove useless columns (labels not used in train dataset - Takes > 5min)\n",
    "cols_to_remove = df_train.columns[df_train.sum() == 0]\n",
    "len(cols_to_remove) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # Remove useless columns\\ndf_train = df_train.drop(columns=cols_to_remove)\\nprint(\"Train dataset:\", df_train.shape)\\ndf_test = df_test.drop(columns=cols_to_remove)\\nprint(\"Test dataset:\", df_test.shape)\\ndf_valid100 = df_valid100.drop(columns=cols_to_remove)\\nprint(\"Validation dataset:\", df_valid100.shape)\\nlabel_cols = df_train.columns[:-1]\\nprint(\"Nombre de labels:\", len(label_cols)) '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Remove useless columns\n",
    "df_train = df_train.drop(columns=cols_to_remove)\n",
    "print(\"Train dataset:\", df_train.shape)\n",
    "df_test = df_test.drop(columns=cols_to_remove)\n",
    "print(\"Test dataset:\", df_test.shape)\n",
    "df_valid100 = df_valid100.drop(columns=cols_to_remove)\n",
    "print(\"Validation dataset:\", df_valid100.shape)\n",
    "label_cols = df_train.columns[:-1]\n",
    "print(\"Nombre de labels:\", len(label_cols)) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87654, 103022) (37566, 103022)\n"
     ]
    }
   ],
   "source": [
    "# Separate train dataset into train and validation sets for model \n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_val = train_test_split(df_train, test_size = 0.3)\n",
    "data_train = data_train.reset_index(drop=True)\n",
    "data_val = data_val.reset_index(drop=True)\n",
    "print(data_train.shape, data_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set number of labels\n",
    "n_labels = len(label_cols)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Tokenization for camembert\n",
    "from transformers import CamembertTokenizer, CamembertModel\n",
    "model_name = \"camembert-base\"\n",
    "tokenizer = CamembertTokenizer.from_pretrained(model_name)\n",
    "bert_model = CamembertModel.from_pretrained(model_name, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description:  N°1 en maths avec Mathador CM1, 9-10 ans Un cahier de vacances fondé sur le jeu de calcul Mathador consistant à retrouver un nombre en utilisant d'autres nombres imposés. Ce faisant, l'enfant est invité à travailler sur les ordres de grandeur et le sens des nombres ainsi qu'à résoudre des problèmes en utilisant les quatre opérations. Avec des dés à construire soi-même\n",
      "\n",
      "tokenization:  ['▁N', '°1', '▁en', '▁maths', '▁avec', '▁Math', 'ador', '▁CM', '1,', '▁9', '-10', '▁ans', '▁Un', '▁cahier', '▁de']\n",
      "\n",
      "Encodings:  [5, 278, 7169, 22, 17865, 42, 12456, 10395, 8026, 5845, 419, 6686, 134, 153, 8221]\n",
      "\n",
      "Recoding:  <s> N°1 en maths avec Mathador CM1, 9-10 ans Un cahier de vacances fondé sur le jeu de calcul Mathador consistant à retrouver un nombre en utilisant d'autres nombres imposés. Ce faisant, l'enfant est invité à travailler sur les ordres de grandeur et le sens des nombres ainsi qu'à résoudre des problèmes en utilisant les quatre opérations. Avec des dés à construire soi-même</s>\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "notice = data_train.iloc[5].descr\n",
    "print(\"description: \", notice)\n",
    "print(\"\\ntokenization: \", tokenizer.tokenize(notice)[:15])\n",
    "print(\"\\nEncodings: \", tokenizer.encode(notice)[:15])\n",
    "print(\"\\nRecoding: \", tokenizer.decode(tokenizer.encode(notice)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 197.8\n",
      "Max length: 1028\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find number of tokens\n",
    "import numpy as np\n",
    "descr_len = [len(tokenizer.encode(descr, max_length=1028))\n",
    "                          for descr in data_train[\"descr\"]]\n",
    "print(\"Average length: {:.1f}\".format(np.mean(descr_len)))\n",
    "print(\"Max length: {}\".format(max(descr_len)))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.histplot(descr_len, bins=150, kde=False)\n",
    "ax.set(xlabel='Number of tokens')\n",
    "\n",
    "# Finalize the plot\n",
    "sns.despine(bottom=True)\n",
    "plt.tight_layout(h_pad=2)\n",
    "\n",
    "# Saving plot\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(os.path.join(fig_path, \"number_of_tokens.png\"), dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "VALID_BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buid Dataset\n",
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_len=128, device=\"cuda\"):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.df.iloc[index]\n",
    "        descr = data_row.descr\n",
    "        title = \" \".join(descr.split())\n",
    "        self.targets = data_row[label_cols].values\n",
    "\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data_train, tokenizer, MAX_LEN)\n",
    "valid_dataset = CustomDataset(data_val, tokenizer, MAX_LEN)\n",
    "test_dataset = CustomDataset(df_test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([103021])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "# Check dataset\n",
    "sample = train_dataset[3]\n",
    "print(sample['targets'].shape)\n",
    "print(sample['input_ids'].shape)\n",
    "print(sample['attention_mask'].shape)\n",
    "print(sample['token_type_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataloaders\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    pin_memory_device=\"cuda\",\n",
    "    shuffle=True,\n",
    "    num_workers=12\n",
    ")\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    pin_memory_device=\"cuda\",\n",
    "    shuffle=False,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    pin_memory_device=\"cuda\",\n",
    "    shuffle=False,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that GPU is available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    \"\"\"\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    model: model that we want to load checkpoint parameters into       \n",
    "    optimizer: optimizer we defined in previous training\n",
    "    \"\"\"\n",
    "    # load check point\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    # initialize state_dict from checkpoint to model\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    # initialize optimizer from checkpoint to optimizer\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
    "    valid_loss_min = checkpoint['valid_loss_min']\n",
    "    # return model, optimizer, epoch value, min validation loss \n",
    "    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert_model): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): CamembertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=103021, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear = torch.nn.Linear(768, n_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial values\n",
    "val_targets=[]\n",
    "val_outputs=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model\n",
    "def train_model(n_epochs, training_loader, validation_loader, model, \n",
    "                optimizer, checkpoint_path, best_model_path, steps_per_epochs=100):\n",
    "   \n",
    "  # initialize tracker for minimum validation loss\n",
    "  valid_loss_min = np.Inf\n",
    "   \n",
    " \n",
    "  for epoch in range(1, n_epochs+1):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
    "    for batch_idx, data in enumerate(training_loader):\n",
    "        if batch_idx <= steps_per_epochs:\n",
    "          print('yyy epoch', batch_idx)\n",
    "          ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "          token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "          targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "          outputs = model(ids, mask, token_type_ids)\n",
    "          \n",
    "          optimizer.zero_grad()\n",
    "          loss = loss_fn(outputs, targets)\n",
    "          if batch_idx%5000==0:\n",
    "            print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
    "          \n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          print('before loss data in training', loss.item(), train_loss)\n",
    "          train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
    "          print('after loss data in training', loss.item(), train_loss)\n",
    "    \n",
    "        else:\n",
    "          break\n",
    "\n",
    "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
    "    \n",
    "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    " \n",
    "    model.eval()\n",
    "   \n",
    "    with torch.no_grad():\n",
    "      for batch_idx, data in enumerate(validation_loader, 0):\n",
    "            if batch_idx <= steps_per_epochs: \n",
    "              ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "              mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "              token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "              targets = data['targets'].to(device, dtype = torch.float)\n",
    "              outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "              loss = loss_fn(outputs, targets)\n",
    "              valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
    "              val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "              val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            else:\n",
    "               break\n",
    "\n",
    "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
    "      # calculate average losses\n",
    "      print('before cal avg train loss', train_loss)\n",
    "      train_loss = train_loss/len(training_loader)\n",
    "      valid_loss = valid_loss/len(validation_loader)\n",
    "      # print training/validation statistics \n",
    "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "      \n",
    "      # create checkpoint variable and add important data\n",
    "      checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'valid_loss_min': valid_loss,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "      }\n",
    "        \n",
    "        # save checkpoint\n",
    "      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "        \n",
    "      ## TODO: save the model if validation loss has decreased\n",
    "      if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "        # save checkpoint as best model\n",
    "        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defin paths\n",
    "ckpt_path = \"./runs/camemBert/curr_ckpt\"\n",
    "best_model_path = \"./runs/camemBert/best_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############# Epoch 1: Training Start   #############\n",
      "yyy epoch 0\n",
      "Epoch: 1, Training Loss:  0.6937242150306702\n",
      "before loss data in training 0.6937242150306702 0\n",
      "after loss data in training 0.6937242150306702 0.6937242150306702\n",
      "yyy epoch 1\n",
      "before loss data in training 0.6934462785720825 0.6937242150306702\n",
      "after loss data in training 0.6934462785720825 0.6935852468013763\n",
      "yyy epoch 2\n",
      "before loss data in training 0.6931669116020203 0.6935852468013763\n",
      "after loss data in training 0.6931669116020203 0.6934458017349243\n",
      "yyy epoch 3\n",
      "before loss data in training 0.6928974986076355 0.6934458017349243\n",
      "after loss data in training 0.6928974986076355 0.6933087259531021\n",
      "yyy epoch 4\n",
      "before loss data in training 0.6926557421684265 0.6933087259531021\n",
      "after loss data in training 0.6926557421684265 0.693178129196167\n",
      "yyy epoch 5\n",
      "before loss data in training 0.6923778653144836 0.693178129196167\n",
      "after loss data in training 0.6923778653144836 0.6930447518825531\n",
      "yyy epoch 6\n",
      "before loss data in training 0.6921454071998596 0.6930447518825531\n",
      "after loss data in training 0.6921454071998596 0.6929162740707397\n",
      "yyy epoch 7\n",
      "before loss data in training 0.6918813586235046 0.6929162740707397\n",
      "after loss data in training 0.6918813586235046 0.6927869096398354\n",
      "yyy epoch 8\n",
      "before loss data in training 0.6916259527206421 0.6927869096398354\n",
      "after loss data in training 0.6916259527206421 0.6926579144265916\n",
      "yyy epoch 9\n",
      "before loss data in training 0.6913700103759766 0.6926579144265916\n",
      "after loss data in training 0.6913700103759766 0.6925291240215301\n",
      "yyy epoch 10\n",
      "before loss data in training 0.6910677552223206 0.6925291240215301\n",
      "after loss data in training 0.6910677552223206 0.6923962723125111\n",
      "yyy epoch 11\n",
      "before loss data in training 0.6907464265823364 0.6923962723125111\n",
      "after loss data in training 0.6907464265823364 0.6922587851683298\n",
      "yyy epoch 12\n",
      "before loss data in training 0.6904144287109375 0.6922587851683298\n",
      "after loss data in training 0.6904144287109375 0.6921169115946842\n",
      "yyy epoch 13\n",
      "before loss data in training 0.6901296973228455 0.6921169115946842\n",
      "after loss data in training 0.6901296973228455 0.6919749677181244\n",
      "yyy epoch 14\n",
      "before loss data in training 0.6897702813148499 0.6919749677181244\n",
      "after loss data in training 0.6897702813148499 0.6918279886245727\n",
      "yyy epoch 15\n",
      "before loss data in training 0.6893687844276428 0.6918279886245727\n",
      "after loss data in training 0.6893687844276428 0.6916742883622646\n",
      "yyy epoch 16\n",
      "before loss data in training 0.6890128254890442 0.6916742883622646\n",
      "after loss data in training 0.6890128254890442 0.6915177317226634\n",
      "yyy epoch 17\n",
      "before loss data in training 0.6885810494422913 0.6915177317226634\n",
      "after loss data in training 0.6885810494422913 0.6913545827070872\n",
      "yyy epoch 18\n",
      "before loss data in training 0.688224196434021 0.6913545827070872\n",
      "after loss data in training 0.688224196434021 0.6911898255348206\n",
      "yyy epoch 19\n",
      "before loss data in training 0.6877423524856567 0.6911898255348206\n",
      "after loss data in training 0.6877423524856567 0.6910174518823624\n",
      "yyy epoch 20\n",
      "before loss data in training 0.6873325705528259 0.6910174518823624\n",
      "after loss data in training 0.6873325705528259 0.6908419813428607\n",
      "yyy epoch 21\n",
      "before loss data in training 0.6868432760238647 0.6908419813428607\n",
      "after loss data in training 0.6868432760238647 0.6906602220101791\n",
      "yyy epoch 22\n",
      "before loss data in training 0.6863356232643127 0.6906602220101791\n",
      "after loss data in training 0.6863356232643127 0.6904721959777501\n",
      "yyy epoch 23\n",
      "before loss data in training 0.6859235763549805 0.6904721959777501\n",
      "after loss data in training 0.6859235763549805 0.6902826701601347\n",
      "yyy epoch 24\n",
      "before loss data in training 0.6852644085884094 0.6902826701601347\n",
      "after loss data in training 0.6852644085884094 0.6900819396972656\n",
      "yyy epoch 25\n",
      "before loss data in training 0.6848958730697632 0.6900819396972656\n",
      "after loss data in training 0.6848958730697632 0.6898824755962079\n",
      "yyy epoch 26\n",
      "before loss data in training 0.6842546463012695 0.6898824755962079\n",
      "after loss data in training 0.6842546463012695 0.6896740374741731\n",
      "yyy epoch 27\n",
      "before loss data in training 0.6834444403648376 0.6896740374741731\n",
      "after loss data in training 0.6834444403648376 0.6894515518631255\n",
      "yyy epoch 28\n",
      "before loss data in training 0.6830160617828369 0.6894515518631255\n",
      "after loss data in training 0.6830160617828369 0.6892296384120811\n",
      "yyy epoch 29\n",
      "before loss data in training 0.6823623180389404 0.6892296384120811\n",
      "after loss data in training 0.6823623180389404 0.6890007277329764\n",
      "yyy epoch 30\n",
      "before loss data in training 0.6818157434463501 0.6890007277329764\n",
      "after loss data in training 0.6818157434463501 0.688768954046311\n",
      "yyy epoch 31\n",
      "before loss data in training 0.6810782551765442 0.688768954046311\n",
      "after loss data in training 0.6810782551765442 0.6885286197066308\n",
      "yyy epoch 32\n",
      "before loss data in training 0.6806640028953552 0.6885286197066308\n",
      "after loss data in training 0.6806640028953552 0.688290297985077\n",
      "yyy epoch 33\n",
      "before loss data in training 0.6796352863311768 0.688290297985077\n",
      "after loss data in training 0.6796352863311768 0.6880357388187859\n",
      "yyy epoch 34\n",
      "before loss data in training 0.6793261170387268 0.6880357388187859\n",
      "after loss data in training 0.6793261170387268 0.6877868924822128\n",
      "yyy epoch 35\n",
      "before loss data in training 0.6783525943756104 0.6877868924822128\n",
      "after loss data in training 0.6783525943756104 0.6875248286459184\n",
      "yyy epoch 36\n",
      "before loss data in training 0.6774400472640991 0.6875248286459184\n",
      "after loss data in training 0.6774400472640991 0.6872522669869503\n",
      "yyy epoch 37\n",
      "before loss data in training 0.676594614982605 0.6872522669869503\n",
      "after loss data in training 0.676594614982605 0.6869718024605201\n",
      "yyy epoch 38\n",
      "before loss data in training 0.6756458878517151 0.6869718024605201\n",
      "after loss data in training 0.6756458878517151 0.6866813943936277\n",
      "yyy epoch 39\n",
      "before loss data in training 0.6746344566345215 0.6866813943936277\n",
      "after loss data in training 0.6746344566345215 0.6863802209496501\n",
      "yyy epoch 40\n",
      "before loss data in training 0.6740555167198181 0.6863802209496501\n",
      "after loss data in training 0.6740555167198181 0.6860796184074591\n",
      "yyy epoch 41\n",
      "before loss data in training 0.6732047200202942 0.6860796184074591\n",
      "after loss data in training 0.6732047200202942 0.6857730732077647\n",
      "yyy epoch 42\n",
      "before loss data in training 0.6721951365470886 0.6857730732077647\n",
      "after loss data in training 0.6721951365470886 0.6854573072389117\n",
      "yyy epoch 43\n",
      "before loss data in training 0.671312153339386 0.6854573072389117\n",
      "after loss data in training 0.671312153339386 0.6851358264684679\n",
      "yyy epoch 44\n",
      "before loss data in training 0.6704990267753601 0.6851358264684679\n",
      "after loss data in training 0.6704990267753601 0.6848105642530655\n",
      "yyy epoch 45\n",
      "before loss data in training 0.6694004535675049 0.6848105642530655\n",
      "after loss data in training 0.6694004535675049 0.6844755618468576\n",
      "yyy epoch 46\n",
      "before loss data in training 0.6686859130859375 0.6844755618468576\n",
      "after loss data in training 0.6686859130859375 0.684139611873221\n",
      "yyy epoch 47\n",
      "before loss data in training 0.6675049066543579 0.684139611873221\n",
      "after loss data in training 0.6675049066543579 0.6837930555144948\n",
      "yyy epoch 48\n",
      "before loss data in training 0.6665281057357788 0.6837930555144948\n",
      "after loss data in training 0.6665281057357788 0.6834407096006434\n",
      "yyy epoch 49\n",
      "before loss data in training 0.6654138565063477 0.6834407096006434\n",
      "after loss data in training 0.6654138565063477 0.6830801725387575\n",
      "yyy epoch 50\n",
      "before loss data in training 0.6646090149879456 0.6830801725387575\n",
      "after loss data in training 0.6646090149879456 0.6827179929789376\n",
      "yyy epoch 51\n",
      "before loss data in training 0.6629830002784729 0.6827179929789376\n",
      "after loss data in training 0.6629830002784729 0.6823384738885441\n",
      "yyy epoch 52\n",
      "before loss data in training 0.6623916625976562 0.6823384738885441\n",
      "after loss data in training 0.6623916625976562 0.6819621189585273\n",
      "yyy epoch 53\n",
      "before loss data in training 0.6613250374794006 0.6819621189585273\n",
      "after loss data in training 0.6613250374794006 0.6815799507829879\n",
      "yyy epoch 54\n",
      "before loss data in training 0.6601582765579224 0.6815799507829879\n",
      "after loss data in training 0.6601582765579224 0.6811904657970776\n",
      "yyy epoch 55\n",
      "before loss data in training 0.6589106321334839 0.6811904657970776\n",
      "after loss data in training 0.6589106321334839 0.6807926116245134\n",
      "yyy epoch 56\n",
      "before loss data in training 0.6578218936920166 0.6807926116245134\n",
      "after loss data in training 0.6578218936920166 0.6803896165730661\n",
      "yyy epoch 57\n",
      "before loss data in training 0.6571528911590576 0.6803896165730661\n",
      "after loss data in training 0.6571528911590576 0.6799889833762728\n",
      "yyy epoch 58\n",
      "before loss data in training 0.6550534963607788 0.6799889833762728\n",
      "after loss data in training 0.6550534963607788 0.6795663480031288\n",
      "yyy epoch 59\n",
      "before loss data in training 0.6537793874740601 0.6795663480031288\n",
      "after loss data in training 0.6537793874740601 0.6791365653276443\n",
      "yyy epoch 60\n",
      "before loss data in training 0.652622401714325 0.6791365653276443\n",
      "after loss data in training 0.652622401714325 0.6787019069077539\n",
      "yyy epoch 61\n",
      "before loss data in training 0.6515955328941345 0.6787019069077539\n",
      "after loss data in training 0.6515955328941345 0.678264707326889\n",
      "yyy epoch 62\n",
      "before loss data in training 0.6495320200920105 0.678264707326889\n",
      "after loss data in training 0.6495320200920105 0.6778086329263354\n",
      "yyy epoch 63\n",
      "before loss data in training 0.6490915417671204 0.6778086329263354\n",
      "after loss data in training 0.6490915417671204 0.6773599283769727\n",
      "yyy epoch 64\n",
      "before loss data in training 0.648457944393158 0.6773599283769727\n",
      "after loss data in training 0.648457944393158 0.6769152824695294\n",
      "yyy epoch 65\n",
      "before loss data in training 0.6471163034439087 0.6769152824695294\n",
      "after loss data in training 0.6471163034439087 0.676463782787323\n",
      "yyy epoch 66\n",
      "before loss data in training 0.6449522376060486 0.676463782787323\n",
      "after loss data in training 0.6449522376060486 0.6759934612174532\n",
      "yyy epoch 67\n",
      "before loss data in training 0.6431191563606262 0.6759934612174532\n",
      "after loss data in training 0.6431191563606262 0.675510015557794\n",
      "yyy epoch 68\n",
      "before loss data in training 0.6425893902778625 0.675510015557794\n",
      "after loss data in training 0.6425893902778625 0.6750329050464906\n",
      "yyy epoch 69\n",
      "before loss data in training 0.6414344310760498 0.6750329050464906\n",
      "after loss data in training 0.6414344310760498 0.6745529268469129\n",
      "yyy epoch 70\n",
      "before loss data in training 0.640038788318634 0.6745529268469129\n",
      "after loss data in training 0.640038788318634 0.6740668122197541\n",
      "yyy epoch 71\n",
      "before loss data in training 0.6373180150985718 0.6740668122197541\n",
      "after loss data in training 0.6373180150985718 0.6735564122597376\n",
      "yyy epoch 72\n",
      "before loss data in training 0.637703537940979 0.6735564122597376\n",
      "after loss data in training 0.637703537940979 0.6730652769950971\n",
      "yyy epoch 73\n",
      "before loss data in training 0.6350346207618713 0.6730652769950971\n",
      "after loss data in training 0.6350346207618713 0.6725513492081616\n",
      "yyy epoch 74\n",
      "before loss data in training 0.6342200636863708 0.6725513492081616\n",
      "after loss data in training 0.6342200636863708 0.6720402654012044\n",
      "yyy epoch 75\n",
      "before loss data in training 0.6322358250617981 0.6720402654012044\n",
      "after loss data in training 0.6322358250617981 0.6715165227651596\n",
      "yyy epoch 76\n",
      "before loss data in training 0.6306580901145935 0.6715165227651596\n",
      "after loss data in training 0.6306580901145935 0.6709858937696977\n",
      "yyy epoch 77\n",
      "before loss data in training 0.6294317841529846 0.6709858937696977\n",
      "after loss data in training 0.6294317841529846 0.6704531487746117\n",
      "yyy epoch 78\n",
      "before loss data in training 0.6281110048294067 0.6704531487746117\n",
      "after loss data in training 0.6281110048294067 0.6699171722689762\n",
      "yyy epoch 79\n",
      "before loss data in training 0.6271499395370483 0.6699171722689762\n",
      "after loss data in training 0.6271499395370483 0.6693825818598271\n",
      "yyy epoch 80\n",
      "before loss data in training 0.6247739791870117 0.6693825818598271\n",
      "after loss data in training 0.6247739791870117 0.6688318583700393\n",
      "yyy epoch 81\n",
      "before loss data in training 0.6230830550193787 0.6688318583700393\n",
      "after loss data in training 0.6230830550193787 0.6682739461340557\n",
      "yyy epoch 82\n",
      "before loss data in training 0.6217007040977478 0.6682739461340557\n",
      "after loss data in training 0.6217007040977478 0.6677128227360278\n",
      "yyy epoch 83\n",
      "before loss data in training 0.6205236315727234 0.6677128227360278\n",
      "after loss data in training 0.6205236315727234 0.6671510466507504\n",
      "yyy epoch 84\n",
      "before loss data in training 0.619456946849823 0.6671510466507504\n",
      "after loss data in training 0.619456946849823 0.6665899395942689\n",
      "yyy epoch 85\n",
      "before loss data in training 0.61661297082901 0.6665899395942689\n",
      "after loss data in training 0.61661297082901 0.6660088120504869\n",
      "yyy epoch 86\n",
      "before loss data in training 0.6149854063987732 0.6660088120504869\n",
      "after loss data in training 0.6149854063987732 0.6654223361234557\n",
      "yyy epoch 87\n",
      "before loss data in training 0.6145995855331421 0.6654223361234557\n",
      "after loss data in training 0.6145995855331421 0.6648448048667476\n",
      "yyy epoch 88\n",
      "before loss data in training 0.6123614311218262 0.6648448048667476\n",
      "after loss data in training 0.6123614311218262 0.6642551040381529\n",
      "yyy epoch 89\n",
      "before loss data in training 0.6107329726219177 0.6642551040381529\n",
      "after loss data in training 0.6107329726219177 0.6636604136890837\n",
      "yyy epoch 90\n",
      "before loss data in training 0.6093151569366455 0.6636604136890837\n",
      "after loss data in training 0.6093151569366455 0.6630632130654305\n",
      "yyy epoch 91\n",
      "before loss data in training 0.6082302927970886 0.6630632130654305\n",
      "after loss data in training 0.6082302927970886 0.6624672030625137\n",
      "yyy epoch 92\n",
      "before loss data in training 0.605431318283081 0.6624672030625137\n",
      "after loss data in training 0.605431318283081 0.661853913978864\n",
      "yyy epoch 93\n",
      "before loss data in training 0.6045147180557251 0.661853913978864\n",
      "after loss data in training 0.6045147180557251 0.6612439225328731\n",
      "yyy epoch 94\n",
      "before loss data in training 0.6023920178413391 0.6612439225328731\n",
      "after loss data in training 0.6023920178413391 0.6606244287992781\n",
      "yyy epoch 95\n",
      "before loss data in training 0.6018221378326416 0.6606244287992781\n",
      "after loss data in training 0.6018221378326416 0.6600119049350422\n",
      "yyy epoch 96\n",
      "before loss data in training 0.5993137955665588 0.6600119049350422\n",
      "after loss data in training 0.5993137955665588 0.6593861512302125\n",
      "yyy epoch 97\n",
      "before loss data in training 0.5991234183311462 0.6593861512302125\n",
      "after loss data in training 0.5991234183311462 0.6587712253843037\n",
      "yyy epoch 98\n",
      "before loss data in training 0.5969690084457397 0.6587712253843037\n",
      "after loss data in training 0.5969690084457397 0.6581469605667424\n",
      "yyy epoch 99\n",
      "before loss data in training 0.5933725833892822 0.6581469605667424\n",
      "after loss data in training 0.5933725833892822 0.6574992167949678\n",
      "yyy epoch 100\n",
      "before loss data in training 0.5936829447746277 0.6574992167949678\n",
      "after loss data in training 0.5936829447746277 0.6568673725175387\n",
      "############# Epoch 1: Training End     #############\n",
      "############# Epoch 1: Validation Start   #############\n",
      "############# Epoch 1: Validation End     #############\n",
      "before cal avg train loss 0.6568673725175387\n",
      "Epoch: 1 \tAvgerage Training Loss: 0.000479 \tAverage Validation Loss: 0.000497\n",
      "Validation loss decreased (inf --> 0.000497).  Saving model ...\n",
      "############# Epoch 1  Done   #############\n",
      "\n",
      "############# Epoch 2: Training Start   #############\n",
      "yyy epoch 0\n",
      "Epoch: 2, Training Loss:  0.5913167595863342\n",
      "before loss data in training 0.5913167595863342 0\n",
      "after loss data in training 0.5913167595863342 0.5913167595863342\n",
      "yyy epoch 1\n",
      "before loss data in training 0.5903013348579407 0.5913167595863342\n",
      "after loss data in training 0.5903013348579407 0.5908090472221375\n",
      "yyy epoch 2\n",
      "before loss data in training 0.5889808535575867 0.5908090472221375\n",
      "after loss data in training 0.5889808535575867 0.5901996493339539\n",
      "yyy epoch 3\n",
      "before loss data in training 0.5861092805862427 0.5901996493339539\n",
      "after loss data in training 0.5861092805862427 0.5891770571470261\n",
      "yyy epoch 4\n",
      "before loss data in training 0.584271252155304 0.5891770571470261\n",
      "after loss data in training 0.584271252155304 0.5881958961486816\n",
      "yyy epoch 5\n",
      "before loss data in training 0.5824148654937744 0.5881958961486816\n",
      "after loss data in training 0.5824148654937744 0.5872323910395304\n",
      "yyy epoch 6\n",
      "before loss data in training 0.581275999546051 0.5872323910395304\n",
      "after loss data in training 0.581275999546051 0.5863814779690334\n",
      "yyy epoch 7\n",
      "before loss data in training 0.5802930593490601 0.5863814779690334\n",
      "after loss data in training 0.5802930593490601 0.5856204256415367\n",
      "yyy epoch 8\n",
      "before loss data in training 0.5777232050895691 0.5856204256415367\n",
      "after loss data in training 0.5777232050895691 0.584742956691318\n",
      "yyy epoch 9\n",
      "before loss data in training 0.5763065814971924 0.584742956691318\n",
      "after loss data in training 0.5763065814971924 0.5838993191719055\n",
      "yyy epoch 10\n",
      "before loss data in training 0.5742442011833191 0.5838993191719055\n",
      "after loss data in training 0.5742442011833191 0.5830215811729431\n",
      "yyy epoch 11\n",
      "before loss data in training 0.5717006325721741 0.5830215811729431\n",
      "after loss data in training 0.5717006325721741 0.5820781687895457\n",
      "yyy epoch 12\n",
      "before loss data in training 0.5717412233352661 0.5820781687895457\n",
      "after loss data in training 0.5717412233352661 0.5812830191392164\n",
      "yyy epoch 13\n",
      "before loss data in training 0.5703404545783997 0.5812830191392164\n",
      "after loss data in training 0.5703404545783997 0.5805014073848723\n",
      "yyy epoch 14\n",
      "before loss data in training 0.567462682723999 0.5805014073848723\n",
      "after loss data in training 0.567462682723999 0.5796321590741474\n",
      "yyy epoch 15\n",
      "before loss data in training 0.5650824904441833 0.5796321590741474\n",
      "after loss data in training 0.5650824904441833 0.5787228047847747\n",
      "yyy epoch 16\n",
      "before loss data in training 0.5634865760803223 0.5787228047847747\n",
      "after loss data in training 0.5634865760803223 0.5778265560374539\n",
      "yyy epoch 17\n",
      "before loss data in training 0.5628882646560669 0.5778265560374539\n",
      "after loss data in training 0.5628882646560669 0.5769966509607102\n",
      "yyy epoch 18\n",
      "before loss data in training 0.5606946349143982 0.5769966509607102\n",
      "after loss data in training 0.5606946349143982 0.5761386501161675\n",
      "yyy epoch 19\n",
      "before loss data in training 0.5584679841995239 0.5761386501161675\n",
      "after loss data in training 0.5584679841995239 0.5752551168203354\n",
      "yyy epoch 20\n",
      "before loss data in training 0.5569812655448914 0.5752551168203354\n",
      "after loss data in training 0.5569812655448914 0.5743849334262666\n",
      "yyy epoch 21\n",
      "before loss data in training 0.5549640655517578 0.5743849334262666\n",
      "after loss data in training 0.5549640655517578 0.573502166704698\n",
      "yyy epoch 22\n",
      "before loss data in training 0.551565945148468 0.573502166704698\n",
      "after loss data in training 0.551565945148468 0.5725484179413837\n",
      "yyy epoch 23\n",
      "before loss data in training 0.5525650382041931 0.5725484179413837\n",
      "after loss data in training 0.5525650382041931 0.5717157771190007\n",
      "yyy epoch 24\n",
      "before loss data in training 0.5497231483459473 0.5717157771190007\n",
      "after loss data in training 0.5497231483459473 0.5708360719680785\n",
      "yyy epoch 25\n",
      "before loss data in training 0.5471011996269226 0.5708360719680785\n",
      "after loss data in training 0.5471011996269226 0.5699231922626494\n",
      "yyy epoch 26\n",
      "before loss data in training 0.5450972318649292 0.5699231922626494\n",
      "after loss data in training 0.5450972318649292 0.569003712247919\n",
      "yyy epoch 27\n",
      "before loss data in training 0.5434828996658325 0.569003712247919\n",
      "after loss data in training 0.5434828996658325 0.5680922546557017\n",
      "yyy epoch 28\n",
      "before loss data in training 0.5435124635696411 0.5680922546557017\n",
      "after loss data in training 0.5435124635696411 0.5672446756527341\n",
      "yyy epoch 29\n",
      "before loss data in training 0.5416288375854492 0.5672446756527341\n",
      "after loss data in training 0.5416288375854492 0.5663908143838245\n",
      "yyy epoch 30\n",
      "before loss data in training 0.5388691425323486 0.5663908143838245\n",
      "after loss data in training 0.5388691425323486 0.565503018517648\n",
      "yyy epoch 31\n",
      "before loss data in training 0.5379675030708313 0.565503018517648\n",
      "after loss data in training 0.5379675030708313 0.5646425336599349\n",
      "yyy epoch 32\n",
      "before loss data in training 0.5355655550956726 0.5646425336599349\n",
      "after loss data in training 0.5355655550956726 0.5637614130973815\n",
      "yyy epoch 33\n",
      "before loss data in training 0.5324403047561646 0.5637614130973815\n",
      "after loss data in training 0.5324403047561646 0.5628402040285222\n",
      "yyy epoch 34\n",
      "before loss data in training 0.5314680337905884 0.5628402040285222\n",
      "after loss data in training 0.5314680337905884 0.5619438563074384\n",
      "yyy epoch 35\n",
      "before loss data in training 0.5312776565551758 0.5619438563074384\n",
      "after loss data in training 0.5312776565551758 0.561092017425431\n",
      "yyy epoch 36\n",
      "before loss data in training 0.5283516645431519 0.561092017425431\n",
      "after loss data in training 0.5283516645431519 0.5602071430232073\n",
      "yyy epoch 37\n",
      "before loss data in training 0.5267413258552551 0.5602071430232073\n",
      "after loss data in training 0.5267413258552551 0.5593264636240507\n",
      "yyy epoch 38\n",
      "before loss data in training 0.5240413546562195 0.5593264636240507\n",
      "after loss data in training 0.5240413546562195 0.5584217172402601\n",
      "yyy epoch 39\n",
      "before loss data in training 0.5235147476196289 0.5584217172402601\n",
      "after loss data in training 0.5235147476196289 0.5575490429997444\n",
      "yyy epoch 40\n",
      "before loss data in training 0.5216343998908997 0.5575490429997444\n",
      "after loss data in training 0.5216343998908997 0.5566730760946507\n",
      "yyy epoch 41\n",
      "before loss data in training 0.5185682773590088 0.5566730760946507\n",
      "after loss data in training 0.5185682773590088 0.5557658189818973\n",
      "yyy epoch 42\n",
      "before loss data in training 0.5160653591156006 0.5557658189818973\n",
      "after loss data in training 0.5160653591156006 0.5548425524733788\n",
      "yyy epoch 43\n",
      "before loss data in training 0.5149776935577393 0.5548425524733788\n",
      "after loss data in training 0.5149776935577393 0.5539365329525687\n",
      "yyy epoch 44\n",
      "before loss data in training 0.5136175751686096 0.5539365329525687\n",
      "after loss data in training 0.5136175751686096 0.5530405561129252\n",
      "yyy epoch 45\n",
      "before loss data in training 0.5115702152252197 0.5530405561129252\n",
      "after loss data in training 0.5115702152252197 0.5521390269631925\n",
      "yyy epoch 46\n",
      "before loss data in training 0.507806658744812 0.5521390269631925\n",
      "after loss data in training 0.507806658744812 0.5511957850862057\n",
      "yyy epoch 47\n",
      "before loss data in training 0.5067554116249084 0.5511957850862057\n",
      "after loss data in training 0.5067554116249084 0.5502699439724287\n",
      "yyy epoch 48\n",
      "before loss data in training 0.5059443712234497 0.5502699439724287\n",
      "after loss data in training 0.5059443712234497 0.5493653404469393\n",
      "yyy epoch 49\n",
      "before loss data in training 0.5047714114189148 0.5493653404469393\n",
      "after loss data in training 0.5047714114189148 0.5484734618663788\n",
      "yyy epoch 50\n",
      "before loss data in training 0.502788245677948 0.5484734618663788\n",
      "after loss data in training 0.502788245677948 0.5475776733136645\n",
      "yyy epoch 51\n",
      "before loss data in training 0.5009818077087402 0.5475776733136645\n",
      "after loss data in training 0.5009818077087402 0.5466815989751083\n",
      "yyy epoch 52\n",
      "before loss data in training 0.4968876242637634 0.5466815989751083\n",
      "after loss data in training 0.4968876242637634 0.5457420900182904\n",
      "yyy epoch 53\n",
      "before loss data in training 0.49759939312934875 0.5457420900182904\n",
      "after loss data in training 0.49759939312934875 0.5448505585944211\n",
      "yyy epoch 54\n",
      "before loss data in training 0.4948215186595917 0.5448505585944211\n",
      "after loss data in training 0.4948215186595917 0.5439409396865151\n",
      "yyy epoch 55\n",
      "before loss data in training 0.4929591715335846 0.5439409396865151\n",
      "after loss data in training 0.4929591715335846 0.5430305509694985\n",
      "yyy epoch 56\n",
      "before loss data in training 0.49155518412590027 0.5430305509694985\n",
      "after loss data in training 0.49155518412590027 0.5421274743582073\n",
      "yyy epoch 57\n",
      "before loss data in training 0.48775994777679443 0.5421274743582073\n",
      "after loss data in training 0.48775994777679443 0.5411901032102518\n",
      "yyy epoch 58\n",
      "before loss data in training 0.48482513427734375 0.5411901032102518\n",
      "after loss data in training 0.48482513427734375 0.5402347647537619\n",
      "yyy epoch 59\n",
      "before loss data in training 0.48526543378829956 0.5402347647537619\n",
      "after loss data in training 0.48526543378829956 0.5393186092376708\n",
      "yyy epoch 60\n",
      "before loss data in training 0.48249301314353943 0.5393186092376708\n",
      "after loss data in training 0.48249301314353943 0.5383870420885867\n",
      "yyy epoch 61\n",
      "before loss data in training 0.48057305812835693 0.5383870420885867\n",
      "after loss data in training 0.48057305812835693 0.537454558476325\n",
      "yyy epoch 62\n",
      "before loss data in training 0.4783931374549866 0.537454558476325\n",
      "after loss data in training 0.4783931374549866 0.5365170756029704\n",
      "yyy epoch 63\n",
      "before loss data in training 0.4774109721183777 0.5365170756029704\n",
      "after loss data in training 0.4774109721183777 0.5355935427360236\n",
      "yyy epoch 64\n",
      "before loss data in training 0.4766824543476105 0.5355935427360236\n",
      "after loss data in training 0.4766824543476105 0.5346872182992788\n",
      "yyy epoch 65\n",
      "before loss data in training 0.47160810232162476 0.5346872182992788\n",
      "after loss data in training 0.47160810232162476 0.5337314741177992\n",
      "yyy epoch 66\n",
      "before loss data in training 0.4717312157154083 0.5337314741177992\n",
      "after loss data in training 0.4717312157154083 0.5328060971267188\n",
      "yyy epoch 67\n",
      "before loss data in training 0.46990978717803955 0.5328060971267188\n",
      "after loss data in training 0.46990978717803955 0.5318811513921794\n",
      "yyy epoch 68\n",
      "before loss data in training 0.4680956304073334 0.5318811513921794\n",
      "after loss data in training 0.4680956304073334 0.5309567235518193\n",
      "yyy epoch 69\n",
      "before loss data in training 0.4663873016834259 0.5309567235518193\n",
      "after loss data in training 0.4663873016834259 0.5300343032394137\n",
      "yyy epoch 70\n",
      "before loss data in training 0.46218201518058777 0.5300343032394137\n",
      "after loss data in training 0.46218201518058777 0.5290786372104161\n",
      "yyy epoch 71\n",
      "before loss data in training 0.46115991473197937 0.5290786372104161\n",
      "after loss data in training 0.46115991473197937 0.5281353216204379\n",
      "yyy epoch 72\n",
      "before loss data in training 0.4599735140800476 0.5281353216204379\n",
      "after loss data in training 0.4599735140800476 0.5272015982294737\n",
      "yyy epoch 73\n",
      "before loss data in training 0.45667755603790283 0.5272015982294737\n",
      "after loss data in training 0.45667755603790283 0.5262485706322902\n",
      "yyy epoch 74\n",
      "before loss data in training 0.45664921402931213 0.5262485706322902\n",
      "after loss data in training 0.45664921402931213 0.5253205792109172\n",
      "yyy epoch 75\n",
      "before loss data in training 0.4548875391483307 0.5253205792109172\n",
      "after loss data in training 0.4548875391483307 0.5243938286837779\n",
      "yyy epoch 76\n",
      "before loss data in training 0.4512076675891876 0.5243938286837779\n",
      "after loss data in training 0.4512076675891876 0.5234433590591728\n",
      "yyy epoch 77\n",
      "before loss data in training 0.45117345452308655 0.5234433590591728\n",
      "after loss data in training 0.45117345452308655 0.5225168218215307\n",
      "yyy epoch 78\n",
      "before loss data in training 0.4477770924568176 0.5225168218215307\n",
      "after loss data in training 0.4477770924568176 0.5215707492979267\n",
      "yyy epoch 79\n",
      "before loss data in training 0.4460398554801941 0.5215707492979267\n",
      "after loss data in training 0.4460398554801941 0.520626613125205\n",
      "yyy epoch 80\n",
      "before loss data in training 0.44284355640411377 0.520626613125205\n",
      "after loss data in training 0.44284355640411377 0.5196663284743273\n",
      "yyy epoch 81\n",
      "before loss data in training 0.44246354699134827 0.5196663284743273\n",
      "after loss data in training 0.44246354699134827 0.518724831139169\n",
      "yyy epoch 82\n",
      "before loss data in training 0.43957898020744324 0.518724831139169\n",
      "after loss data in training 0.43957898020744324 0.5177712666701121\n",
      "yyy epoch 83\n",
      "before loss data in training 0.4380003809928894 0.5177712666701121\n",
      "after loss data in training 0.4380003809928894 0.5168216132691927\n",
      "yyy epoch 84\n",
      "before loss data in training 0.4356013238430023 0.5168216132691927\n",
      "after loss data in training 0.4356013238430023 0.515866080452414\n",
      "yyy epoch 85\n",
      "before loss data in training 0.43477925658226013 0.515866080452414\n",
      "after loss data in training 0.43477925658226013 0.5149232104074122\n",
      "yyy epoch 86\n",
      "before loss data in training 0.4305908977985382 0.5149232104074122\n",
      "after loss data in training 0.4305908977985382 0.5139538734808734\n",
      "yyy epoch 87\n",
      "before loss data in training 0.42976123094558716 0.5139538734808734\n",
      "after loss data in training 0.42976123094558716 0.5129971389066088\n",
      "yyy epoch 88\n",
      "before loss data in training 0.4294930398464203 0.5129971389066088\n",
      "after loss data in training 0.4294930398464203 0.5120588906025617\n",
      "yyy epoch 89\n",
      "before loss data in training 0.42622700333595276 0.5120588906025617\n",
      "after loss data in training 0.42622700333595276 0.511105202966266\n",
      "yyy epoch 90\n",
      "before loss data in training 0.4253641963005066 0.511105202966266\n",
      "after loss data in training 0.4253641963005066 0.5101629941018071\n",
      "yyy epoch 91\n",
      "before loss data in training 0.42263659834861755 0.5101629941018071\n",
      "after loss data in training 0.42263659834861755 0.5092116202349246\n",
      "yyy epoch 92\n",
      "before loss data in training 0.42036283016204834 0.5092116202349246\n",
      "after loss data in training 0.42036283016204834 0.5082562569008077\n",
      "yyy epoch 93\n",
      "before loss data in training 0.4182341992855072 0.5082562569008077\n",
      "after loss data in training 0.4182341992855072 0.5072985754368151\n",
      "yyy epoch 94\n",
      "before loss data in training 0.4185860753059387 0.5072985754368151\n",
      "after loss data in training 0.4185860753059387 0.5063647596459638\n",
      "yyy epoch 95\n",
      "before loss data in training 0.41450035572052 0.5063647596459638\n",
      "after loss data in training 0.41450035572052 0.5054078387717404\n",
      "yyy epoch 96\n",
      "before loss data in training 0.41140398383140564 0.5054078387717404\n",
      "after loss data in training 0.41140398383140564 0.504438726865139\n",
      "yyy epoch 97\n",
      "before loss data in training 0.40935420989990234 0.504438726865139\n",
      "after loss data in training 0.40935420989990234 0.5034684766920243\n",
      "yyy epoch 98\n",
      "before loss data in training 0.4095429480075836 0.5034684766920243\n",
      "after loss data in training 0.4095429480075836 0.5025197339780401\n",
      "yyy epoch 99\n",
      "before loss data in training 0.4043924808502197 0.5025197339780401\n",
      "after loss data in training 0.4043924808502197 0.5015384614467618\n",
      "yyy epoch 100\n",
      "before loss data in training 0.40507277846336365 0.5015384614467618\n",
      "after loss data in training 0.40507277846336365 0.500583355674649\n",
      "############# Epoch 2: Training End     #############\n",
      "############# Epoch 2: Validation Start   #############\n",
      "############# Epoch 2: Validation End     #############\n",
      "before cal avg train loss 0.500583355674649\n",
      "Epoch: 2 \tAvgerage Training Loss: 0.000365 \tAverage Validation Loss: 0.000335\n",
      "Validation loss decreased (0.000497 --> 0.000335).  Saving model ...\n",
      "############# Epoch 2  Done   #############\n",
      "\n",
      "############# Epoch 3: Training Start   #############\n",
      "yyy epoch 0\n",
      "Epoch: 3, Training Loss:  0.40268608927726746\n",
      "before loss data in training 0.40268608927726746 0\n",
      "after loss data in training 0.40268608927726746 0.40268608927726746\n",
      "yyy epoch 1\n",
      "before loss data in training 0.40096959471702576 0.40268608927726746\n",
      "after loss data in training 0.40096959471702576 0.4018278419971466\n",
      "yyy epoch 2\n",
      "before loss data in training 0.3992917537689209 0.4018278419971466\n",
      "after loss data in training 0.3992917537689209 0.4009824792544047\n",
      "yyy epoch 3\n",
      "before loss data in training 0.39768120646476746 0.4009824792544047\n",
      "after loss data in training 0.39768120646476746 0.4001571610569954\n",
      "yyy epoch 4\n",
      "before loss data in training 0.39569994807243347 0.4001571610569954\n",
      "after loss data in training 0.39569994807243347 0.399265718460083\n",
      "yyy epoch 5\n",
      "before loss data in training 0.3926830589771271 0.399265718460083\n",
      "after loss data in training 0.3926830589771271 0.398168608546257\n",
      "yyy epoch 6\n",
      "before loss data in training 0.38995161652565 0.398168608546257\n",
      "after loss data in training 0.38995161652565 0.3969947525433132\n",
      "yyy epoch 7\n",
      "before loss data in training 0.3897387981414795 0.3969947525433132\n",
      "after loss data in training 0.3897387981414795 0.39608775824308395\n",
      "yyy epoch 8\n",
      "before loss data in training 0.38675829768180847 0.39608775824308395\n",
      "after loss data in training 0.38675829768180847 0.39505115151405334\n",
      "yyy epoch 9\n",
      "before loss data in training 0.3856835961341858 0.39505115151405334\n",
      "after loss data in training 0.3856835961341858 0.39411439597606657\n",
      "yyy epoch 10\n",
      "before loss data in training 0.3827771246433258 0.39411439597606657\n",
      "after loss data in training 0.3827771246433258 0.3930837349458174\n",
      "yyy epoch 11\n",
      "before loss data in training 0.3826003074645996 0.3930837349458174\n",
      "after loss data in training 0.3826003074645996 0.39221011598904926\n",
      "yyy epoch 12\n",
      "before loss data in training 0.3794228136539459 0.39221011598904926\n",
      "after loss data in training 0.3794228136539459 0.39122647734788746\n",
      "yyy epoch 13\n",
      "before loss data in training 0.3783882260322571 0.39122647734788746\n",
      "after loss data in training 0.3783882260322571 0.390309459396771\n",
      "yyy epoch 14\n",
      "before loss data in training 0.37752556800842285 0.390309459396771\n",
      "after loss data in training 0.37752556800842285 0.3894571999708811\n",
      "yyy epoch 15\n",
      "before loss data in training 0.37540414929389954 0.3894571999708811\n",
      "after loss data in training 0.37540414929389954 0.38857888430356974\n",
      "yyy epoch 16\n",
      "before loss data in training 0.373554527759552 0.38857888430356974\n",
      "after loss data in training 0.373554527759552 0.38769509862450985\n",
      "yyy epoch 17\n",
      "before loss data in training 0.3700352609157562 0.38769509862450985\n",
      "after loss data in training 0.3700352609157562 0.3867139965295791\n",
      "yyy epoch 18\n",
      "before loss data in training 0.3675957918167114 0.3867139965295791\n",
      "after loss data in training 0.3675957918167114 0.3857077752289019\n",
      "yyy epoch 19\n",
      "before loss data in training 0.366406112909317 0.3857077752289019\n",
      "after loss data in training 0.366406112909317 0.3847426921129226\n",
      "yyy epoch 20\n",
      "before loss data in training 0.36311766505241394 0.3847426921129226\n",
      "after loss data in training 0.36311766505241394 0.38371292891956504\n",
      "yyy epoch 21\n",
      "before loss data in training 0.36330607533454895 0.38371292891956504\n",
      "after loss data in training 0.36330607533454895 0.3827853446657007\n",
      "yyy epoch 22\n",
      "before loss data in training 0.36114904284477234 0.3827853446657007\n",
      "after loss data in training 0.36114904284477234 0.3818446358908777\n",
      "yyy epoch 23\n",
      "before loss data in training 0.3592764139175415 0.3818446358908777\n",
      "after loss data in training 0.3592764139175415 0.38090429330865533\n",
      "yyy epoch 24\n",
      "before loss data in training 0.3566998839378357 0.38090429330865533\n",
      "after loss data in training 0.3566998839378357 0.37993611693382257\n",
      "yyy epoch 25\n",
      "before loss data in training 0.3573439419269562 0.37993611693382257\n",
      "after loss data in training 0.3573439419269562 0.3790671871258662\n",
      "yyy epoch 26\n",
      "before loss data in training 0.35473474860191345 0.3790671871258662\n",
      "after loss data in training 0.35473474860191345 0.37816598569905313\n",
      "yyy epoch 27\n",
      "before loss data in training 0.35197344422340393 0.37816598569905313\n",
      "after loss data in training 0.35197344422340393 0.3772305377892085\n",
      "yyy epoch 28\n",
      "before loss data in training 0.3503910005092621 0.3772305377892085\n",
      "after loss data in training 0.3503910005092621 0.3763050365036931\n",
      "yyy epoch 29\n",
      "before loss data in training 0.34882980585098267 0.3763050365036931\n",
      "after loss data in training 0.34882980585098267 0.3753891954819361\n",
      "yyy epoch 30\n",
      "before loss data in training 0.34734103083610535 0.3753891954819361\n",
      "after loss data in training 0.34734103083610535 0.37448441597723187\n",
      "yyy epoch 31\n",
      "before loss data in training 0.3424844741821289 0.37448441597723187\n",
      "after loss data in training 0.3424844741821289 0.3734844177961349\n",
      "yyy epoch 32\n",
      "before loss data in training 0.3429987132549286 0.3734844177961349\n",
      "after loss data in training 0.3429987132549286 0.37256060856761347\n",
      "yyy epoch 33\n",
      "before loss data in training 0.33932408690452576 0.37256060856761347\n",
      "after loss data in training 0.33932408690452576 0.37158306381281675\n",
      "yyy epoch 34\n",
      "before loss data in training 0.3374449908733368 0.37158306381281675\n",
      "after loss data in training 0.3374449908733368 0.3706076903002602\n",
      "yyy epoch 35\n",
      "before loss data in training 0.33674824237823486 0.3706076903002602\n",
      "after loss data in training 0.33674824237823486 0.36966715008020395\n",
      "yyy epoch 36\n",
      "before loss data in training 0.3352909982204437 0.36966715008020395\n",
      "after loss data in training 0.3352909982204437 0.36873806489480504\n",
      "yyy epoch 37\n",
      "before loss data in training 0.33348166942596436 0.36873806489480504\n",
      "after loss data in training 0.33348166942596436 0.36781026501404607\n",
      "yyy epoch 38\n",
      "before loss data in training 0.3307255804538727 0.36781026501404607\n",
      "after loss data in training 0.3307255804538727 0.3668593756663493\n",
      "yyy epoch 39\n",
      "before loss data in training 0.32916900515556335 0.3668593756663493\n",
      "after loss data in training 0.32916900515556335 0.36591711640357966\n",
      "yyy epoch 40\n",
      "before loss data in training 0.3274044692516327 0.36591711640357966\n",
      "after loss data in training 0.3274044692516327 0.3649777835462151\n",
      "yyy epoch 41\n",
      "before loss data in training 0.32596319913864136 0.3649777835462151\n",
      "after loss data in training 0.32596319913864136 0.3640488648698443\n",
      "yyy epoch 42\n",
      "before loss data in training 0.3235616087913513 0.3640488648698443\n",
      "after loss data in training 0.3235616087913513 0.3631073007749956\n",
      "yyy epoch 43\n",
      "before loss data in training 0.3250070810317993 0.3631073007749956\n",
      "after loss data in training 0.3250070810317993 0.36224138668992295\n",
      "yyy epoch 44\n",
      "before loss data in training 0.3201736807823181 0.36224138668992295\n",
      "after loss data in training 0.3201736807823181 0.36130654878086504\n",
      "yyy epoch 45\n",
      "before loss data in training 0.3193264901638031 0.36130654878086504\n",
      "after loss data in training 0.3193264901638031 0.3603939388109289\n",
      "yyy epoch 46\n",
      "before loss data in training 0.31685447692871094 0.3603939388109289\n",
      "after loss data in training 0.31685447692871094 0.35946756728152\n",
      "yyy epoch 47\n",
      "before loss data in training 0.31490978598594666 0.35946756728152\n",
      "after loss data in training 0.31490978598594666 0.3585392801711956\n",
      "yyy epoch 48\n",
      "before loss data in training 0.3132583796977997 0.3585392801711956\n",
      "after loss data in training 0.3132583796977997 0.35761518016153443\n",
      "yyy epoch 49\n",
      "before loss data in training 0.3133007884025574 0.35761518016153443\n",
      "after loss data in training 0.3133007884025574 0.3567288923263549\n",
      "yyy epoch 50\n",
      "before loss data in training 0.3103953003883362 0.3567288923263549\n",
      "after loss data in training 0.3103953003883362 0.35582039052364867\n",
      "yyy epoch 51\n",
      "before loss data in training 0.3080320358276367 0.35582039052364867\n",
      "after loss data in training 0.3080320358276367 0.3549013837025715\n",
      "yyy epoch 52\n",
      "before loss data in training 0.30581119656562805 0.3549013837025715\n",
      "after loss data in training 0.30581119656562805 0.35397515375659144\n",
      "yyy epoch 53\n",
      "before loss data in training 0.3062942922115326 0.35397515375659144\n",
      "after loss data in training 0.3062942922115326 0.35309217483909033\n",
      "yyy epoch 54\n",
      "before loss data in training 0.3038110136985779 0.35309217483909033\n",
      "after loss data in training 0.3038110136985779 0.35219615372744467\n",
      "yyy epoch 55\n",
      "before loss data in training 0.3029663860797882 0.35219615372744467\n",
      "after loss data in training 0.3029663860797882 0.35131705073373654\n",
      "yyy epoch 56\n",
      "before loss data in training 0.3003352880477905 0.35131705073373654\n",
      "after loss data in training 0.3003352880477905 0.3504226338445094\n",
      "yyy epoch 57\n",
      "before loss data in training 0.30050623416900635 0.3504226338445094\n",
      "after loss data in training 0.30050623416900635 0.3495620062638973\n",
      "yyy epoch 58\n",
      "before loss data in training 0.2967674136161804 0.3495620062638973\n",
      "after loss data in training 0.2967674136161804 0.3486671826596987\n",
      "yyy epoch 59\n",
      "before loss data in training 0.29657647013664246 0.3486671826596987\n",
      "after loss data in training 0.29657647013664246 0.34779900411764775\n",
      "yyy epoch 60\n",
      "before loss data in training 0.2939905822277069 0.34779900411764775\n",
      "after loss data in training 0.2939905822277069 0.3469168988407635\n",
      "yyy epoch 61\n",
      "before loss data in training 0.29176631569862366 0.3469168988407635\n",
      "after loss data in training 0.29176631569862366 0.34602737330621286\n",
      "yyy epoch 62\n",
      "before loss data in training 0.2921903431415558 0.34602737330621286\n",
      "after loss data in training 0.2921903431415558 0.3451728172718532\n",
      "yyy epoch 63\n",
      "before loss data in training 0.28897786140441895 0.3451728172718532\n",
      "after loss data in training 0.28897786140441895 0.3442947710864246\n",
      "yyy epoch 64\n",
      "before loss data in training 0.28716224431991577 0.3442947710864246\n",
      "after loss data in training 0.28716224431991577 0.3434158091361706\n",
      "yyy epoch 65\n",
      "before loss data in training 0.28633853793144226 0.3434158091361706\n",
      "after loss data in training 0.28633853793144226 0.342551001996705\n",
      "yyy epoch 66\n",
      "before loss data in training 0.2843349874019623 0.342551001996705\n",
      "after loss data in training 0.2843349874019623 0.341682106256485\n",
      "yyy epoch 67\n",
      "before loss data in training 0.28220489621162415 0.341682106256485\n",
      "after loss data in training 0.28220489621162415 0.3408074414028841\n",
      "yyy epoch 68\n",
      "before loss data in training 0.28114789724349976 0.3408074414028841\n",
      "after loss data in training 0.28114789724349976 0.3399428103281104\n",
      "yyy epoch 69\n",
      "before loss data in training 0.27926141023635864 0.3399428103281104\n",
      "after loss data in training 0.27926141023635864 0.3390759331839425\n",
      "yyy epoch 70\n",
      "before loss data in training 0.2790801227092743 0.3390759331839425\n",
      "after loss data in training 0.2790801227092743 0.33823092176880637\n",
      "yyy epoch 71\n",
      "before loss data in training 0.2757957875728607 0.33823092176880637\n",
      "after loss data in training 0.2757957875728607 0.337363767127196\n",
      "yyy epoch 72\n",
      "before loss data in training 0.2742306888103485 0.337363767127196\n",
      "after loss data in training 0.2742306888103485 0.3364989304379241\n",
      "yyy epoch 73\n",
      "before loss data in training 0.273041695356369 0.3364989304379241\n",
      "after loss data in training 0.273041695356369 0.3356414002341193\n",
      "yyy epoch 74\n",
      "before loss data in training 0.2735857367515564 0.3356414002341193\n",
      "after loss data in training 0.2735857367515564 0.33481399138768514\n",
      "yyy epoch 75\n",
      "before loss data in training 0.2708221971988678 0.33481399138768514\n",
      "after loss data in training 0.2708221971988678 0.33397199409572703\n",
      "yyy epoch 76\n",
      "before loss data in training 0.270048052072525 0.33397199409572703\n",
      "after loss data in training 0.270048052072525 0.3331418130304906\n",
      "yyy epoch 77\n",
      "before loss data in training 0.2677777111530304 0.3331418130304906\n",
      "after loss data in training 0.2677777111530304 0.33230381172436935\n",
      "yyy epoch 78\n",
      "before loss data in training 0.2650664448738098 0.33230381172436935\n",
      "after loss data in training 0.2650664448738098 0.3314527058148686\n",
      "yyy epoch 79\n",
      "before loss data in training 0.26414698362350464 0.3314527058148686\n",
      "after loss data in training 0.26414698362350464 0.33061138428747655\n",
      "yyy epoch 80\n",
      "before loss data in training 0.2615770399570465 0.33061138428747655\n",
      "after loss data in training 0.2615770399570465 0.3297591084315453\n",
      "yyy epoch 81\n",
      "before loss data in training 0.26307857036590576 0.3297591084315453\n",
      "after loss data in training 0.26307857036590576 0.3289459311380619\n",
      "yyy epoch 82\n",
      "before loss data in training 0.2606929838657379 0.3289459311380619\n",
      "after loss data in training 0.2606929838657379 0.3281236064721303\n",
      "yyy epoch 83\n",
      "before loss data in training 0.25803396105766296 0.3281236064721303\n",
      "after loss data in training 0.25803396105766296 0.32728920593148186\n",
      "yyy epoch 84\n",
      "before loss data in training 0.256686270236969 0.32728920593148186\n",
      "after loss data in training 0.256686270236969 0.32645858315860526\n",
      "yyy epoch 85\n",
      "before loss data in training 0.2559940218925476 0.32645858315860526\n",
      "after loss data in training 0.2559940218925476 0.32563922779504645\n",
      "yyy epoch 86\n",
      "before loss data in training 0.2537441849708557 0.32563922779504645\n",
      "after loss data in training 0.2537441849708557 0.32481284799246957\n",
      "yyy epoch 87\n",
      "before loss data in training 0.25212082266807556 0.32481284799246957\n",
      "after loss data in training 0.25212082266807556 0.3239868022501469\n",
      "yyy epoch 88\n",
      "before loss data in training 0.25124025344848633 0.3239868022501469\n",
      "after loss data in training 0.25124025344848633 0.3231694252973193\n",
      "yyy epoch 89\n",
      "before loss data in training 0.25091424584388733 0.3231694252973193\n",
      "after loss data in training 0.25091424584388733 0.32236658997005896\n",
      "yyy epoch 90\n",
      "before loss data in training 0.2479843646287918 0.32236658997005896\n",
      "after loss data in training 0.2479843646287918 0.32154920287839667\n",
      "yyy epoch 91\n",
      "before loss data in training 0.24596209824085236 0.32154920287839667\n",
      "after loss data in training 0.24596209824085236 0.3207276039149451\n",
      "yyy epoch 92\n",
      "before loss data in training 0.24727313220500946 0.3207276039149451\n",
      "after loss data in training 0.24727313220500946 0.319937770885806\n",
      "yyy epoch 93\n",
      "before loss data in training 0.24306495487689972 0.319937770885806\n",
      "after loss data in training 0.24306495487689972 0.3191199749708176\n",
      "yyy epoch 94\n",
      "before loss data in training 0.2415235936641693 0.3191199749708176\n",
      "after loss data in training 0.2415235936641693 0.31830317095706345\n",
      "yyy epoch 95\n",
      "before loss data in training 0.24077966809272766 0.31830317095706345\n",
      "after loss data in training 0.24077966809272766 0.3174956344688933\n",
      "yyy epoch 96\n",
      "before loss data in training 0.23756973445415497 0.3174956344688933\n",
      "after loss data in training 0.23756973445415497 0.3166716561182259\n",
      "yyy epoch 97\n",
      "before loss data in training 0.23931358754634857 0.3166716561182259\n",
      "after loss data in training 0.23931358754634857 0.3158822880715741\n",
      "yyy epoch 98\n",
      "before loss data in training 0.23761747777462006 0.3158822880715741\n",
      "after loss data in training 0.23761747777462006 0.31509173443221095\n",
      "yyy epoch 99\n",
      "before loss data in training 0.23413099348545074 0.31509173443221095\n",
      "after loss data in training 0.23413099348545074 0.31428212702274333\n",
      "yyy epoch 100\n",
      "before loss data in training 0.2343989610671997 0.31428212702274333\n",
      "after loss data in training 0.2343989610671997 0.31349120458753993\n",
      "############# Epoch 3: Training End     #############\n",
      "############# Epoch 3: Validation Start   #############\n",
      "############# Epoch 3: Validation End     #############\n",
      "before cal avg train loss 0.31349120458753993\n",
      "Epoch: 3 \tAvgerage Training Loss: 0.000229 \tAverage Validation Loss: 0.000191\n",
      "Validation loss decreased (0.000335 --> 0.000191).  Saving model ...\n",
      "############# Epoch 3  Done   #############\n",
      "\n",
      "############# Epoch 4: Training Start   #############\n",
      "yyy epoch 0\n",
      "Epoch: 4, Training Loss:  0.23311543464660645\n",
      "before loss data in training 0.23311543464660645 0\n",
      "after loss data in training 0.23311543464660645 0.23311543464660645\n",
      "yyy epoch 1\n",
      "before loss data in training 0.2307177484035492 0.23311543464660645\n",
      "after loss data in training 0.2307177484035492 0.23191659152507782\n",
      "yyy epoch 2\n",
      "before loss data in training 0.23016245663166046 0.23191659152507782\n",
      "after loss data in training 0.23016245663166046 0.2313318798939387\n",
      "yyy epoch 3\n",
      "before loss data in training 0.22830408811569214 0.2313318798939387\n",
      "after loss data in training 0.22830408811569214 0.23057493194937706\n",
      "yyy epoch 4\n",
      "before loss data in training 0.22813479602336884 0.23057493194937706\n",
      "after loss data in training 0.22813479602336884 0.23008690476417543\n",
      "yyy epoch 5\n",
      "before loss data in training 0.2259666621685028 0.23008690476417543\n",
      "after loss data in training 0.2259666621685028 0.22940019766489667\n",
      "yyy epoch 6\n",
      "before loss data in training 0.22560815513134003 0.22940019766489667\n",
      "after loss data in training 0.22560815513134003 0.22885847730296\n",
      "yyy epoch 7\n",
      "before loss data in training 0.22407464683055878 0.22885847730296\n",
      "after loss data in training 0.22407464683055878 0.22826049849390984\n",
      "yyy epoch 8\n",
      "before loss data in training 0.22317510843276978 0.22826049849390984\n",
      "after loss data in training 0.22317510843276978 0.22769545515378317\n",
      "yyy epoch 9\n",
      "before loss data in training 0.220411479473114 0.22769545515378317\n",
      "after loss data in training 0.220411479473114 0.22696705758571625\n",
      "yyy epoch 10\n",
      "before loss data in training 0.21966706216335297 0.22696705758571625\n",
      "after loss data in training 0.21966706216335297 0.22630342163822867\n",
      "yyy epoch 11\n",
      "before loss data in training 0.21761061251163483 0.22630342163822867\n",
      "after loss data in training 0.21761061251163483 0.22557902087767917\n",
      "yyy epoch 12\n",
      "before loss data in training 0.21861690282821655 0.22557902087767917\n",
      "after loss data in training 0.21861690282821655 0.2250434733354128\n",
      "yyy epoch 13\n",
      "before loss data in training 0.21633581817150116 0.2250434733354128\n",
      "after loss data in training 0.21633581817150116 0.22442149796656197\n",
      "yyy epoch 14\n",
      "before loss data in training 0.21550381183624268 0.22442149796656197\n",
      "after loss data in training 0.21550381183624268 0.223826985557874\n",
      "yyy epoch 15\n",
      "before loss data in training 0.21348904073238373 0.223826985557874\n",
      "after loss data in training 0.21348904073238373 0.22318086400628087\n",
      "yyy epoch 16\n",
      "before loss data in training 0.21227170526981354 0.22318086400628087\n",
      "after loss data in training 0.21227170526981354 0.22253914878648867\n",
      "yyy epoch 17\n",
      "before loss data in training 0.2107376605272293 0.22253914878648867\n",
      "after loss data in training 0.2107376605272293 0.22188351054986316\n",
      "yyy epoch 18\n",
      "before loss data in training 0.2110743522644043 0.22188351054986316\n",
      "after loss data in training 0.2110743522644043 0.22131460748220744\n",
      "yyy epoch 19\n",
      "before loss data in training 0.20809823274612427 0.22131460748220744\n",
      "after loss data in training 0.20809823274612427 0.22065378874540328\n",
      "yyy epoch 20\n",
      "before loss data in training 0.2079828679561615 0.22065378874540328\n",
      "after loss data in training 0.2079828679561615 0.2200504115649632\n",
      "yyy epoch 21\n",
      "before loss data in training 0.2057245820760727 0.2200504115649632\n",
      "after loss data in training 0.2057245820760727 0.21939923749728635\n",
      "yyy epoch 22\n",
      "before loss data in training 0.20451177656650543 0.21939923749728635\n",
      "after loss data in training 0.20451177656650543 0.21875195658725238\n",
      "yyy epoch 23\n",
      "before loss data in training 0.20308639109134674 0.21875195658725238\n",
      "after loss data in training 0.20308639109134674 0.21809922469158965\n",
      "yyy epoch 24\n",
      "before loss data in training 0.20400232076644897 0.21809922469158965\n",
      "after loss data in training 0.20400232076644897 0.21753534853458403\n",
      "yyy epoch 25\n",
      "before loss data in training 0.20157667994499207 0.21753534853458403\n",
      "after loss data in training 0.20157667994499207 0.2169215535888305\n",
      "yyy epoch 26\n",
      "before loss data in training 0.20033733546733856 0.2169215535888305\n",
      "after loss data in training 0.20033733546733856 0.2163073232880345\n",
      "yyy epoch 27\n",
      "before loss data in training 0.20085875689983368 0.2163073232880345\n",
      "after loss data in training 0.20085875689983368 0.2157555887741702\n",
      "yyy epoch 28\n",
      "before loss data in training 0.1982303410768509 0.2157555887741702\n",
      "after loss data in training 0.1982303410768509 0.21515126988805575\n",
      "yyy epoch 29\n",
      "before loss data in training 0.19718965888023376 0.21515126988805575\n",
      "after loss data in training 0.19718965888023376 0.21455254952112834\n",
      "yyy epoch 30\n",
      "before loss data in training 0.19674983620643616 0.21455254952112834\n",
      "after loss data in training 0.19674983620643616 0.21397826844646087\n",
      "yyy epoch 31\n",
      "before loss data in training 0.19581866264343262 0.21397826844646087\n",
      "after loss data in training 0.19581866264343262 0.21341078076511624\n",
      "yyy epoch 32\n",
      "before loss data in training 0.1933082789182663 0.21341078076511624\n",
      "after loss data in training 0.1933082789182663 0.21280161404248443\n",
      "yyy epoch 33\n",
      "before loss data in training 0.1933603137731552 0.21280161404248443\n",
      "after loss data in training 0.1933603137731552 0.2122298110933865\n",
      "yyy epoch 34\n",
      "before loss data in training 0.19266700744628906 0.2122298110933865\n",
      "after loss data in training 0.19266700744628906 0.21167087384632657\n",
      "yyy epoch 35\n",
      "before loss data in training 0.19158457219600677 0.21167087384632657\n",
      "after loss data in training 0.19158457219600677 0.21111292102270657\n",
      "yyy epoch 36\n",
      "before loss data in training 0.1899707317352295 0.21111292102270657\n",
      "after loss data in training 0.1899707317352295 0.2105415105014234\n",
      "yyy epoch 37\n",
      "before loss data in training 0.18830253183841705 0.2105415105014234\n",
      "after loss data in training 0.18830253183841705 0.20995627422081797\n",
      "yyy epoch 38\n",
      "before loss data in training 0.18659524619579315 0.20995627422081797\n",
      "after loss data in training 0.18659524619579315 0.2093572735022276\n",
      "yyy epoch 39\n",
      "before loss data in training 0.18623389303684235 0.2093572735022276\n",
      "after loss data in training 0.18623389303684235 0.20877918899059297\n",
      "yyy epoch 40\n",
      "before loss data in training 0.18565142154693604 0.20877918899059297\n",
      "after loss data in training 0.18565142154693604 0.20821509710172328\n",
      "yyy epoch 41\n",
      "before loss data in training 0.18475249409675598 0.20821509710172328\n",
      "after loss data in training 0.18475249409675598 0.2076564636968431\n",
      "yyy epoch 42\n",
      "before loss data in training 0.18457598984241486 0.2076564636968431\n",
      "after loss data in training 0.18457598984241486 0.20711970849092617\n",
      "yyy epoch 43\n",
      "before loss data in training 0.18399806320667267 0.20711970849092617\n",
      "after loss data in training 0.18399806320667267 0.20659421655264767\n",
      "yyy epoch 44\n",
      "before loss data in training 0.18205010890960693 0.20659421655264767\n",
      "after loss data in training 0.18205010890960693 0.20604879193835787\n",
      "yyy epoch 45\n",
      "before loss data in training 0.18040050566196442 0.20604879193835787\n",
      "after loss data in training 0.18040050566196442 0.2054912204975667\n",
      "yyy epoch 46\n",
      "before loss data in training 0.178241565823555 0.2054912204975667\n",
      "after loss data in training 0.178241565823555 0.20491144061088562\n",
      "yyy epoch 47\n",
      "before loss data in training 0.1767827868461609 0.20491144061088562\n",
      "after loss data in training 0.1767827868461609 0.20432542699078718\n",
      "yyy epoch 48\n",
      "before loss data in training 0.17916974425315857 0.20432542699078718\n",
      "after loss data in training 0.17916974425315857 0.2038120457104274\n",
      "yyy epoch 49\n",
      "before loss data in training 0.17681683599948883 0.2038120457104274\n",
      "after loss data in training 0.17681683599948883 0.20327214151620865\n",
      "yyy epoch 50\n",
      "before loss data in training 0.17517225444316864 0.20327214151620865\n",
      "after loss data in training 0.17517225444316864 0.2027211633383059\n",
      "yyy epoch 51\n",
      "before loss data in training 0.17429031431674957 0.2027211633383059\n",
      "after loss data in training 0.17429031431674957 0.20217441624173751\n",
      "yyy epoch 52\n",
      "before loss data in training 0.17494873702526093 0.20217441624173751\n",
      "after loss data in training 0.17494873702526093 0.20166072418104927\n",
      "yyy epoch 53\n",
      "before loss data in training 0.17192189395427704 0.20166072418104927\n",
      "after loss data in training 0.17192189395427704 0.2011100051027757\n",
      "yyy epoch 54\n",
      "before loss data in training 0.17134669423103333 0.2011100051027757\n",
      "after loss data in training 0.17134669423103333 0.20056885399601676\n",
      "yyy epoch 55\n",
      "before loss data in training 0.17095080018043518 0.20056885399601676\n",
      "after loss data in training 0.17095080018043518 0.20003996017788137\n",
      "yyy epoch 56\n",
      "before loss data in training 0.1697711944580078 0.20003996017788137\n",
      "after loss data in training 0.1697711944580078 0.19950892920033972\n",
      "yyy epoch 57\n",
      "before loss data in training 0.16926246881484985 0.19950892920033972\n",
      "after loss data in training 0.16926246881484985 0.19898743850403816\n",
      "yyy epoch 58\n",
      "before loss data in training 0.1675301194190979 0.19898743850403816\n",
      "after loss data in training 0.1675301194190979 0.1984542636042934\n",
      "yyy epoch 59\n",
      "before loss data in training 0.16876813769340515 0.1984542636042934\n",
      "after loss data in training 0.16876813769340515 0.19795949483911193\n",
      "yyy epoch 60\n",
      "before loss data in training 0.16661472618579865 0.19795949483911193\n",
      "after loss data in training 0.16661472618579865 0.19744564617266416\n",
      "yyy epoch 61\n",
      "before loss data in training 0.16570048034191132 0.19744564617266416\n",
      "after loss data in training 0.16570048034191132 0.19693362736894235\n",
      "yyy epoch 62\n",
      "before loss data in training 0.16323405504226685 0.19693362736894235\n",
      "after loss data in training 0.16323405504226685 0.1963987135224872\n",
      "yyy epoch 63\n",
      "before loss data in training 0.16373887658119202 0.1963987135224872\n",
      "after loss data in training 0.16373887658119202 0.19588840357027945\n",
      "yyy epoch 64\n",
      "before loss data in training 0.16183359920978546 0.19588840357027945\n",
      "after loss data in training 0.16183359920978546 0.19536448350319494\n",
      "yyy epoch 65\n",
      "before loss data in training 0.16164396703243256 0.19536448350319494\n",
      "after loss data in training 0.16164396703243256 0.19485356658697126\n",
      "yyy epoch 66\n",
      "before loss data in training 0.16149559617042542 0.19485356658697126\n",
      "after loss data in training 0.16149559617042542 0.19435568643150042\n",
      "yyy epoch 67\n",
      "before loss data in training 0.15803159773349762 0.19435568643150042\n",
      "after loss data in training 0.15803159773349762 0.19382150865652978\n",
      "yyy epoch 68\n",
      "before loss data in training 0.15880636870861053 0.19382150865652978\n",
      "after loss data in training 0.15880636870861053 0.1933140428601831\n",
      "yyy epoch 69\n",
      "before loss data in training 0.15766414999961853 0.1933140428601831\n",
      "after loss data in training 0.15766414999961853 0.19280475867646077\n",
      "yyy epoch 70\n",
      "before loss data in training 0.1573396474123001 0.19280475867646077\n",
      "after loss data in training 0.1573396474123001 0.1923052500671064\n",
      "yyy epoch 71\n",
      "before loss data in training 0.15681950747966766 0.1923052500671064\n",
      "after loss data in training 0.15681950747966766 0.19181239253116975\n",
      "yyy epoch 72\n",
      "before loss data in training 0.15561281144618988 0.19181239253116975\n",
      "after loss data in training 0.15561281144618988 0.19131650785877277\n",
      "yyy epoch 73\n",
      "before loss data in training 0.15447823703289032 0.19131650785877277\n",
      "after loss data in training 0.15447823703289032 0.19081869338815274\n",
      "yyy epoch 74\n",
      "before loss data in training 0.15181788802146912 0.19081869338815274\n",
      "after loss data in training 0.15181788802146912 0.1902986826499303\n",
      "yyy epoch 75\n",
      "before loss data in training 0.15252099931240082 0.1902986826499303\n",
      "after loss data in training 0.15252099931240082 0.18980160786917333\n",
      "yyy epoch 76\n",
      "before loss data in training 0.15199893712997437 0.18980160786917333\n",
      "after loss data in training 0.15199893712997437 0.18931066409333958\n",
      "yyy epoch 77\n",
      "before loss data in training 0.15203779935836792 0.18931066409333958\n",
      "after loss data in training 0.15203779935836792 0.18883280685314763\n",
      "yyy epoch 78\n",
      "before loss data in training 0.1496962159872055 0.18883280685314763\n",
      "after loss data in training 0.1496962159872055 0.1883374069687686\n",
      "yyy epoch 79\n",
      "before loss data in training 0.14927533268928528 0.1883374069687686\n",
      "after loss data in training 0.14927533268928528 0.18784913104027506\n",
      "yyy epoch 80\n",
      "before loss data in training 0.14745593070983887 0.18784913104027506\n",
      "after loss data in training 0.14745593070983887 0.1873504495547141\n",
      "yyy epoch 81\n",
      "before loss data in training 0.14861851930618286 0.1873504495547141\n",
      "after loss data in training 0.14861851930618286 0.18687810894192713\n",
      "yyy epoch 82\n",
      "before loss data in training 0.14783811569213867 0.18687810894192713\n",
      "after loss data in training 0.14783811569213867 0.18640774757747186\n",
      "yyy epoch 83\n",
      "before loss data in training 0.14518046379089355 0.18640774757747186\n",
      "after loss data in training 0.14518046379089355 0.1859169465800126\n",
      "yyy epoch 84\n",
      "before loss data in training 0.14523722231388092 0.1859169465800126\n",
      "after loss data in training 0.14523722231388092 0.18543836158864635\n",
      "yyy epoch 85\n",
      "before loss data in training 0.143013596534729 0.18543836158864635\n",
      "after loss data in training 0.143013596534729 0.18494505036708916\n",
      "yyy epoch 86\n",
      "before loss data in training 0.14594769477844238 0.18494505036708916\n",
      "after loss data in training 0.14594769477844238 0.184496804900553\n",
      "yyy epoch 87\n",
      "before loss data in training 0.1430768519639969 0.184496804900553\n",
      "after loss data in training 0.1430768519639969 0.18402612361718304\n",
      "yyy epoch 88\n",
      "before loss data in training 0.14309227466583252 0.18402612361718304\n",
      "after loss data in training 0.14309227466583252 0.18356619273008923\n",
      "yyy epoch 89\n",
      "before loss data in training 0.14218655228614807 0.18356619273008923\n",
      "after loss data in training 0.14218655228614807 0.18310641894737878\n",
      "yyy epoch 90\n",
      "before loss data in training 0.14010769128799438 0.18310641894737878\n",
      "after loss data in training 0.14010769128799438 0.1826339054566163\n",
      "yyy epoch 91\n",
      "before loss data in training 0.14080065488815308 0.1826339054566163\n",
      "after loss data in training 0.14080065488815308 0.18217919621130693\n",
      "yyy epoch 92\n",
      "before loss data in training 0.14066796004772186 0.18217919621130693\n",
      "after loss data in training 0.14066796004772186 0.18173283883320387\n",
      "yyy epoch 93\n",
      "before loss data in training 0.13884185254573822 0.18173283883320387\n",
      "after loss data in training 0.13884185254573822 0.18127655174503934\n",
      "yyy epoch 94\n",
      "before loss data in training 0.1382722407579422 0.18127655174503934\n",
      "after loss data in training 0.1382722407579422 0.18082387478728043\n",
      "yyy epoch 95\n",
      "before loss data in training 0.13756610453128815 0.18082387478728043\n",
      "after loss data in training 0.13756610453128815 0.1803732730137805\n",
      "yyy epoch 96\n",
      "before loss data in training 0.13632941246032715 0.1803732730137805\n",
      "after loss data in training 0.13632941246032715 0.17991921259570368\n",
      "yyy epoch 97\n",
      "before loss data in training 0.1377754956483841 0.17991921259570368\n",
      "after loss data in training 0.1377754956483841 0.1794891746676698\n",
      "yyy epoch 98\n",
      "before loss data in training 0.1364404410123825 0.1794891746676698\n",
      "after loss data in training 0.1364404410123825 0.17905433897418205\n",
      "yyy epoch 99\n",
      "before loss data in training 0.13521946966648102 0.17905433897418205\n",
      "after loss data in training 0.13521946966648102 0.17861599028110503\n",
      "yyy epoch 100\n",
      "before loss data in training 0.1337813287973404 0.17861599028110503\n",
      "after loss data in training 0.1337813287973404 0.17817208274166182\n",
      "############# Epoch 4: Training End     #############\n",
      "############# Epoch 4: Validation Start   #############\n",
      "############# Epoch 4: Validation End     #############\n",
      "before cal avg train loss 0.17817208274166182\n",
      "Epoch: 4 \tAvgerage Training Loss: 0.000130 \tAverage Validation Loss: 0.000108\n",
      "Validation loss decreased (0.000191 --> 0.000108).  Saving model ...\n",
      "############# Epoch 4  Done   #############\n",
      "\n",
      "############# Epoch 5: Training Start   #############\n",
      "yyy epoch 0\n",
      "Epoch: 5, Training Loss:  0.1338750720024109\n",
      "before loss data in training 0.1338750720024109 0\n",
      "after loss data in training 0.1338750720024109 0.1338750720024109\n",
      "yyy epoch 1\n",
      "before loss data in training 0.13358433544635773 0.1338750720024109\n",
      "after loss data in training 0.13358433544635773 0.1337297037243843\n",
      "yyy epoch 2\n",
      "before loss data in training 0.13162674009799957 0.1337297037243843\n",
      "after loss data in training 0.13162674009799957 0.13302871584892273\n",
      "yyy epoch 3\n",
      "before loss data in training 0.13132280111312866 0.13302871584892273\n",
      "after loss data in training 0.13132280111312866 0.1326022371649742\n",
      "yyy epoch 4\n",
      "before loss data in training 0.1303117722272873 0.1326022371649742\n",
      "after loss data in training 0.1303117722272873 0.13214414417743683\n",
      "yyy epoch 5\n",
      "before loss data in training 0.13057194650173187 0.13214414417743683\n",
      "after loss data in training 0.13057194650173187 0.13188211123148602\n",
      "yyy epoch 6\n",
      "before loss data in training 0.1275145411491394 0.13188211123148602\n",
      "after loss data in training 0.1275145411491394 0.13125817264829365\n",
      "yyy epoch 7\n",
      "before loss data in training 0.12929819524288177 0.13125817264829365\n",
      "after loss data in training 0.12929819524288177 0.13101317547261715\n",
      "yyy epoch 8\n",
      "before loss data in training 0.1290850192308426 0.13101317547261715\n",
      "after loss data in training 0.1290850192308426 0.13079893589019775\n",
      "yyy epoch 9\n",
      "before loss data in training 0.1269301027059555 0.13079893589019775\n",
      "after loss data in training 0.1269301027059555 0.13041205257177352\n",
      "yyy epoch 10\n",
      "before loss data in training 0.12682925164699554 0.13041205257177352\n",
      "after loss data in training 0.12682925164699554 0.1300863433967937\n",
      "yyy epoch 11\n",
      "before loss data in training 0.12489692866802216 0.1300863433967937\n",
      "after loss data in training 0.12489692866802216 0.12965389216939607\n",
      "yyy epoch 12\n",
      "before loss data in training 0.12589052319526672 0.12965389216939607\n",
      "after loss data in training 0.12589052319526672 0.1293644022483092\n",
      "yyy epoch 13\n",
      "before loss data in training 0.12363304197788239 0.1293644022483092\n",
      "after loss data in training 0.12363304197788239 0.12895501937185014\n",
      "yyy epoch 14\n",
      "before loss data in training 0.12545566260814667 0.12895501937185014\n",
      "after loss data in training 0.12545566260814667 0.12872172892093658\n",
      "yyy epoch 15\n",
      "before loss data in training 0.12484396994113922 0.12872172892093658\n",
      "after loss data in training 0.12484396994113922 0.12847936898469925\n",
      "yyy epoch 16\n",
      "before loss data in training 0.1227196678519249 0.12847936898469925\n",
      "after loss data in training 0.1227196678519249 0.12814056303571253\n",
      "yyy epoch 17\n",
      "before loss data in training 0.12223532795906067 0.12814056303571253\n",
      "after loss data in training 0.12223532795906067 0.12781249442034298\n",
      "yyy epoch 18\n",
      "before loss data in training 0.1227855235338211 0.12781249442034298\n",
      "after loss data in training 0.1227855235338211 0.12754791700526288\n",
      "yyy epoch 19\n",
      "before loss data in training 0.12151003628969193 0.12754791700526288\n",
      "after loss data in training 0.12151003628969193 0.12724602296948434\n",
      "yyy epoch 20\n",
      "before loss data in training 0.12023214250802994 0.12724602296948434\n",
      "after loss data in training 0.12023214250802994 0.12691202866179604\n",
      "yyy epoch 21\n",
      "before loss data in training 0.12003300338983536 0.12691202866179604\n",
      "after loss data in training 0.12003300338983536 0.12659934569488873\n",
      "yyy epoch 22\n",
      "before loss data in training 0.11989985406398773 0.12659934569488873\n",
      "after loss data in training 0.11989985406398773 0.12630806345006695\n",
      "yyy epoch 23\n",
      "before loss data in training 0.11754149198532104 0.12630806345006695\n",
      "after loss data in training 0.11754149198532104 0.12594278963903588\n",
      "yyy epoch 24\n",
      "before loss data in training 0.11898881196975708 0.12594278963903588\n",
      "after loss data in training 0.11898881196975708 0.12566463053226473\n",
      "yyy epoch 25\n",
      "before loss data in training 0.1163957342505455 0.12566463053226473\n",
      "after loss data in training 0.1163957342505455 0.1253081345214294\n",
      "yyy epoch 26\n",
      "before loss data in training 0.11827202141284943 0.1253081345214294\n",
      "after loss data in training 0.11827202141284943 0.12504753773963012\n",
      "yyy epoch 27\n",
      "before loss data in training 0.11510742455720901 0.12504753773963012\n",
      "after loss data in training 0.11510742455720901 0.1246925336974008\n",
      "yyy epoch 28\n",
      "before loss data in training 0.11562569439411163 0.1246925336974008\n",
      "after loss data in training 0.11562569439411163 0.1243798840662529\n",
      "yyy epoch 29\n",
      "before loss data in training 0.11401631683111191 0.1243798840662529\n",
      "after loss data in training 0.11401631683111191 0.12403443182508153\n",
      "yyy epoch 30\n",
      "before loss data in training 0.11384627223014832 0.12403443182508153\n",
      "after loss data in training 0.11384627223014832 0.12370578151556756\n",
      "yyy epoch 31\n",
      "before loss data in training 0.11471202969551086 0.12370578151556756\n",
      "after loss data in training 0.11471202969551086 0.12342472677119079\n",
      "yyy epoch 32\n",
      "before loss data in training 0.1128222718834877 0.12342472677119079\n",
      "after loss data in training 0.1128222718834877 0.12310344025944221\n",
      "yyy epoch 33\n",
      "before loss data in training 0.11145235598087311 0.12310344025944221\n",
      "after loss data in training 0.11145235598087311 0.12276076131007253\n",
      "yyy epoch 34\n",
      "before loss data in training 0.11264374852180481 0.12276076131007253\n",
      "after loss data in training 0.11264374852180481 0.1224717038018363\n",
      "yyy epoch 35\n",
      "before loss data in training 0.11162061244249344 0.1224717038018363\n",
      "after loss data in training 0.11162061244249344 0.12217028459741011\n",
      "yyy epoch 36\n",
      "before loss data in training 0.11100948601961136 0.12217028459741011\n",
      "after loss data in training 0.11100948601961136 0.12186864139260474\n",
      "yyy epoch 37\n",
      "before loss data in training 0.10934190452098846 0.12186864139260474\n",
      "after loss data in training 0.10934190452098846 0.12153899042229904\n",
      "yyy epoch 38\n",
      "before loss data in training 0.1103854775428772 0.12153899042229904\n",
      "after loss data in training 0.1103854775428772 0.12125300291257028\n",
      "yyy epoch 39\n",
      "before loss data in training 0.11015914380550385 0.12125300291257028\n",
      "after loss data in training 0.11015914380550385 0.12097565643489362\n",
      "yyy epoch 40\n",
      "before loss data in training 0.11033458262681961 0.12097565643489362\n",
      "after loss data in training 0.11033458262681961 0.12071611804933084\n",
      "yyy epoch 41\n",
      "before loss data in training 0.10900320112705231 0.12071611804933084\n",
      "after loss data in training 0.10900320112705231 0.12043723907499088\n",
      "yyy epoch 42\n",
      "before loss data in training 0.10789906978607178 0.12043723907499088\n",
      "after loss data in training 0.10789906978607178 0.12014565374269043\n",
      "yyy epoch 43\n",
      "before loss data in training 0.10646851360797882 0.12014565374269043\n",
      "after loss data in training 0.10646851360797882 0.1198348096487197\n",
      "yyy epoch 44\n",
      "before loss data in training 0.10732152312994003 0.1198348096487197\n",
      "after loss data in training 0.10732152312994003 0.11955673661496904\n",
      "yyy epoch 45\n",
      "before loss data in training 0.10654430091381073 0.11955673661496904\n",
      "after loss data in training 0.10654430091381073 0.11927385757798734\n",
      "yyy epoch 46\n",
      "before loss data in training 0.10564989596605301 0.11927385757798734\n",
      "after loss data in training 0.10564989596605301 0.11898398605432915\n",
      "yyy epoch 47\n",
      "before loss data in training 0.10519222170114517 0.11898398605432915\n",
      "after loss data in training 0.10519222170114517 0.11869665763030449\n",
      "yyy epoch 48\n",
      "before loss data in training 0.10533027350902557 0.11869665763030449\n",
      "after loss data in training 0.10533027350902557 0.11842387428089063\n",
      "yyy epoch 49\n",
      "before loss data in training 0.10381639748811722 0.11842387428089063\n",
      "after loss data in training 0.10381639748811722 0.11813172474503517\n",
      "yyy epoch 50\n",
      "before loss data in training 0.10531927645206451 0.11813172474503517\n",
      "after loss data in training 0.10531927645206451 0.1178805002687024\n",
      "yyy epoch 51\n",
      "before loss data in training 0.10307634621858597 0.1178805002687024\n",
      "after loss data in training 0.10307634621858597 0.11759580499850786\n",
      "yyy epoch 52\n",
      "before loss data in training 0.10360544919967651 0.11759580499850786\n",
      "after loss data in training 0.10360544919967651 0.11733183602117142\n",
      "yyy epoch 53\n",
      "before loss data in training 0.10296620428562164 0.11733183602117142\n",
      "after loss data in training 0.10296620428562164 0.11706580580384643\n",
      "yyy epoch 54\n",
      "before loss data in training 0.1022944450378418 0.11706580580384643\n",
      "after loss data in training 0.1022944450378418 0.11679723560810089\n",
      "yyy epoch 55\n",
      "before loss data in training 0.10139995068311691 0.11679723560810089\n",
      "after loss data in training 0.10139995068311691 0.11652228409158331\n",
      "yyy epoch 56\n",
      "before loss data in training 0.10105350613594055 0.11652228409158331\n",
      "after loss data in training 0.10105350613594055 0.11625090202218608\n",
      "yyy epoch 57\n",
      "before loss data in training 0.10004221647977829 0.11625090202218608\n",
      "after loss data in training 0.10004221647977829 0.11597144192662732\n",
      "yyy epoch 58\n",
      "before loss data in training 0.10114721208810806 0.11597144192662732\n",
      "after loss data in training 0.10114721208810806 0.11572018379377107\n",
      "yyy epoch 59\n",
      "before loss data in training 0.09996341168880463 0.11572018379377107\n",
      "after loss data in training 0.09996341168880463 0.11545757092535495\n",
      "yyy epoch 60\n",
      "before loss data in training 0.09955169260501862 0.11545757092535495\n",
      "after loss data in training 0.09955169260501862 0.11519681882174289\n",
      "yyy epoch 61\n",
      "before loss data in training 0.09881952404975891 0.11519681882174289\n",
      "after loss data in training 0.09881952404975891 0.11493266890606572\n",
      "yyy epoch 62\n",
      "before loss data in training 0.09855243563652039 0.11493266890606572\n",
      "after loss data in training 0.09855243563652039 0.11467266520337453\n",
      "yyy epoch 63\n",
      "before loss data in training 0.09828559309244156 0.11467266520337453\n",
      "after loss data in training 0.09828559309244156 0.1144166172016412\n",
      "yyy epoch 64\n",
      "before loss data in training 0.09607695043087006 0.1144166172016412\n",
      "after loss data in training 0.09607695043087006 0.11413446848209087\n",
      "yyy epoch 65\n",
      "before loss data in training 0.09667859226465225 0.11413446848209087\n",
      "after loss data in training 0.09667859226465225 0.11386998550909938\n",
      "yyy epoch 66\n",
      "before loss data in training 0.09613966941833496 0.11386998550909938\n",
      "after loss data in training 0.09613966941833496 0.11360535392565514\n",
      "yyy epoch 67\n",
      "before loss data in training 0.09672147780656815 0.11360535392565514\n",
      "after loss data in training 0.09672147780656815 0.11335706162978622\n",
      "yyy epoch 68\n",
      "before loss data in training 0.09502290189266205 0.11335706162978622\n",
      "after loss data in training 0.09502290189266205 0.1130913491698279\n",
      "yyy epoch 69\n",
      "before loss data in training 0.09513159841299057 0.1130913491698279\n",
      "after loss data in training 0.09513159841299057 0.11283478130187308\n",
      "yyy epoch 70\n",
      "before loss data in training 0.09468590468168259 0.11283478130187308\n",
      "after loss data in training 0.09468590468168259 0.11257916332130702\n",
      "yyy epoch 71\n",
      "before loss data in training 0.09418913722038269 0.11257916332130702\n",
      "after loss data in training 0.09418913722038269 0.11232374629212752\n",
      "yyy epoch 72\n",
      "before loss data in training 0.09362373501062393 0.11232374629212752\n",
      "after loss data in training 0.09362373501062393 0.11206758175402473\n",
      "yyy epoch 73\n",
      "before loss data in training 0.09318451583385468 0.11206758175402473\n",
      "after loss data in training 0.09318451583385468 0.11181240518753595\n",
      "yyy epoch 74\n",
      "before loss data in training 0.09301411360502243 0.11181240518753595\n",
      "after loss data in training 0.09301411360502243 0.11156176129976911\n",
      "yyy epoch 75\n",
      "before loss data in training 0.09352738410234451 0.11156176129976911\n",
      "after loss data in training 0.09352738410234451 0.1113244668629609\n",
      "yyy epoch 76\n",
      "before loss data in training 0.09209471195936203 0.1113244668629609\n",
      "after loss data in training 0.09209471195936203 0.11107472978629078\n",
      "yyy epoch 77\n",
      "before loss data in training 0.09082651883363724 0.11107472978629078\n",
      "after loss data in training 0.09082651883363724 0.11081513733817984\n",
      "yyy epoch 78\n",
      "before loss data in training 0.09113051742315292 0.11081513733817984\n",
      "after loss data in training 0.09113051742315292 0.11056596493419216\n",
      "yyy epoch 79\n",
      "before loss data in training 0.09105615317821503 0.11056596493419216\n",
      "after loss data in training 0.09105615317821503 0.11032209228724245\n",
      "yyy epoch 80\n",
      "before loss data in training 0.09089451283216476 0.11032209228724245\n",
      "after loss data in training 0.09089451283216476 0.11008224562730322\n",
      "yyy epoch 81\n",
      "before loss data in training 0.09002985805273056 0.11008224562730322\n",
      "after loss data in training 0.09002985805273056 0.10983770431541819\n",
      "yyy epoch 82\n",
      "before loss data in training 0.08952157944440842 0.10983770431541819\n",
      "after loss data in training 0.08952157944440842 0.10959293172661085\n",
      "yyy epoch 83\n",
      "before loss data in training 0.08971653878688812 0.10959293172661085\n",
      "after loss data in training 0.08971653878688812 0.10935630800113795\n",
      "yyy epoch 84\n",
      "before loss data in training 0.08852643519639969 0.10935630800113795\n",
      "after loss data in training 0.08852643519639969 0.10911125067402339\n",
      "yyy epoch 85\n",
      "before loss data in training 0.08870653063058853 0.10911125067402339\n",
      "after loss data in training 0.08870653063058853 0.10887398648747182\n",
      "yyy epoch 86\n",
      "before loss data in training 0.08814351260662079 0.10887398648747182\n",
      "after loss data in training 0.08814351260662079 0.10863570517849652\n",
      "yyy epoch 87\n",
      "before loss data in training 0.0886472687125206 0.10863570517849652\n",
      "after loss data in training 0.0886472687125206 0.10840856385501953\n",
      "yyy epoch 88\n",
      "before loss data in training 0.08833707869052887 0.10840856385501953\n",
      "after loss data in training 0.08833707869052887 0.10818304154980053\n",
      "yyy epoch 89\n",
      "before loss data in training 0.0872117206454277 0.10818304154980053\n",
      "after loss data in training 0.0872117206454277 0.10795002687308528\n",
      "yyy epoch 90\n",
      "before loss data in training 0.0864742249250412 0.10795002687308528\n",
      "after loss data in training 0.0864742249250412 0.1077140290494804\n",
      "yyy epoch 91\n",
      "before loss data in training 0.08670061826705933 0.1077140290494804\n",
      "after loss data in training 0.08670061826705933 0.10748562241054103\n",
      "yyy epoch 92\n",
      "before loss data in training 0.0861358642578125 0.10748562241054103\n",
      "after loss data in training 0.0861358642578125 0.10725605511857621\n",
      "yyy epoch 93\n",
      "before loss data in training 0.08652811497449875 0.10725605511857621\n",
      "after loss data in training 0.08652811497449875 0.10703554511704347\n",
      "yyy epoch 94\n",
      "before loss data in training 0.08486945182085037 0.10703554511704347\n",
      "after loss data in training 0.08486945182085037 0.1068022178191888\n",
      "yyy epoch 95\n",
      "before loss data in training 0.08439090102910995 0.1068022178191888\n",
      "after loss data in training 0.08439090102910995 0.10656876660262549\n",
      "yyy epoch 96\n",
      "before loss data in training 0.08464350551366806 0.10656876660262549\n",
      "after loss data in training 0.08464350551366806 0.1063427329831517\n",
      "yyy epoch 97\n",
      "before loss data in training 0.08472274988889694 0.1063427329831517\n",
      "after loss data in training 0.08472274988889694 0.10612212091076134\n",
      "yyy epoch 98\n",
      "before loss data in training 0.08391904085874557 0.10612212091076134\n",
      "after loss data in training 0.08391904085874557 0.10589784737488239\n",
      "yyy epoch 99\n",
      "before loss data in training 0.08233389258384705 0.10589784737488239\n",
      "after loss data in training 0.08233389258384705 0.10566220782697204\n",
      "yyy epoch 100\n",
      "before loss data in training 0.08181479573249817 0.10566220782697204\n",
      "after loss data in training 0.08181479573249817 0.10542609483593765\n",
      "############# Epoch 5: Training End     #############\n",
      "############# Epoch 5: Validation Start   #############\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "from datetime import datetime\n",
    "start = datetime.now()\n",
    "trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path, steps_per_epochs=100)\n",
    "end =  datetime.now()\n",
    "duration = end-start\n",
    "print(\"time elapsed:\", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert_model): CamembertModel(\n",
       "    (embeddings): CamembertEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): CamembertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x CamembertLayer(\n",
       "          (attention): CamembertAttention(\n",
       "            (self): CamembertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): CamembertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): CamembertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): CamembertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): CamembertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=103021, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['valid_loss_min']\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:  Le coenseignement en pratique Présentation et analyse de tous les aspects du coenseignement, ses particularités et ses bénéfices, tant du point de vue des élèves que des professeurs. Ce guide explique comment mettre en place cette façon d'enseigner en abordant plusieurs points techniques : la forme, la posture des enseignants, la répartition des apprentissages, l'autorité, l'évaluation, entre autres.\n",
      "Description:  Les sommets de l'État : essai sur l'élite du pouvoir en France u XIXe siècle à nos jours, l'Etat \"fort\" à la française a connu bien des vicissitudes que l'on se propose de retracer ici. Institution prestigieuse attirant vers elle les élites de la nation issues des Grandes Ecoles, l'Etat organise les activités les plus diverses grâce à son armée de fonctionnaires fidèles à la logique de leur rôle et aux valeurs du service public. Les élites politiques et celles de l'Etat en viennent alors souvent à se confondre, d'autant que l'état demeure fermé aux intrus du monde des affaires, des professions libérales ou des milieux syndicaux. La République des fonctionnaires étend son contrôle loin dans la société à travers les entreprises publiques, ou encore par le biais du pantouflage. Cet Etat \"fort\" n'en rencontre pas moins la vive hostilité des élites issues des partis de masse ou des notables de province solidement attachés à leurs fiefs ; le mouvement ouvrier et, davantage encore, les milieux économiques dominants récusent aussi sa légitimité au nom de leurs valeurs propres. En dépit de ces refus, en France, l'Etat est demeuré le lieu de régulation de la vie sociale ou culturelle. De nos jours pourtant, après les diverses alternances, de nouveaux processus de circulation des élites se profilent, estompant peu à peu les frontières autrefois si nettement défendues de l'Etat.\n",
      "Initiales ornées\n",
      "Description:  Le dollar La quatrième de couverture indique : \"Quelle est l'importance pour l'économie mondiale des fluctuations quotidiennes du cours du dollar américain ? Pourquoi le taux de change du dollar a-t-il connu depuis quinze ans des variations ausi importantes, en baisse puis en hausse ? Qu'est-ce que le \"marché de l'eurodollar\" ? En quoi le dollar est-il un instrument de puissance pour les Etats-Unis ? Qu'est-ce qu'une monnaie internationale ? Quels sont les liens entre l'or et le dollar ? La dominaton du dollar sur els marchés internationaux peut-elle être remise en cause ? Quelles sont les chances de l'Ecu, future monnaie européenne ? Peut-on manipuler un taux de change ? Quels sont les enjeux des réformes monétaires internationales ?\"\n",
      "Initiales ornées\n",
      "Description:  Les intellectuels sous la Ve République : 1958-1990 Célèbres, influents, on les voit, on en parle : les intellectuels, décriés ou adulés, s'ils font toujours d'objet d'un culte en France, souffrent pourtant d'une crise d'identité. Démantèlement de l'URSS, chute des régimes communistes, emprise de la communication sur la connaissance... La hiérarchie des intellectuels est bousculée, dominants et dominés échangent leurs vestes, un nouvel équilibre, précaire, se dessine. Sous forme d'analyse sociologique et d'enquête - plus de quatre-vingts de ces \"clercs \" ont été interrogés - Rémy Rieffel étudie leurs nouveaux modes d'affiliation (T 1), les instruments de leur légitimation (T 2) et les voies de leur consécration (T 3)\n",
      "Initiales ornées\n",
      "Description:  Bouddha, bouddhisme La 4e de couv. indique : \"Ce petit livre répond de façon claire, brève et précise à un grand nombre d'interrogations que nous nous posons. Le bouddhisme est-il une religion ? Qui était le Bouddha ? Quel est le contenu de son enseignement ? Il propose, en même temps que de faire comprendre le rayonnement de son message, d'accompagner la lecture par des extraits de textes, des illustrations, des citations. Un guide pratique et un glossaire indiquent enfin à ceux qui le désirent comment aller sur les traces du Bouddha.\"\n",
      "Initiales ornées\n",
      "Description:  Apprendre à aimer les mathématiques : conditions socio-institutionnelles et élaboration psychique dans les ateliers mathématiques Les entretiens d'élèves et d'enseignants, analysés dans cet ouvrage dans une perspective clinique, mettent en question à la fois les théories de l'apprentissage et les problématiques de l'institution et de l'innovation. A quelles conditions institutionnelles une innovation peut-elle aider les élèves à apprendre, notamment quand ils sont en difficulté ?\n",
      "Initiales ornées\n",
      "Description:  L'homme devant l'incertain La quatrième de couverture indique : \"Réintroduire dans la science la flèche du temps fut l'une des ambitions qui auront marqué mes recherches sur une période de l'ordre d'un demi-siècle. Nous voyons aujourd'hui se dégager de nouveaux horizons qui nous permettent d'envisager une reformulation des lois de la nature. A l'univers automate succède une nature en construction ; la notion de \"nouveau\" prend un sens cosmologique. Le futur n'est pas donné. Nous pouvons aujourd'hui donner un sens précis à cette condition de toute créativité grâce aux outils que nous procurent les mathématiques modernes. La flèche du temps et l'évolution créatrice, notions étroitement associées, posent, dans de nombreux domaines, des questions que je crois décisives. Pour traiter de tels sujets, il fallait ouvrir le dialogue avec des représentants de pratiques scientifiques différentes. C'est ce que nous avons tenté de faire.\"\n",
      "Initiales ornées\n",
      "Description:  Imperator Caesar Augustus : recherches sur le pouvoir impérial romain (Ier siècle av. - début du IVe siècle ap. J.-C.) Ce dossier comporte quatre volumes qui abordent la définition du pouvoir impérial romain en retenant l'espace urbain de Rome comme lieu privilégié d'analyse de ses manifestations les plus significatives. Le mémoire de synthèse permet en ouverture de retracer un itinéraire de recherche en dressant l'état des lieux d'une enquête menéedans le cadre de travaux universitaires mais également relayée par l'enseignement. Deux essais traitant de  la titulature du prince et de son pouvoir normatif structurent cette présentation des modalités d'approche du sujet justifiant de placer l'empereur romain entre communauté des hommes et communauté des dieux. Figure en deuxième position le livre tiré de notre thèse de doctorat soutenue en 1992 et publiée sous le titre La Fête à Rome au premier siècle de l'Empire. Recherches sur l'univers festif sous les règnes d'Auguste et des Julio-Claudiens, \" collection Latomus \" no248, Bruxelles, 1999. Le troisième volume correspond à un nouvel ouvrage intitulé Le prince et la cité. pouvoir impérial et cérémonies publiques. Il aborde le problème de la définition du régime impérial en prenant appui sur les manifestations publiques du pouvoir de l'empereur à Rome et en choisissant une période d'analyse vaste mais cohérente qui nous semble susceptible - de la fin de la République au tournant de l'Empire chrétien sous Constantin avec l'abandon de Rome comme résidence impériale - d'éclairer les principales évolutions et de traduire les grandes lignes de ce principat, entre république et monarchie. Ont été regroupées sous les thèmes de la reconnaissance et de l'identification du prince, puis des liens entre ce dernier, la cité et les communautés des hommes et des dieux, trois démarches exploitant les données selon des angles d'approche différents pour traiter de l'entrée solennelle et des funérailles publiques des empereurs, puis de leur triomphe et de l'éternité de la cité et de la fonction impériale, enfin d'une image du prince en prêtre et père de la patrie, à mi-chemin entre hommes et dieux\n",
      "Initiales ornées\n",
      "Description:  L'escalade en situation Au cours des trois premiers axes de travail, Serge Testevuide propose, pour le cours d'EPS, des situations d'escalade en bloc et en moulinette : saisir et utiliser les prises, varier et coordonner ses gestuelles, lire et enchaîner les passages d'une voie. Dans l'axe 4, plus particulièrement, il met l'accent sur le grimper en tête et la sécurité.\n",
      "Initiales ornées\n",
      "Description:  Cahier d'activités : B2i, Brevet Informatique et Internet, niveau 2 Chaque compétence est traitée en trois points : les notions fondamentales, des exercices d'entraînement, des situations de validation de la compétence.\n",
      "Initiales ornées\n",
      "Description:  Eoliennes en mer et maîtrise foncière : éléments juridiques Ce document vise principalement à être un outil pratique à la destination des porteurs de projets et des différents acteurs intervenant dans le cadre de l'implantation d'éoliennes en mer. Il se veut également instrument en vue des discussions inévitables sur la définition de la procédure d'implantation des dites éoliennes. Fondé sur le droit existant et sur la jurisprudence, il ne préjuge en rien des éventuelles modifications législatives ou réglementaires apparues après sa date de réalisation. Ce document est avant tout un document juridique et, même s'il énonce des propositions, il ne s'exprime en aucun cas sur les décisions politiques qui ont été prises ou qui seront prises en matière d'implantation d'éoliennes en mer. Ainsi, ses propositions sont avant tout techniques visant à aider à la prise de décision et à améliorer le droit existant, en aucun cas à s'y substituer.\n",
      "Initiales ornées\n",
      "Description:  La Nouvelle-Calédonie Possession française depuis 1853, terre de bagne qui accueillit des communards, eldorado du nickel pendant le boom du début des années 1970, rassemblant un peuple autochtone, les Kanak, des descendants de Français venus d'Europe et des immigrants venus d'Océanie, d'Asie et même d'Afrique du Nord et de l'océan Indien, la Nouvelle-Calédonie demeure peu connue en métropole. Elle a pourtant été à la une de l'actualité à plusieurs reprises, à la suite des manifestations indépendantistes qui s'y sont développées à partir de 1984 et ont culminé avec les violences d'Ouvéa, puis de l'apaisement des accords de Matignon de 1988, ratifiés par un référendum national, enfin, en 1998, de l'accord de Nouméa, qui a nécessité une révision de la Constitution pour mettre en place en Nouvelle-Calédonie un système institutionnel évolutif. Alain Christnacht, qui a exercé en Nouvelle-Calédonie et à Paris des responsabilités aux différentes étapes de ces évolutions, propose des clés pour aborder ce territoire si particulier. Il décrit les étapes de la colonisation, explique les caractéristiques essentielles de la riche culture kanak, montre l'originalité des mécanismes mis en place par les accords de 1988 et 1998. Il s'attache également à analyser les différents secteurs de l'économie de l'île, notamment celui du nickel, à exposer les grandes lignes des politiques éducatives et sociales mises en œuvre tant par l'Etat que par les autorités locales et à replacer la Nouvelle-Calédonie dans son environnement géographique, celui du Pacifique insulaire, dont il présente l'organisation régionale. Unique désormais par ses institutions, qui en font un quasi-Etat fédéré dans notre République unitaire, la Nouvelle-Calédonie illustre aussi les difficultés et les succès d'une politique de \" rééquilibrage \", entre des communautés ethnoculturelles, dans une République qui ne connaît pas, en principe, de telles distinctions. La Nouvelle-Calédonie apparaît ainsi comme un cas bien particulier, en quelque sorte \" limite \", mais qui peut apporter des enseignements utiles pour d'autres situations.\n",
      "Initiales ornées\n",
      "Description:  Alerte francophone : plaidoyer et plan d'action pour les générations futures Tantôt décriée, tantôt surestimée, la Francophonie mérite d'être jugée pour ce qu'elle est, avec son passé tumultueux, son présent fragile et ses promesses d'avenir. En ces temps d'hésitation, d'apathie, voire de démission collective, cet espace de 56 membres où vivent 600 millions d'habitants s'avère une chance en ce qu'elle permet d'échapper au choix entre le \"tout Coca Cola\" et le \"tout Ayatollah\". Dans cet ouvrage exhaustif et rigoureux, les deux auteurs ne se contentent pas de décrire la genèse de la saga francophone, de dresser le bilan des forces et faiblesses de la Francophonie, de soupeser les enjeux et les risques de cette aventure collective. Ils tracent aussi des perspectives d'avenir en proposant un grand dessein qui se décline en un plan d'action audacieux, complet et cohérent, assorti de mesures concrètes. [4e de couv.]\n",
      "Initiales ornées\n",
      "Description:  Etude du vascular endothelial growth factor (VEGF) dans le mélanome cutané L'incidence du mélanome dans les populations à peaux claires double tous les 10 ans. Actuellement, seul l'indice de Breslow est un facteur pronostique lié à la survie. L'étude de nouveaux marqueurs, comme les facteurs angiogéniques, qui sont indispensables à la croissance tumorale et à la diffusion métastatique nous semble donc pertinente. Parmi ceux-ci, le vascular endothelial growth factor (VEGF), glycoprotéine dimérique spécifique des cellules endothéliales, est l'un des plus importants. Matériels et méthodes : Nous avons étudié, entre février 2002 et juillet 2003, les concentrations plasmatiques de VEGF (méthode ELISA) d'une population de 254 patients souffrant d'un mélanome. Résultats : Les concentrations plasmatiques moyennes de VEGF sont statistiquement différentes de la population témoin quel que soit le stade, métastatique ou non. Une différence significative est observée entre les stades I et IIB comparés au stade III. Quatre patients ont eu, au cours du suivi, une élévation de leurs concentrations de VEGF prédictive d'une rechute métastatique ganglionnaire. Toutefois, un patient avec une valeur initiale basse de VEGF a présenté des métastases ganglionnaires un an plus tard. Conclusion : Le VEGF plasmatique peut être considéré comme un marqueur intéressant corrélé à la masse tumorale. La poursuite de l'étude permettra de préciser sa valeur prédictive et pronostique.\n",
      "Initiales ornées\n",
      "Description:  L' Astronomie Une présentation de l'histoire de l'astronomie, des astronomes les plus célèbres et du système solaire ainsi que des principaux types d'objets célestes.\n",
      "Initiales ornées\n",
      "Description:  Friedrich A. von Hayek : vie, oeuvres, concepts La 4e de couverture indique: \"Les œuvres de Friedrich Hayek ne figurent en haut des lectures qu'au milieu des années 1970 en raison sans doute du succès du keynésianisme dans un contexte de guerre froide. En réalité, ce prix Nobel d'économie est tout sauf un économiste étriqué, confiné dans le culte des agrégats car il croit au jeu subtil des disciplines. Grand \" pape \" du libéralisme, la vision qu'il a du marché est naturellement conditionnée par sa croyance en un ordre spontané. Cependant, son œuvre considérable prodigue toutes les facettes d'une pensée complexe qui se déploie dans la philosophie, le droit et la politique et c'est pour cela qu'il fait partie de ces grands hommes incontournables.\"\n",
      "Initiales ornées\n",
      "Description:  Super, nouveau, génial aParodies de publicités vantant les mérites des produits insolites ou loufoques : de la neige de plage, une balance qui fait perdre un kilo par jour, des lits-trampolines, une grande soeur poupée Barbie, etc. Un livre très drôle, satyrique, jubilatoire pour ceux et celles exaspérés par la publicité, envahissante dans notre société de consommation.\n",
      "Initiales ornées\n",
      "Description:  Circulations Au sommaire : Manières de circuler en France depuis 1880 (Lavenir, Catherine Bertho). Un troisième sexe ? Les bourgeoises et la bicyclette dans la France fin de siècle (Thompson, Christopher). Les femmes et l'automobile à la Belle Époque (Buisseret, Alexandre). Valeurs du sport catholique, valeurs catholiques du sport. L'Église catholique et le vélo (Rocher, Philippe). Infrastructures et citadins : réflexions sur l'acceptation et l'impact de l'automobile à Paris au xxe siècle (Flonneau, Matthieu). Aux sources du voyage contemporain : la diffusion d'un art du voyage à Lyon de la Belle Époque aux années folles (Fontaines, Gérard)\n",
      "Initiales ornées\n",
      "Description:  Tennis : soyez P.R.O. : la méthode pour oser Comment réussir à donner le meilleur de soi lors d'une compétition ? Quelles sont les recettes pour exploiter son potentiel et optimiser ses qualités ? Résultat de la collaboration de Ronan Lafaix avec plusieurs joueurs français du circuit ATP, découvrez une méthode inédite pour atteindre la performance. Mais être P.R.O., qu'est-ce que c'est ? Une méthode résumée en trois lettres, trois mots-clés pour vous libérer et vous permettre d'oser. P, pour se Poser, adopter une attitude stable qui vous permette de consolider vos acquis et de suivre les objectifs fixés ; R, pour Respirer de façon naturelle et harmonieuse afin de mieux gérer vos émotions ; O, pour Observer, prendre conscience de vos sensations intérieures et mieux appréhender votre environnement. Suivez ce livre concret et progressif qui vous guide pas à pas, découvrez les témoignages de grands joueurs, explorez et adoptez une démarche novatrice et... osez !\n",
      "Initiales ornées\n",
      "Description:  Une mécanique donnée à voir : les thèses illustrées défendues à Louvain en juillet 1624 par Grégoire de Saint-Vincent S. J. En 1624, quelques mois après l’accession d’Urbain VIII au trône de Saint Pierre, plusieurs espéraient un infléchissement de la condamnation venue interdire en 1616 l’enseignement du mouvement de la Terre autour du Soleil alors défendue publiquement par Galilée. Etait de ceux-là l’inspirateur des thèses, le jésuite Grégoire de Saint-Vincent, né à Bruges une quarantaine d’années auparavant : il avait activement participé à la séance du Collège Romain lorsque Galilée en 1611 commentait ses observations au télescope de planètes comme Saturne ou Vénus, ce qui faisait « murmurer les philosophes ». Ainsi le raconte Grégoire lui-même. Les thèses de 1624 montrent une extraordinaire représentation de Saturne. Voilà un exemple parmi bien d’autres des surprises de ces thèses. Onze chapitres, suivis d’une bibliographie, organisent l’enquête sur les thèses, cellesci étant traduites au chapitre IX. Le document est d’abord présenté avec les problèmes qu’il pose à l’historien. Puis le moment même des thèses, l’imaginaire des hommes de cette période, et les positions épistémologiques d’alors sont discutés, tant avec le texte qu’avec les images. Cette conjonction d’analyses est essentielle à l’enquête qui se poursuit sur les acteurs des thèses, avec trois récits possibles, le récit historique de la journée des thèses, le récit scientifique du contenu mais aussi le récit iconologique. A ce point, on peut entrer d’une part dans la tradition des thèses universitaires, d’autre part dans la tradition du livre illustré. Ce qui, à partir des travaux des historiens de la mécanique, permet d’aboutir à une discussion sur la place de ces thèses dans une histoire qui a tant servi à constituer les diverses philosophies des sciences, dont le positivisme, le constructivisme, etc. Après la traduction proposée, il convient de revenir à titre de justification sur le détail de chaque théorème et de chaque vignette, et de terminer par le vocabulaire latin des thèses. Cette démarche est tout le contraire de la démarche dogmatique si naturelle à l’histoire des sciences, discipline dont il faut se rappeler qu’elle doit beaucoup au positivisme. Si l’enquête sur les textes et les images s’avère beaucoup plus longue que les courtes thèses, le plaisir n’est-il pas au final de retrouver la cohérence d’un des mondes du baroque à l’aube de la science moderne ? L’intérêt est en particulier de surprendre la façon dont un intellectuel issu d’un ordre religieux connu pour son obéissance disciplinaire, parvient malgré la rigoureuse orthodoxie récemment mise en place, à raisonnablement donner sa place à une nouvelle imagination, sans entrer en dissidence mais sans céder, cherchant sans aucun doute à libérer la pensée religieuse de la pensée scientifique, et s’aidant alors de la pensée toute profane d’un peintre d’emblèmes.\n",
      "Initiales ornées\n",
      "Description:  L'abbé Pierre et Jean Prouvé, la Maison des jours meilleurs En 1956, face à la question cruciale du logement pour les plus démunis, Jean Prouvé propose, en réponse à l’appel de l’abbé Pierre lancé deux ans auparavant, sa Maison des jours meilleurs. Cet habitat temporaire – composé de 3 pièces, cuisine et sanitaires – est construit à base d’éléments usinés en amont. Le projet restera à l’état de prototype.\n",
      "Initiales ornées\n",
      "Description:  Créativité et innovation dans les territoires Les territoires sont tous confrontés aux mêmes crises et mutations. Cependant, certains réagissent mieux que d'autres et même, rebondissent grâce à l'innovation. Les exigences du développement durable et les révolutions technologiques vont se traduire par la relocalisation des activités dans les territoires créatifs. Ces derniers, indépendamment de leur taille et de leur localisation, savent conjuguer la qualité de vie et l'harmonie sociale avec la dynamique des pôles de compétitivité. Le terme \"innovation\" désigne souvent des réalités très différentes, depuis la recherche-développement jusqu'à la nouveauté purement commerciale. Cet ouvrage montre comment se présente cette innovation, qui fait avant tout appel au bon sens et à la créativité d'hommes capables de progresser ensemble avec l'intelligence des savoir-faire managérial et organisationnel. Retenons le message principal de ce rapport : les facteurs de développement ou de récession sont d'abord endogènes et les entrepreneurs sont les magiciens de la croissance. La baguette magique de ces créateurs réside dans l'innovation sous tous ses aspects technologiques, commerciaux, financiers, organisationnels. Le rôle des pouvoirs publics modernes est de rester modestes, d'accompagner, et non de freiner, la contagion des initiatives dans les territoires pour que les créateurs deviennent plus entrepreneurs et innovants. [4e de couv.]\n",
      "Initiales ornées\n",
      "Description:  Ethologie animale et humaine : communication et comportement L'éthologie, science du comportement animal et humain, s'est d'abord développée à partir de l'observation des animaux dans leur milieu naturel. Ainsi, l'éthologie a ouvert de nouvelles perspectives de compréhension des comportements tels que ceux d'attachement ou d'agression, de territoire, d'espace vital, de dominance-hiérarchie et apporté de nouveaux éléments aux rapports entre nature et culture, aux apprentissages et conditionnements, aux interactions sociales (rapports familiaux, relations dans le monde du travail et de la vie politique...). Il en est de même des manifestations motrices (mimique, gestique, proxémie), sonores (cris, vocalisations, chants, bruitages), visuelles (modifications cutanées et du système pileux, vêtements, insignes), sécrétoires (larmes, sueur et toutes modifications glandulaires) qui ont permis de mieux comprendre les différents registres de la communication et des comportements sociaux. Dérivée de l'éthologie animale, l'éthologie humaine s'intéresse aux comportements de l'Homme. L'étude des communications non verbales y occupe une place de choix comme au cours d'entretiens et de questionnaires, de la circulation des personnes à l'intérieur de l'entreprise, sans oublier le langage verbal lui-même ou les comportements mettant en présence l'Homme avec la machine (informatique notamment). En éthologie, il ne s'agit pas de comparer simplement l'Homme à l'Animal mais de transposer les méthodes d'approche et d'investigation. Ce livre, enrichi de nombreux dessins originaux, s'adresse aux étudiants et chercheurs en éthologie, sciences humaines et communication ainsi qu'au grand public s'intéressant au comportement et à la communication, en raison de l'importance de certaines applications pratiques (comportement de l'enfant, interactions avec la machine, entretiens, marketing et questionnaires, thérapies par l'animal etc). [4e de couverture]\n",
      "Initiales ornées\n",
      "Description:  Les paradoxes de l'empathie : philosophie, psychanalyse, sciences sociales La 4e de couv. indique : \"Pourquoi, après des décennies de critique, de discrédit, voire d’ignorance, l’empathie connaît-elle aujourd’hui un regain d’intérêt théorique en philosophie, en psychanalyse et en sciences sociales ? C’est à cette question que cet ouvrage collectif souhaite répondre en interrogeant les recherches contemporaines sur l’empathie à partir de ses paradoxes théoriques, cliniques et moraux : une compréhension d’autrui à la fois affective et cognitive, identificatoire et sélective, automatique et inégalement distribuée. En examinant les perspectives récemment ouvertes qui montrent l’empathie sous un jour nouveau, en prenant en compte la complexité de son histoire et de ses modèles, la diversité de ses pratiques et de ses usages, les auteurs proposent des analyses originales qui la remettent au cœur d’une dialectique de l’affectivité et de la représentation, de l’intersubjectivité et des rapports sociaux, de l’expérience individuelle et de l’expérience sociale. Cet ouvrage s’appuie sur des rencontres et des programmes de recherche pluridisciplinaires qui renouvellent l’approche de l’empathie, en examinent les mécanismes, les usages et la portée dans les sciences sociales, la philosophie morale et sociale, et la psychanalyse. Il constituera un instrument irremplaçable de découverte, d’approfondissement et de critique d’une notion qui a enfin trouvé toute sa place dans la pensée contemporaine.\"\n",
      "Initiales ornées\n",
      "Description:  Les sociétés coloniales à l'âge des empires : Afrique, Antilles, Asie, années 1850 - années 1950 : manuel et dissertations corrigées, + textes commentés L'ouvrage propose une synthèse sur les enjeux, les repères et les pistes de réflexion de ce sujet. Cette question de concours permet d’entrer dans une problématique historiographique : les métropoles au prisme de leur possessions ultramarines et leur interrelations, étudiées par le biais du colonial et de l’impérial. A la fin de chaque chapitre, des références bibliographiques permettent d’aller plus loin\n",
      "Initiales ornées\n",
      "Description:  L'articulation du sanitaire et du social La maîtrise des problèmes (la souffrance mentale comme la misère sociale) par un seul type de professionnels est illusoire. Il est essentiel que les dispositifs psychiatriques, sociaux, médico-sociaux avancent vers des modes de coopération qui tiennent compte, par-delà leurs logiques propres, de la réalité complexe et instable des besoins des personnes. II ne suffit pas de dénoncer le corporatisme des professionnels quand sont en cause la juxtaposition des législations et des règlementations, ainsi que le cloisonnement des modes d'organisation. La loi du 21 juillet 2009, dite loi HPST a déplacé les lignes de partage : désormais le secteur médico-social se détache du secteur social proprement dit et s'intègre dans le perimètre de la santé. Pour autant, la nécessité dépasser les logiques de cloisonnement pour donner un sens concret aux termes de partenariat, d'ouverture et de travail en réseau demeure. Cette nouvelle édition augmentée, qui tent cornpte des évolutions législatives, permet de se rendre compte chernin parcouru, mais aussi des obstacles qu'il reste encore à franchir.\n",
      "Initiales ornées\n",
      "Description:  Le cri d'Archimède : l'art de la Découverte et la découverte de l'Art Y a-t-il un lien entre la création littéraire et la découverte scientifique ? Entre ces dernières et l'inspiration comique ? Pour Arthur Koestler, ce lien se trouve dans ce qu'il nomme \"l'acte bisociatif\", autrement dit le bond novateur qui, en reliant soudain des systèmes de référence jusqu'alors séparés, nous fait vivre ou comprendre le réel sur plusieurs plans à la fois. Contribution fondamentale à la psychologie moderne, cette histoire des découvertes scientifiques se double d'un essai remarquable sur la création littéraire et artistique.\n",
      "Initiales ornées\n",
      "Description:  Dix façons d'assassiner notre planète Anthologie qui regroupe 10 nouvelles d'auteurs classiques de science-fiction, de Dick à Bordage autour d'un thème écologiste catastrophiste. Ces récits d'anticipation abordent les problèmes de climat, de pollution, de biodiversité et de nucléaire, en mettant l'accent sur leurs conséquences les plus redoutables et sur l'aspect primordial des actes du présent.\n",
      "Initiales ornées\n",
      "Description:  Urbanité et tourisme L'urbanisation et la diffusion du tourisme constituent depuis le XIXe siècle deux processus majeurs de développement des sociétés occidentales, puis mondiales. Ces processus entretiennent des relations à la fois d'interdépendance et de tension. Sortant des questions classiques du « tourisme urbain », ce numéro interroge les manifestations de l'urbain et de l'urbanité (centralité, accessibilité, diversité, accumulation de capital urbain, etc.). La question des relations avec les processus de développement touristique dans les métropoles, les villes moyennes et les stations touristiques est ainsi posée.\n",
      "Initiales ornées\n",
      "Description:  L'hygiène alimentaire Cet ouvrage propose un tour d'horizon de l'hygiène alimentaire. Des fiches synthétiques richement illustrées renseignent aussi bien sur les bases du fonctionnement de notre corps (digestion, aliments, nutrition, santé) que sur les aspects technologiques de l'alimentation. Elles donnent les clés d'une alimentation équilibrée. Les mises à jour indispensables sont apportées sur des sujets d'actualité : l'obésité chez les enfants, le 'bio', les OGM... [Source : 4e de couv.]\n",
      "Initiales ornées\n",
      "Description:  Israël entre quatre murs : la politique sécuritaire dans l'impasse «L’armée et la sécurité sont la véritable religion de ce pays», s’inquiète Gideon Levy, journaliste du Haaretz. Pour l’État d’Israël, la question du «vivre en sécurité» tourne, en effet, à l’obsession. Telle une «villa dans la jungle» ─ expression d’Ehoud Barak ─, il a développé une vraie culture de la forteresse assiégée. Pour se protéger, Israël se replie, s’enferme, mise sur sa force militaire, se lance dans des guerres préventives au nom de la «légitime défense», colonise les terres «incertaines» à ses frontières... À Jérusalem-Est, «judaïser» des quartiers devient ainsi synonyme de sécurisation. Mais cette politique pose question puisqu’elle s’avère incompatible avec l’existence et les droits des Palestiniens... On ne peut comprendre ce complexe sécuritaire sans se replonger dans l’Histoire de cette «nation» atypique. Le syndrome d’Amalek, ─ référence à une attaque contre le peuple juif à l’époque de Moïse ─, est souvent avancé comme le mythe fondateur du «bellicisme» israélien. Après un détour par l’époque biblique, l’auteur revisite surtout des pages d’Histoire récentes: la profonde blessure laissée par la guerre du Kippour (1973), le traumatisme des premiers attentats-suicide dans les années 1990, les échecs des interventions au Liban (2006) et dans la bande de Gaza (comme à l’été 2014), les cyber-attaques contre l’Iran, l’incidence des Printemps arabes... Ce tableau ne saurait toutefois être complet sans une analyse de la situation sociale interne ─ incontestable bombe à retardement ─ et de la radicalisation de l’opinion publique israélienne. Autant d’éclairages essentiels qui permettent de mieux appréhender la politique complexe de ce pays, tiraillé de toutes parts.\n",
      "Initiales ornées\n",
      "Description:  Millefeuille territorial et décentralisation : de la commune à la région : plaidoyer pour une réforme Les structures d'organisation du territoire français se sont empilées au fil des années et s'inscrivent dans une pyramide au sommet de laquelle se situent Paris, l'État et le Gouvernement. Ce système centralisé n'a fait que renforcer le déséquilibre entre l'Ile-de-France et les Régions. Des réformes territoriales ont pourtant été réalisées qui continuent d'affecter les communes regroupées dans des intercommunalités, les pays, les cantons, les agglomérations urbaines et, plus récemment, les départements, les régions et les métropoles. Pourtant ces bouleversements territoriaux n'on pas fait bouger le millefeuille territorial dont il faut réduire le nombre de couches et décentraliser le fonctionnement. L'auteur plaide pour une réorganisation des collectivités locales et régionales dans un nouveau partage du pouvoir politique et économique entre Paris, l'Ile-de-France et les Régions, faisant du pouvoir régional la voie de sortie du centralisme. Il formule 12 propositions qui constituent les pièces d'un nouvel édifice géopolitique à construire.\n",
      "Initiales ornées\n",
      "Description:  Géants des profondeurs Présentation du calmar géant, des mythes et légendes qui l'entourent, de son anatomie, sa vie dans les grands fonds, sa répartition géographique, etc.\n",
      "Initiales ornées\n",
      "Description:  L'habitat du Néolithique ancien de Colombelles : Le Lazarro, Calvados Depuis les années 1970, la composante orientale, en particulier celle de la céramique Linéaire, dans l origine du premier Néolithique normand n a cessé d être confortée. L intrigante découverte de la fameuse céramique de la Hoguette à Fontenay-le-Marmion (Calvados) constituait un premier signal. Plus récemment, les découvertes successives ont confirmé l existence de nombreux sites d habitat de la culture de Blicquy/Villeneuve-Saint-Germain.   La fouille du site de Colombelles montre aujourd hui que l Ouest du Bassin parisien  s inscrit bien dans la sphère de la colonisation rubanée à la fin du VIe  millénaire av. J.-C. Avec une dizaine d unités d habitation alignées sur une même rangée orientée nord-sud, ce village offre tous les signes d une transmission rigoureuse des normes culturelles rubanées, tant dans ses dimensions matérielles qu économiques ou   sociales. Seul l attrait pour de nouvelles ressources minérales, telles que l hématite oolithique ordovicienne, les silex jurassiques ou les schistes, distingue Colombelles des sites plus continentaux.  La situation isolée de ce site vers l ouest n est pas sans soulever des questions quant au rythme de l expansion rubanée à partir des zones colonisées antérieurement, cette progression étant difficilement compatible avec une colonisation agricole lente. La fin de la céramique Linéaire semble donc s accompagner d une avancée rapide des groupes villageois ; elle apparaît également comme une phase d ouverture  vers la sphère méridionale, comme en témoigne, à Colombelles, la présence de parures fabriquées dans des matériaux provenant des régions méditerranéennes.   L analyse spatiale tente ici de pallier le manque de données sur l architecture de la maison rubanée. Avec ses structures délicates à percevoir en milieu limoneux, la fouille pose également, plus globalement, la question de la détection de ce type d habitat, en particulier dans les zones de plateau.\n",
      "Initiales ornées\n",
      "Description:  Vive la retraite ! : une mine de sagesse que vous ne trouverez pas chez votre conseiller financier La clé d'une retraite active et satisfaisante requiert bien davantage que des conseils financiers appropriés. Elle englobe tous les autres aspects de la vie - des loisirs intéressants, des passe-temps créatifs, un bien-être physique et mental et des relations sociales épanouissantes. Vive la retraite vous apprend, entre autre choses, comment: Avoir le courage de prendre une retraite anticipée - Replacer les questions financières dans une juste perspective - Donner libre cours à votre créativité et découvrir des passions - Redevenir un éternel étudiant et cultiver votre curiosité - Concrétiser vos rêves d'hier et d'aujourd'hui - Prendre en main votre santé physique, mentale et spirituelle - Se faire de bons amis et les conserver. Un ouvrage au ton enjoué, aux illustrations amusantes, aux citations pertinentes et rehaussé d'une pincée de zen, qui aidera à bien apprivoiser l'une des périodes les plus exaltantes de la vie - à mille lieues de toutes les idées reçues, de tous les préjugés ... La retraite est une libération, pas une condamnation.\n",
      "Initiales ornées\n",
      "Description:  Grand Bassam : ville inscrite sur la Liste du patrimoine mondial : intervenir sur le site historique, conseils et prescriptions La ville de Grand Bassam a été inscrite sur la Liste du patrimoine mondial en 2012. Cette inscription marque l'engagement de l'Etat, de la commune et de l'ensemble de ses habitants à protéger le site. Préserver un site tel que Grand Bassam est un processus complexe, soumis à des contraintes fortes et des dynamiques contradictoires: un équilibre doit être recherché entre la réponse aux besoins légitimes de la population, l'accompagnement du développement économique et la préservation des valeurs culturelles. Face à la densification de la population, à la multiplication des investissements privés et au développement des infrastructures urbaines, le site historique risque de se dégrader très rapidement si la réglementation de protection du patrimoine n'est pas appliquée efficacement et comprise par tous. Des signes avant-coureurs de cette évolution - dégradation du bâti ou du couvert végétal - sont déjà perceptibles. Il est donc important de permettre une compréhension globale des valeurs culturelles du centre historique et une appropriation, par l'ensemble des acteurs et décideurs, de la réglementation patrimoniale, afin de créer une dynamique de restauration de la ville qui soit respectueuse de ses valeurs essentielles.\n",
      "Initiales ornées\n",
      "Description:  L'ONU dans le nouveau désordre mondial La 4e de couv. indique : \"Les défis auxquels nous devons faire face aujourd'hui, qu'ils soient économiques, géopolitiques, interreligieux, énergétiques ou climatiques, nécessitent plus que jamais des solutions globales élaborées par l'ensemble des pays de la planète : l'Organisation des Nations unies est la seule structure où une telle concertation est possible. Pourtant, soixante-dix ans après sa fondation, est-elle encore capable de rassembler dans un dialogue constructif petits États et grandes puissances afin de lutter contre les maux de l'humanité ? Dix observateurs des questions internationales, dont cinq secrétaires généraux des Nations unies, mais aussi des personnalités comme Noam Chomsky, livrent leurs réflexions dans cet ouvrage. Ils montrent qu'il est urgent de réformer une organisation qui, aujourd'hui, ne parvient plus que difficilement à incarner l'idéal démocratique, l'aspiration universelle à la paix et la défense des droits de l'homme qui motivèrent sa création en 1945. En ce début de XXIe siècle, l'ONU a en effet progressivement cédé la place à une série d'organisations internationales ou régionales, politiques ou économiques, nées au cours de ces dernières décennies, laissant ainsi trop souvent les pays les plus riches gouverner le monde. Relancer le multilatéralisme, redéfinir les opérations de maintien de la paix, renforcer les moyens de l'ONU... tout cela semble indispensable pour que le « Parlement des nations » ne se limite pas au rôle humanitaire auquel certains veulent le cantonner et occupe à nouveau le centre de la scène politique internationale.\"\n",
      "Initiales ornées\n",
      "Description:  L'âge, le désir et l'amour : un avenir pour l'intimité amoureuse Marie de HENNEZEL est psychologue clinicienne. Pionnière du développement et de la reconnaissance des soins palliatifs, elle a travaillé dix ans auprès des malades avant d'être chargée de mission au ministère de la Santé sur les questions de la fin de vie. Elle a reçu les insignes de chevalier de la Légion d'honneur. La sexualité et l'amour ne sont pas le monopole de la jeunesse ! La persistance d'une vie amoureuse érotique, quand on avance en âge, demande une évolution de la sexualité. Marie de Hennezel sonde avec finesse le mystère de ce nouveau chapitre de la vie. Au fil de ses rencontres, de ses lectures, de ses incursions sur des terres lointaines, comme celles du tantrisme ou des arts d'aimer de l'Orient, elle invite le lecteur à un voyage au cœur d'un territoire méconnu. Un livre qui ouvre de nombreux chemins. Ce livre a précédemment paru sous le titre : Sex and sixty, un avenir pour l'intimité amoureuse. Postface inédite de l'auteur\n",
      "Initiales ornées\n",
      "Description:  Mes basiques à coudre : avec patrons à taille réelle : 20 modèles pour femme du 36 au 44 Une jolie robe, un caraco aérien, une jupe élégante, une veste ou un pantalon raffinés, un short coloré... Découvrez ces 20 essentiels mode à coudre à avoir absolument dans votre dressing ! A la fois intemporelles et tendances, ces créations déclinées dans les incontournables tissus Frou-Frou sont agrémentées e noeuds, rubans ou fronces... des détails qui font toute la différence ! Les explications dessinées pas à pas accompagnées de patrons à taille réelle, de la taille 36 au 44, séduiront toutes les couturières, débutantes ou aguerries.\n",
      "Initiales ornées\n",
      "Description:  Les dangers naturels en Suisse : pratiques et développements : comptes rendus de la deuxième Journée de rencontre sur les dangers naturels (Université de Lausanne, 18 février 2011) Dangers hydrologiques, prévision des précipitations et des crues. Danger d'inondation, Analyse et gestion du risque, Instabilités rocheuses et glaciaires, Trajectographies et zonage du danger, Analyse quantitative du risque, Glissements de terrain, Gestion des risques liés aux dangers naturels: Parmi les contributions: Nadia Christinet, Christian Gerber, Claire-Anne Dvorak, Avancement du projet de réalisation de cartes de dangers naturels et de leur transcription dans l'aménagement du territoire, canton de Vaud - p. 355-362\n",
      "Initiales ornées\n",
      "Description:  La petite enfance dans la cour des grands : une politique et des métiers à redécouvrir L’accueil de la petite enfance est souvent considéré comme un « service à la population » qui irait de soi. Les professionnels y sont encore considérés comme des « nounous » qui réalisent des missions prosaïques : nourrir, changer, jouer. Or c'est tout le contraire : les crèches sont des lieux névralgiques du lien social. Elles jouent un rôle très important dans l’éducation, dans la construction des identités parentales et dans une économie, où les femmes aspirent à concilier emploi et vie familiale. Si on le veut vraiment, autour de ce lieu privilégié, bassin de paix et de confiance dans les quartiers, peut émerger une nouvelle dynamique du social. [Résumé éditeur]\n",
      "Initiales ornées\n",
      "Description:  Masterclass de photographie La 4e de couverture indique : \"Avec Tom Ang, participez à une stimulante masterclass de photographie pour découvrir comment mettre la technique au service de vos buts créatifs et développer une vision qui vous est personnelle. Assimilez les techniques de prise de vue et les savoir-faire propres aux dix domaines les plus appréciés de la photographie. Explorez différentes approches en vous inspirant des témoignages et des conseils de 20 photographes du monde entier et plongez dans les coulisses de leurs séances photo. Apprenez à analyser une photo et à identifier ses points forts afin de réaliser à votre tour des images percutantes.\"\n",
      "Initiales ornées\n",
      "Description:  Physiologie animale La 4e de couverture indique : \"L'ouvrage traite, de façon exhaustive, des différentes fonctions qui sous-tendent la vie des animaux. Chacune d'elles est appréhendée à l'échelle des organismes, dans ses manifestations visibles extérieurement (comportementales notamment), mais aussi à l'échelle des appareils et organes qui en sont les sites d'expression et à celle des cellules — avec une incursion pointue dans les signalisations intracellulaires mises en jeu, les intermédiaires moléculaires sollicités, les gènes qui en détiennent l'information et les contrôles qui s'exercent dans le cadre du maintien de l'homéostasie. Des contenus accessibles, actuels et exacts : L'ouvrage est conçu selon un format logique et est enrichi de multiples exemples. De l'information pertinente basée sur des découvertes récentes, ainsi que de nouvelles méthodologies de recherche ont été incluses dans chacun des chapitres. Des idées controversées et des hypothèses sont également présentées pour illustrer le fait que la physiologie est une discipline dynamique et évolutive. De la globalité à la spécificité : La physiologie est replacée dans le contexte de l'évolution animale, avec le souci permanent de mettre l'accent sur le fait que chaque concept est un élément incontournable qui s'intègre dans le sujet considéré dans sa globalité. Des efforts particuliers ont été faits pour assurer une lecture fluide avec des transitions aisées, un raisonnement logique et l'intégration d'idées clés et fondamentales à travers l'intégralité de l'ouvrage.\"\n",
      "Initiales ornées\n",
      "Description:  Lettre ouverte contre l'instrumentalisation politique de la laïcité « Nous sommes dix-huit hommes et femmes d’horizons différents, intellectuels, journalistes, universitaires de diverses disciplines (anthropologie, histoire, sciences de l’éducation, sociolinguistique, sociologie, philosophie), qui nous élevons contre l’utilisation frauduleuse de ce bien commun qu’est la laïcité. Accaparée à des fins électoralistes, elle ne peut en aucun cas servir de support idéologique au rejet et à l���exclusion. Elle ne peut pas être ce mur de séparation que d’aucuns aimeraient bâtir au sein de notre communauté nationale. Elle est un état social qui nous permet de vivre ensemble, au-delà de nos différences d’origine et d’options spirituelles. » avec Étienne Balibar, Pascal Blanchard, Philippe Blanchet, Jean-François Bruneaud, Rokhaya Diallo, Ahmed El Keiy, Antoine Hennion, Martine Janner-Raimondi, Pierre Merle, Edgar Morin, Benjamin Stora, Pascal Tisserant, Laurent Visier, Christiane Vollaire, Geneviève Zoïa\n",
      "Initiales ornées\n",
      "Description:  Ressources humaines : outils & méthodes de management des RH Proposant une approche inédite des ressources humaines, ce livre présente les outils de management les plus pertinents qu'utilisent les grands groupes et montre comment les adapter et les mettre en œuvre dans toutes les entreprises. Il part du constat que la gestion des ressources humaines est un processus autonome et transversal impliquant la seule ressource de l'entreprise qui l'est également intrinsèquement : le capital humain. De ce fait, la mise en œuvre du processus des ressources humaines requiert la maîtrise de nombreux outils de management différenciés qui convergent et constituent un tout unifié au service de la performance économique et sociale. S'appuyant sur ce postulat, ce guide RH explique non seulement le processus lui-même, sa philosophie et son fonctionnement, mais détaille exhaustivement les méthodes et outils utilisés dans les organisations qui ont intégré cette dimension essentielle de la fonction ressources humaines. [Source : 4e de couv.]\n",
      "Initiales ornées\n",
      "Description:  Palmyre : histoire et archéologie d'une cité caravanière à la croisée des cultures La 4e de couv. indique : \"La tragique actualité syrienne a ravivé le mythe de Palmyre. Il est devenu urgent de redécouvrir les grandeurs de cette cité gréco-romaine d'Orient aux confins des mondes romain et parthe, puis perse. Dans cette synthèse historique et archéologique inédite, Christiane Delplace, l'une des meilleures spécialistes du site, met des images sur les mots, fait resurgir les monuments, nous donne à voir l'histoire politique et culturelle. En réunissant une iconographie rare, l'auteur nous guide dans la suite des histoires palmyréniennes et nous livre les différentes facettes de ce trésor antique. Les grandes étapes historiques de la ville y sont contées de la manière la plus précise : Palmyre avant les Romains et le raid d'Antoine, la montée en puissance de la ville au Ier siècle, son apogée au IIe jusqu'à la percée des périls au IIIe siècle, et son déclin. À chacune de ces étapes, l'auteur nous fait sentir la personnalité de la ville, à travers la place des cultes, du commerce, de la guerre ou des morts. Les monuments sont expliqués, commentés, datés et replacés dans leur contexte. Le sanctuaire de Bêl, les temples, la grande colonnade, les monuments de spectacles, l'habitat ou les tombeaux de Palmyre se relèvent. Les caravanes et les tribus palmyréniennes renaissent. Palmyre scintille à nouveau\"\n",
      "Initiales ornées\n",
      "Description:  La fabrique des classiques africains : écrivains d'Afrique subsaharienne francophone (1960-2012) La 4e de couv. indique : \"Comment une oeuvre littéraire accède-t-elle au rang de \"classique\" lorsque son auteur est issu d'Afrique subsaharienne francophone, l'une des zones les plus déshéritées du monde selon les standards culturels internationaux? Si les noms de Léopold Sédar Senghor et d'Ahmadou Kourouma se sont imposés partout, pourquoi d'autres auteurs, portés au pinacle en Europe, restent-ils peu connus dans leurs pays d'origine, quand les textes d'Aminata Sow Fall et de Seydou Badian, étudiés et discutés au Sénégal et au Mali, ne le sont pas en France ? Ce livre propose une histoire sociale collective de ces écrivains depuis1960. II distingue deux protagonistes majeurs : des intermédiaires culturels (organisateurs de festival, éditeurs, agents littéraires), souvent français, et des auteurs nés et socialisés en Afrique subsaharienne francophone, dont les trajectoires sont situées les unes par rapport aux autres dans un espace littéraire africain en recomposition. Nourri de nombreux entretiens, fondé sur le dépouillement d'archives inédites et sur une étude statistique, cet ouvrage majeur décrit par quels mécanismes symboliques et matériels des oeuvres d'écrivains originaires d'Afrique subsaharienne francophone sont devenus, sous différentes formes, des classiques africains.\"\n",
      "Initiales ornées\n",
      "Description:  Le vote FN au village : trajectoires de ménages populaires du périurbain La 4e de couverture indique : \"Les ménages modestes du périurbain sont devenus, au fil des succès électoraux du FN, une figure centrale des commentaires médiatiques. Peu d'enquêtes sérieuses se sont pourtant intéressées à leurs conditions de travail, à leurs parcours résidentiels et aux devenirs de leurs enfants. Basé sur un travail de terrain de longue durée, ce livre restitue les trajectoires des salarié-e-s d'un parc d'activités comme il en existe beaucoup à la périphérie des grandes villes. A partir d'extraits d'entretiens et d'observations, il revient sur les transformations qui affectent depuis plusieurs décennies les fractions stables des milieux populaires : restructurations dans l'industrie et les services, durcissement des conditions d'accès à la propriété et évolution du système de formation. Même si leurs pratiques électorales s'avèrent plus diversifiées qu'on ne le pense, le FN attire une part de ces ménages du périurbain, qui sont pourtant loin de tous figurer parmi les plus démunis. Au-delà des explications convenues, notamment celles du déclassement, l'ouvrage offre ainsi des clés de compréhension des processus de radicalisation politique. Il montre enfin comment des élus municipaux, sans être encartés au FN, s'emploient très concrètement à empêcher la venue de familles issues de l'immigration et de ménages en situation précaire, contribuant par-là à banaliser le rejet de ces catégories sociales stigmatisées. Dans un contexte de hausse de l'abstention et de défiance envers les responsables politiques, ce livre éclaire les aspirations et les tensions vécues par nombre de ménages populaires.\"\n",
      "Initiales ornées\n",
      "Description:  Quintessence de soi : j'active la luminescence de mon être Portons un nouveau regard sur nous et sur tout ce qui nous entoure. Agissons avec simplicité. Accueillons le meilleur dans notre vie. Dévoilons notre diamant intérieur et libérons notre Quintessence. L'auteur nous propose d'effeuiller notre être au rythme de notre lecture. Quintessence de soi – J'active la luminescence de mon être est un ouvrage empreint de maturité et de sagesse. Un trésor inestimable d'enseignements pour révéler la Luminescence de notre être. Liliane Bassanetti est l'auteur des livres à succès : Sois-Toi ! et Les enfants de la Terre d'Émeraude Indigo, Cristal, Arc-en-ciel et Doré !\n",
      "Initiales ornées\n",
      "Description:  Les couples illustres de l'histoire de France La 4ème de couv. indique : \"Si le pouvoir tend à se conjuguer au singulier, il en va autrement pour la célébrité qui s'incarne mieux à deux. Le premier repose sur le temps long et le secret alors que la seconde est fondée sur l'exposition et le perpétuel présent de l'actualité. Ce livre collectif raconte pour la première fois l'histoire publique et privée des vingt couples emblématiques qui ont incarné et, pour beaucoup, fait la France du Moyen Age à nos jours. Couples politiques – monarchiques, impériaux puis présidentiels – mariant les sentiments, parfois, et l'intérêt ; couples littéraires et/ou d'influence, couples mythiques ou médiatiques, passionnels ou énigmatiques... S'ils ont chacun une histoire singulière, heureuse ou malheureuse, tous racontent les mutations successives du pouvoir mais aussi de la société. Sous la direction de Patrice Gueniffey et Lorraine de Meaux, historiens et journalistes de renom les font revivre dans des contributions qui allient l'exigence historique et la fluidité littéraire, l'histoire intime et le roman national.\"\n",
      "Initiales ornées\n",
      "Description:  Les grandes dates de l'histoire économique et financière de la France En 1988, le ministre de l'Économie et des Finances installait le Comité pour l'histoire économique et financière de la France (CHEFF) dont le secrétariat scientifique est aujourd'hui assuré par le Bureau de la Recherche de l'Institut de la gestion publique et du développement économique. La mission impartie au CHEFF est de développer la recherche en histoire économique. Il recueille et préserve la mémoire des ministères économiques et financiers et promeut une meilleure connaissance de l'évolution de l'État dans ces domaines particuliers. Afin de célébrer les trente ans du Comité, ses membres (universitaires et hauts fonctionnaires) ont choisi 56 dates qui symbolisent les grandes transformations de l'économie, de la monnaie, de la fiscalité et des finances publiques de notre pays. Du IXe siècle jusqu'à aujourd'hui, l'innovation a été au coeur de ces transformations. L'État en gestation a, au Moyen Âge, cherché à financer la guerre, à organiser l'espace territorial et économique, à combattre les fléaux sanitaires. À l'époque moderne, l'État royal s'organise pour se réformer et solliciter le plus de contributions possibles à l'effort de guerre. Les privilèges tombent lors de la nuit du 4 août 1789 et la souveraineté devient nationale. Au XIXe siècle après les désordres de la Révolution il faut réorganiser les finances publiques ainsi que l'économie à travers les réseaux bancaires. Une nouvelle société émerge avec l'apport d'une ébauche de protection sociale. Le développement des transports et des moyens de communication ouvrent de vastes perspectives commerciales. Le XXe siècle est celui des grands conflits mondiaux. Il voit la naissance de l'État-providence et d'une organisation économique plus complexe. La reconfiguration des espaces commerciaux conduit la France à s'ancrer dans la Communauté économique européenne. À travers ces 56 dates, les auteurs nous content l'évolution de l'État en lien avec notre économie [source : 4e de couv.]\n",
      "Initiales ornées\n",
      "Description:  Défi rangement : 1 mois pour alléger votre vie Objectif : 1 mois pour tout ranger, libérer votre maison et votre esprit ! Faire du vide dans la maison pour retrouver son calme intérieur. Le thème du rangement, un grand classique. Le rangement et le minimalisme, deux tendances qui s'accordent parfaitement pour faire du tri efficacement, ne plus dépenser trop, ne plus accumuler sans limites et ne plus se sentir oppressé à la vue de ces tas de babioles futiles. Un plan détaillé pour s'organiser pas à pas et se débarrasser du superflu au fur et à mesure.\n",
      "Initiales ornées\n",
      "Description:  Quiches, tartes & pizzas : recettes inratables Vous ne jurez que par des quiches et tartes toutes prêtes  ? Et vous vous demandez comment diable font les autres pour réussir des quiches maison parfaites et originales  ? C'est en fait très simple, et nous allons vous le prouver !\n",
      "Initiales ornées\n",
      "Description:  Preuves d'éternité Que se passe-t-il vraiment quand nous mourons  ? L'âme survit-elle au corps  ? Retrouvons-nous ceux que nous aimons  ? Peut-on communiquer avec les défunts  ? Depuis la nuit des temps, des scientifiques et des théologiens ont tenté de répondre à ces questions existentielles.En s'appuyant sur leurs découvertes, l'auteur redéfinit complètement notre perception de la mort. Il nous entraîne dans une aventure vertigineuse en réunissant toutes les preuves de l'existence de l'âme dans l'au-delà.Grâce aux dernières découvertes scientifiques, la vie après la mort n'est plus une simple hypothèse. En répondant à des questions essentielles, ce livre nous aide à mieux comprendre la mort et à ne pas en avoir peur.Au-delà de la vie  : les preuves scientifiques.\n",
      "Initiales ornées\n",
      "Description:  Associations : fondations, congrégations, fonds de dotation : 2018 La 4e de couverture indique : \"Le secteur associatif c'est 1,3 million d'associations (65000 nouvelles chaque année). Le secteur non lucratif ce sont aussi des fonds de dotation (en plein essor), des fondations et des congrégations. Grandes ou petites, d'utilité publique ou non, sportives ou culturelles... seul le Mémento Associations peut répondre à leurs questions juridiques, fiscales, sociales ou comptables. Il analyse non seulement les règles communes mais aussi les règles propres à chaque type d'associations. Une partie substantielle des développements est consacrée aux fondations reconnues d'utilité publique et aux fondations d'entreprise. Des dossiers thématiques complètent l'ouvrage : mécénat et parrainage, subventions, statut des bénévoles, Alsace-Moselle, manifestations publiques, etc. Conçu dans un esprit résolument pratique, le Mémento Associations rend les meilleurs services à l'ensemble des dirigeants et membres d'associations ainsi qu'aux juristes et praticiens (experts comptables, avocats, etc.) qui les conseillent. Au carrefour de toutes les branches du droit, le droit associatif est en perpétuelle évolution ! Le Mémento aussi !\"\n",
      "Initiales ornées\n",
      "Description:  Le contrat naturel La 4e de couv. indique : \"En 1990 paraissait ce \"brûlot\" qui proposait d'élever la Nature au. rang de sujet de droit. L'état de violence \"sans limites\" entre l'Homme et le Monde appelait L'élaboration d'un nouveau, droit, à fonder sur un Contrat naturel qui compléterait le Contrat social établi, entre les hommes. Cette solution, juridique, commence à pénétrer les textes législatifs de nombreuses nations, y compris ceux de la France, où l'on parle enfin de citer quelques éléments de nature dans la Constitution. Faut-il donc trente ans pour qu'une idée nouvelle, en n'effarouchant plus, devienne raisonnable ? 2018. Notre monde, est confronté à une urgence environnementale sans précédent. Mais, hélas, les puissants de la planète ne se décident pas à signer avec le monde qui nous accueille ce Contrat naturel qui nous permettrait de vivre en symbiose. Au secours ! Nous sommes tous concernés !\n",
      "Initiales ornées\n",
      "Description:  Bienvenue chez les Danois ! : un an pour découvrir les secrets du pays le plus heureux du monde Le Danemark, c'est bien plus que le hygge ! Alors que l'occasion lui est donné de démarrer une nouvelle vie dans le Jutland rural, la journaliste britannique Helen Russell apprend avec stupéfaction que les habitants du pays des hivers interminables, du hareng mariné et des Lego sont considérés comme les plus heureux sur Terre. Mais quel est donc le secret des Danois ? Leur bonheur est-il inné ou acquis ? Helen se donne une mission, et un an pour l'accomplir : découvrir la formule du bonheur à la danoise. Garde d'enfants, éducation, nourriture, décoration d'intérieur, impôts, mais aussi un certain sexisme et une malheureuse inclinaison pour la chasse aux sorcières... Bienvenue chez les Danois ! est le récit bourré d'humour d'un voyage qui révèle là où les Danois font bien les choses, là où ils se trompent, et comment nous pourrions tous vivre un peu plus « danoisement ».\n",
      "Initiales ornées\n",
      "Description:  La justice prédictive La 4e de couv. : \"L'idée de prédiction appliquée aux décisions judiciaires a été de longue date revendiquée par les grands théoriciens américains du droit. Toutefois, la puissance des traitements informatiques et l'intelligence artificielle la rendent dorénavant crédible à grande échelle. Si les décisions judiciaires et à travers elles les comportements des individus ou des entreprises sont en masse analysé, structurés, corrélés, le législateur, les juges, les experts, les enquêteurs ne se verront-ils pas incités à respecter ces nouvelles \"normes numériques\" et les justiciables tentés d'éviter la \"case procés\" par la négociation de gré à gré sur des plateformes ? Ce volume rassemble plus d'une vingtaine de contributions d'universitaires et de praticiens, magistrats civils et administratifs, avocats, entrepreneurs, qui recentrent le concept de justice prédictive, en montrent les limites théoriques et pratiques mais aussi dessinent les opportunités, sous des conditions juridiques et techniques précises propres à chaque branche du droit, qu'offrent le traitement de masse des données juridiques pour un droit et une justice à la fois plus efficaces et plus équitables.\"\n",
      "Initiales ornées\n",
      "Description:  Les oscillations de Joseph Fourier : en BD Référence pour la communauté scientifique mais personnalité peu connue du grand public, Joseph Fourier est un mathématicien qui vécut une existence aux mille rebondissements. Après s'être engagé dans la Révolution française, il explore l'Egypte lors des expéditions napoléoniennes et fonde l'Université de Grenoble... Musique, photographie, laser ou télécommunications sont autant de domaines que nous maîtrisons aujourd'hui grâce à Joseph Fourier\n",
      "Initiales ornées\n",
      "Description:  L'alliance contre-nature : quand les religions nourrissent le populisme La 4e de couverture indique : \"Comment des religions, qui prônent l'amour des ennemis, peuvent-elles composer avec le populisme, qui déchaîne l'exclusion et la haine? En réponse à cette question, cet essai dévoile les contradictions internes d'un «populisme chrétien» et dénonce la dialectique agressive créée conjointement par le repli identitaire chrétien en Occident et les intégrismes islamistes en Orient.\"\n",
      "Initiales ornées\n",
      "Description:  Manuel d'analyse des politiques publiques à l'usage des ingénieurs et des urbanistes : exemples dans le domaine de l'eau et de l'environnement Qui est le public concerné par un aménagement ? Peut-on en présenter les enjeux sans les politiser ? Est-ce que les non-experts sont légitimes sur les sujets techniques ? Pourquoi certains discours portent plus que d’autres ? Qu’est-ce qui fait le succès d’une décision ? Les ingénieurs et les urbanistes qui ont affaire à des décisions publiques expriment souvent le besoin d’être accompagnés dans leur réflexivité pour comprendre ce qu’il se joue au niveau politique et comment ils interviennent eux-même dans ce contexte. Ce manuel leur est destiné. Il offre des clés d’analyse sur la construction des problèmes publics, la conception, l’adoption et la mise en œuvre des politiques publiques en France. La sociologie de l’action publique est expliquée ici de manière simple et mise en contexte par de nombreux exemples pris dans le domaine de l’eau et de l’environnement.\n",
      "Initiales ornées\n",
      "Description:  Un  lion sur le canapé : comment les chats ont pris le pouvoir Partout, on trouve des chats ! Ils sont même sur Internet, où certaines vidéos qui les mettent en scène sont vues plus de dix millions de fois. Comment les chats ont-ils donc réussi à dominer le monde, alors qu'ils ne sont pas utiles à l'homme - pas même pour attraper les rats !Abigail Tucker, journaliste au Smithsonian Magazine, est allée à la rencontre d'éleveurs, de scientifiques et de militants qui ont consacré leur vie aux chats, elle a sillonné des étendues sauvages sur leurs traces et a même rencontré certaines des plus grandes célébrités félines afin de saisir la manière dont les chats ont conquis le monde et nos coeurs.En la suivant dans cette aventure haute en couleur à travers l'histoire, les sciences naturelles et la culture populaire, vous comprendrez comment ces petites créatures ont su utiliser leur relation avec les humains pour compter parmi les animaux les plus puissants de la planète. Best-seller du New York Times, en tête des ventes aux États-Unis pendant plusieurs semaines, ce livre vous permettra aussi de mieux connaître votre animal préféré.\n",
      "Initiales ornées\n",
      "Description:  Grands arrêts du droit de la concurrence : 2004-2018 Ce troisième tome des Grands arrêts du droit de la concurrence couvre la pratique décisionnelle et la jurisprudence en matière de pratiques commerciales déloyales et de contrats de distribution. L'ouvrage réunit plus de 120 commentaires de décisions européennes et nationales parus dans la revue Concurrences de 2004 à 2018. Le tome premier des Grands arrêts couvre les pratiques anticoncurrentielles (Art. 101, 102 TFUE) et le tome second, les concentrations.\n",
      "Initiales ornées\n",
      "Description:  Les hostas : les meilleurs choix, les plus beaux feuillages, tous les conseils pour les cultiver La 4ème de couverture indique : Les Hostas comptent parmi les plantes les plus faciles à cultiver dans nos jardins. Pourquoi ? Parce qu'elles n'exigent qu'un minimum de soins et qu'il n'est pas nécessaire de les diviser. De plus ce sont des plantes d'ombre, bien qu'il existe aujourd'hui des variétés qui tolèrent l'ensoleillement. Dans ce livre, le premier du genre en français, retrouvez plus de 200 cultivars vendus dans la plupart des pépinières. Découvrez tous les secrets de leur culture et de leur entretien et apprenez à les intégrer harmonieusement dans l'aménagement de vos plates-bandes. L'auteur révèle ici ses coups de cur, présente des listes par catégories et illustre son propos avec plus de 450 photos en couleurs. Les hostas s'imposent assurément comme un ouvrage de référence pour tous les amateurs de jardinage\n",
      "Initiales ornées\n",
      "Description:  Droit général de l'environnement : introduction au droit de l'environnement La 4e de couv. indique : \"L'ouvrage présente les fondements du droit de l'environnement en un cours élémentaire. Simple et précis, étayé par des extraits de textes et de jurisprudence, enrichi de la doctrine française et étrangère, il aborde l'essentiel du droit de l'environnement en situant ce droit vital parvenu à maturité dans ses dimensions théorique et juridique. L'objectif premier est de le comprendre pour l'aimer. Il s'agit de donner la grammaire de cet ordre juridique, en montrant les logiques et les articulations à l'oeuvre en droit international et européen de l'environnement, comme en droit public, pénal, privé et fiscal de l'environnement. Le parti est pris d'une pédagogie démonstrative faite de méthode et d'analyses claires, renforcée par des synthèses bibliographiques sélectives, un index raisonné, une table des jurisprudences exposées et des encadrés qui mettent en lumière des points cruciaux. Le livre s'adresse d'abord aux étudiants. Il est aussi destiné aux praticiens, aux institutions et aux citoyens\"\n",
      "Initiales ornées\n",
      "Description:  Fantasy et histoire(s) : colloque des Imaginales L a 4e de couverture indique : \"La fantasy, ou \"l'histoire-fiction\" ? Ce livre rassemble les actes du premier colloque universitaire organisé dans le cadre du Festival des Imaginales, entièrement consacré à la fantasy sous l'angle de ses rapports avec l'Histoire. Seize articles de spécialistes s'intéressent aussi bien aux romans (Eddison, Tolkien, Le Guin, Rowling) qu'aux formes audiovisuelles ou encore au vaste domaine des jeux de rôle et des reconstitutions historiques. Érudit mais très accessible, il donne un éclairage nouveau sur ce genre, actualisant sa place dans les littératures de l'imaginaire\"\n",
      "Initiales ornées\n",
      "Description:  Droit public et patrimoine : le rôle du Conseil d'État la 4e de couv. indique : \"L'histoire du droit du patrimoine est le plus souvent comprise sous l'angle soit des textes principaux, soit du rôle de l'Etat et de ses services. Si ces derniers sont des acteurs centraux dans le projet de conservation et de transmission du patrimoine culturel, la matière est aussi le lieu d'un dialogue entre le juge, l'administration et le législateur. Cet ouvrage a pour objet d'interroger, dans une double perspective historique et juridique et à partir de l'exploitation de ses archives, la contribution du Conseil d'Etat à l'édification de ce droit, tant au travers de sa fonction juridictionnelle que dans son rôle de conseiller du gouvernement. Les auteurs se sont intéressés ici à l'institution mais aussi à l'histoire sociale et aux conseillers d'Etat qui l'ont marquée de leur empreinte. Sont aussi évoquées des expériences étrangères, en particulier celles du Royaume-Uni et de l'Italie. Ce colloque a été organisé par les Comités d'histoire du ministère de la Culture et du Conseil d'Etat en partenariat avec l'Institut des sciences sociales du politique.\"\n",
      "Initiales ornées\n",
      "Description:  Maths [concours 2020] : admissibilité, écrit Une préparation à l'épreuve d'admissibilité de mathématiques du concours de recrutement de professeur des écoles : cours, exercices et sujets corrigés, conseils méthodologiques ou encore annales corrigées.\n",
      "Initiales ornées\n",
      "Description:  Le photojournalisme peut-il sauver la presse  Hubert Henrotte est le créateur avec quatre confrères de l'agence de photojournalisme, GAMMA et SYGMA qu'il a dirigées pendant plus de 30 ans. Il revient sur la période où le photographe s'alliait au journalisme, sur l'identité du métier, sur la nécessité de créer une agence. Il raconte aussi les impératifs du métier de photographe qui trouve le terrain par lui-même. Depuis, l'époque a changé et ce regard en arrière permet aujourd'hui de se redéfinir dans ce métier qui semblerait en voie d'extension. Il n'est pas le seul à se soucier du métier de photojournaliste et de l'avenir de la presse. Car les dirigeants de la presse et les photographes célèbres qu'il a interviewés ici avec la collaboration de Floris de Bonneville lui donnent leur vision de la situation actuelle et expliquent chacun leur façon de fonctionner dans leurs collaborations ou livrent leur regard sur le métier tel qu'il est aujourd'hui. Soucieuses de préserver une éthique du métier mais aussi son économie, qui, comme dans le passé, devait pouvoir les faire vivre, les personnes interviewées permettent de mieux connaître l'envers du décor d'un métier complexe, parfois au péril de la vie de celui qui l'exerce. L'auteur s'interroge sur l'avenir et fait des propositions, là aussi appuyées par les interviews des différents acteurs du photojournalisme et de la communication. L'ouvrage est une référence pour établir un état des lieux dans le milieu de l'information et pour proposer des réponses aux questions d'actualité.\n",
      "Initiales ornées\n",
      "Description:  Faut-il sortir du nucléaire  Les clés pour tout comprendre au débat sur le nucléaire ! Pourquoi la France a-t-elle eu recours au nucléaire ? Quels sont les risques environnementaux du nucléaire ? Le nucléaire coûte-t-il vraiment moins cher ? Peut-il sauver le climat ? Autant de questions décisives qui ponctuent le débat sur le nucléaire depuis quelques années, et auxquelles la journaliste Géraldine Woessner répond d'une manière simple, pédagogique et impartiale, en se fondant sur des données incontestables. Comprendre les enjeux, se faire une opinion. [Source : 4e de couv.]\n",
      "Initiales ornées\n",
      "Description:  Lars Fredrikson : [exposition Musée d'art moderne et d'art contemporain, Nice, 16.11, 2019-22.03, 2020] Première grande monographie consacrée à l'œuvre radicale du Suédois, peintre, dessinateur, sculpteur et précurseur des arts sonores (CD audio inclus). Publié à l'occasion de l'exposition rétrospective de Lars Fredrikson au Musée d'Art Moderne et d'Art Contemporain à Nice, organisé en collaboration avec le Nouveau Musée National de Monaco et le fils de l'artiste, cet ouvrage est la première monographie consacrée à l'œuvre radicale du Suédois, peintre, dessinateur, sculpteur et précurseur de l'art sonore (1926-1997). Au cours de sa longue carrière, l'artiste a créé un vaste corpus multidisciplinaire visant à remettre en question notre perception – les frontières entre le visible et l'invisible, la présence et l'absence, l'intérieur et l'extérieur. Cette publication s'intéresse en particulier à l'approche non linéaire de l'art prônée par Lars Fredrikson, son exploration corporelle des vibrations et son objectif de saisir les ondes et les fréquences invisibles qui nous entourent.\n",
      "Initiales ornées\n",
      "Description:  Lino-gravure inspirée de la nature : les techniques & plus de 80 modèles Apparue à la fin du XIXème siècle cette technique est facile, rapide et gratifiante. Le linoleum (composé de poudre de liège, d'huile de lin, de gomme et de résine compressés sur une toile de jute) est plus tendre et plus facile à graver que le bois. Il permet un tracé souple, précis et sa surface lisse donne une qualité optimale lors de l'impression. Ce livre vous propose plus de 60 modèles conçus pour être associés vous permettant d'exprimer toute votre créativité. La linogravure donnera à vos réalisations un rendu de caractère. • Découvrez toutes les techniques de base en pas à pas photographiques et vidéos. • Comprenez facilement les différentes étapes, taille, encrage et impression. • Associez plus de 60 modèles pour créer d'innombrables motifs originaux Auteur : Florie Nguyen Van Florie Nguyen Van est une artiste de talent, diplômée de Penninghen (école d'art graphique). Elle est la fondatrice, la créatrice et illustratrice de la marque de papeterie évènementielle, «2 fois oui». Spécialiste de l'illustration en linogravure elle a exercé son talent dans le dessin animé (Arte), l'industrie textile et l'édition. Elle nous offre ici la possibilité d'explorer une nature magique et luxuriante.\n",
      "Initiales ornées\n",
      "Description:  Le travail du care A partir d'une enquête menée dans une maison de retraite de la région parisienne en 2012, P. Molinier étudie l'activité des travailleuses du care (aides-soignantes, infirmières, brancardières, etc.) et les questions politiques et éthiques qu'elle pose à la société. La préface de cette édition apporte des réponses aux polémiques sur le care qui se sont développées au cours des années 2010.\n",
      "Initiales ornées\n",
      "Description:  Comptabilité À quoi sert la comptabilité ? Comment l'information financière est-elle élaborée ? Comment lire les états comptables ? Quelles sont les obligations comptables des entreprises et des groupes ? Alliant théorie et pratique, ce manuel met l'accent sur l'acquisition des méthodes et des compétences indispensables à tout étudiant pour réussir sa licence ou son bachelor. Il propose : - des situations concrètes pour introduire les concepts ; - un cours visuel et illustré par des exemples pour acquérir les connaissances fondamentales en comptabilité ; - des conseils méthodologiques et des éclairages professionnels pour traduire la théorie en pratique ; - des exercices progressifs et variés (QCM, mises en situation, sujets d'examen) et leurs corrigés détaillés pour s'évaluer et s'entraîner. Un chapitre en anglais sur les états financiers anglo-saxons et un lexique en fin d'ouvrage permettent de se familiariser avec le vocabulaire anglais utilisé en entreprise\n",
      "Initiales ornées\n",
      "Description:  Atlas géopolitique du monde global : 100 cartes pour comprendre un monde chaotique Dans un monde globalisé, rien de ce qui se passe sur la planète ne peut nous être réellement étranger. Attrition des ressources, pandémie du Covid-19, compétitions commerciales, bras de fer sino-américain, enjeux écologiques, rivalités multidimensionnelles : les multiples aspects de la géopolitique structurent notre existence. En 100 cartes claires et détaillées, Pascal Boniface et Hubert Védrine jettent un regard inédit et englobant sur la planète, vue par ses principaux acteurs comme la Chine, les États-Unis, l’Europe, mais aussi l’Afrique du Sud, le Brésil, l’Indonésie… Avec la clarté et les qualités pédagogiques qui leur sont reconnues, ils décryptent un monde complexe qu’ils rendent accessible à tous et montrent qu’une des clés de l’avenir est de prendre en compte la diversité des points de vue selon les pays et les peuples. La remarquable cartographie de Jean-Pierre Magnier renforce la limpidité de leur propos. Salué par la critique et immédiatement adopté par le public dès sa première édition, l’Atlas géopolitique du monde global s’affirme comme une référence désormais incontestable.\n",
      "Initiales ornées\n",
      "Description:  Mocktails book : 35 cocktails sans alcool surperfrais pour l'été : des cocktails festifs et sains à boire sans modération Parfois connu sous le nom de virgin cocktail ou de spirit free, le mocktail désigne un cocktail qui ne contient pas d'alcool. Mais qui dit sans alcool ne dit pas sans saveurs !Car oui, les mocktails peuvent être tout aussi inventifs, originaux et sophistiqués que leurs cousins alcoolisés… et ont l'avantage de pouvoir être bus sans modération !Fruités, acidulés, fleuris, il y en a pour tous les goûts et de toutes les saveurs ! Plongez donc sans hésiter dans 35 recettes fraîches, gourmandes et surprenantes à déguster jusqu'à plus soif !\n",
      "Initiales ornées\n",
      "Description:  Haute curiosité « Le thème de la comédie jouée chaque jour à Drouot, comme dans toute salle des ventes, c'est quelque chose comme “l'objet mis en jugement” et “ouragan sur la brocante”. Stimuler les passions, énumérer les chiffres, donner des coups de maillet fut mon occupation pendant plus de trente ans. Une sorte de juge de paix chargé de tenir la balance égale entre celui qui désire se défaire au plus haut prix de son objet et le voisin qui brûle de l'acquérir pour pas cher. “Pour rien.” Ainsi dira-t-on d'un Van Gogh adjugé seulement 100 millions, comme on peut affirmer de la même œuvre qu'elle est sans prix. C'est Babel et la confusion des langages. » Ainsi s'exprimait l'auteur de ce livre, mi-mémoires mi-réflexion sur le métier de commissaire-priseur. Maurice Rheims exerça cette profession pendant plus de quatre décennies avec un talent souverain. Haute Curiosité fut salué en son temps par de bons esprits comme un chef-d'œuvre à l'écriture brillante, où l'anecdote toujours savoureuse abonde et où l'humour n'est jamais absent.\n",
      "Initiales ornées\n",
      "Description:  Jouissez jeunesse ! : Petit manuel à l'attention de ceux qui choisiraient de ne pas croire à la fin du monde Comment tenir un discours raisonné, progressiste et humaniste  sur le climat et l'environnement sans être écrasé par le fanatisme  vert ?La dérive collapsologique peut aujourd'hui nous faire perdre la  guerre technologique. À force de refuser l'innovation au nom  de la préservation supposée de la nature, nous prenons le risque  de sortir de l'Histoire face à la Chine et aux géants américains.  La France doit se débarrasser de ses fausses croyances et faire  de nouveau confiance au progrès.C'est bien à la jeunesse qu'il revient de réaliser cela, sans céder  à l'alarmisme écologique qui conduirait à la violence politique.Nous ne sommes pas à la veille de la fin du monde, l'aventure  humaine ne fait que commencer !\n",
      "Initiales ornées\n",
      "Description:  L' amour, toujours, l'amour... Quel thème plus approprié que celui de l’amour pour s’engager dans une nouvelle année, après des mois de distanciation sociale et d’effacement des corps ? Alors qu’on use du même mot pour désigner des sentiments, des attitudes et des comportements tellement divers, ce dossier tente de définir les différentes formes que peut prendre l’amour et la manière dont la littérature pour la jeunesse s’empare du thème. Parler d’amour, c’est parler de passion, de sensualité, d’attachement, de sexualité, d’amitié, d’empathie, d’altruisme mais aussi de rupture, de séparation, de chagrin, autant d’expériences présentes aux différentes étapes de l’enfance [Source : http://cnlj.bnf.fr/fr/page-editorial/la-revue-des-livres-pour-enfants]\n",
      "Initiales ornées\n",
      "Description:  Droit processuel européen : procédures devant la Cour de justice, le Tribunal et la Cour européenne des droits de l'homme Le droit de l'Union européenne et le droit de la Convention européenne des droits de l'homme façonnent notre quotidien autant, si ce n'est plus selon les domaines, que le droit national. Ces droits, pour qu'ils soient opérationnels, doivent être intégrés à la stratégie juridictionnelle des acteurs associatifs, économiques ou juridiques nationaux dès le départ des procédures nationales. Cette anticipation procédurale s'appuie sur une très bonne connaissance des règles régissant les recours possibles devant la Cour européenne des droits de l'homme (requête individuelle) et à la Cour de justice de l'Union européenne (renvoi préjudiciel, recours en manquement d'Etat, recours en annulation ou en carence...). Cet ouvrage vous présentera l'organisation, le fonctionnement et l'activité de la Cour EDH, de la Cour de justice ainsi que du Tribunal et vous permettra de manier les procédures s'appliquant à ces juridictions européennes. À jour des plus récentes jurisprudences et illustré de nombreux schémas, il constitue un outil clair et structuré pour comprendre et pratiquer ces procédures. \n",
      "Initiales ornées\n",
      "Description:  Dans les coulisses du théâtre : [anecdotes, confidences et indiscrétions] La 4e de couverture indique : \"Entrez dans ce livre et écoutez le dialogue entre le brigadier et l'administrateur du théâtre, qui rapportent anecdotes et confidences, curieuses ou drôles... Pénétrez à leur suite dans les coulisses de Charles-de-Rochefort, la Potinière, les Bouffes Parisiens et Fontaine. Découvrez les métiers du théâtre, depuis la « boîte à sel » jusqu'au baisser de rideau. Suivez les animateurs de cet art magique : ouvreuses, habilleuses, costumières, régisseurs, comédiens, éclairagistes, décorateurs et metteurs en scène. Retrouvez ces comédiennes d'exception que furent Michèle Morgan, Danièle Darrieux, Arletty... Applaudissez les talents de Jean Marais, Michel Simon, Jean le Poulain, Michel Legrand, Jean-Loup Dabadie, Pierre Arditi, Jacques Villeret, Robert Lamoureux. Découvrez ce que vous cache le rideau rouge et ce que dévoilent Georges Gaudry et Jean-Pierre Delaune. Dans une préface originale, Molière, « le Patron », assure le lien indispensable entre les Anciens et les Modernes. Et faites silence... le rideau se lève !\"\n",
      "Initiales ornées\n",
      "Description:  Épreuves écrites du CRFPA : spécialité droit pénal L'ouvrage \"tout en un\" pour réussir les épreuves écrites de droit pénal du CRFPA 2021 avec des fiches de révision et des exercices Toutes les épreuves écrites de l'examen sont traitées dans cet ouvrage, chaque partie se compose de la manière suivante : - une méthodologie spécifique pour chaque épreuve ; - des fiches Actualité et Problématique qui proposent des sujets présents au programme des épreuves afin de renforcer la capacité d'analyse des candidats ; - des outils de travail en relation avec chaque fiche traitée : principaux textes de lois avec reproduction d'articles, résumé des lois récentes, jurisprudence essentielle, listes d'articles issus de différentes revues juridiques (articles de synthèse, articles de débat, articles de fond) ; - des annales et exercices corrigés, afin de permettre aux candidats de s'entraîner et de se préparer à chaque type d'épreuves.\n",
      "Responsabilité civile\n",
      "Description:  L'ère des soulèvements : émeutes et confinements : les derniers soubresauts de la modernité Dès les années 1980, Michel Maffesoli se fait l'observateur averti et implacable des temps postmodernes. Il annonce un effondrement social porteur d'un paradoxal retour des tribus, ce que prouveront les décennies suivantes. Il pronostique également que, profitant de la fin des idéologies, les élites au pouvoir entendent instaurer un ordre nouveau qu'il qualifie de totalitarisme doux. Ce que démontre l'actualité récente. De l'éruption des gilets jaunes devenus un phénomène international à la contestation globale de la gestion de la pandémie, des grèves émeutières pour contrecarrer le libéralisme mondialisé à la vague d'émotion planétaire suscitée par l'incendie de Notre-Dame, le sociologue du quotidien et de l'imaginaire traque, de son oeil inégalé, le changement de paradigme que nous vivons. Le règne de la rationalité, de la technicité et de l'individualité agonise convulsivement sous nos yeux. Pour le meilleur et pour le pire, l'ère des révoltes a commencé et ne cessera pas avant longtemps. Cet essai flamboyant dit pourquoi et comment le peuple a raison de se rebeller.\n",
      "Initiales ornées\n",
      "Description:  Vivre plus simplement : analyse sociologique de la distanciation normative Changer de mode de vie au XXIe siècle n'est pas seulement une lubie Individuelle, mais une nécessité collective pour faire face aux enjeux écologiques planétaires. Or, faire évoluer son quotidien pour vivre plus simplement est loin d'être une évidence et implique une transformation matérielle et identitaire profonde. Il s'agit non seulement de changer ses habitudes de consommation, mais aussi de faire évoluer son travail, ses propres aspirations et ses rapports sociaux pour qu'ils reflètent et soutiennent ce changement de vie. En analysant l'expérience de celles et ceux qui essaient de vivre plus simplement, cette recherche met en lumière le processus de distanciation normative dans lequel ces personnes sont engagées. Cet ouvrage explore ce qui évolue dans leur quotidien et les répercussions que ce choix de vie a sur leurs interactions sociales, sur leurs engagements personnels et sur leur vision du monde à court et à long terme. Changer de vie pour aller vers plus de simplicité dans le monde du toujours plus, c'est remettre en question son propre rapport à la norme. C'est changer de paradigme et accepter le changement comme compagnon de route, car l'objectif final n'est ni arrêté ni évident. Les personnes qui font ce choix-là acceptent de vivre en transition, pour aller vers un avenir qu'il s'agit d'inventer chemin faisant.\n",
      "Initiales ornées\n",
      "Description:  La nouvelle économique Planification écologique, écosocialisme... autant de mots d'ordre qui remettent en cause le privilège de la classe dominante de décider par ses choix d'investissement de la trajectoire de toute une société. Autant de mots d'ordre qui s'opposent à la logique de la maximisation du profit qui a simultanément conduit à l'explosion des inégalités sociales et à la crise écologique. Autant de mots d'ordre qui appellent à repenser la place de la coordination des échanges économiques dans une société post-capitaliste et les relations coopératives entre les peuples du monde. Dans cette perspective, le défi auquel nous faisons face aujourd'hui consiste à trouver un calcul économique en mesure d'incorporer correctement à la fois les besoins sociaux et les contraintes environnementales. C'est à partir de ce double impératif que La nouvelle économique, l'ouvrage classique de l'économiste soviétique Evgueni Preobrajensky portant sur les débuts de la planification socialiste, s'avère un guide théorique d'une actualité frappante. Il s'agit là d'un ouvrage important, à la fois pour l'histoire de la transition (avortée) au socialisme après la révolution russe et comme première esquisse théorique d'un modèle de socialisme. Michel Husson, Pierre Naville et Ernest Mandel situent l'ouvrage dans ce double contexte historique et théorique.\n",
      "Initiales ornées\n",
      "Description:  Justice numérique La 4e de couv. indique :\"Expression ambiguë, le terme de \"justice numérique\" renvoie aux transformations que fait subir au monde judiciaire l'avènement de la vie en réseau et l'abandon progressif des supports matériels au profit de l'électronique et de l'information en ligne. Elle désigne également l'impact sur la justice du recours aux technologies les plus avancées, à l'heure où l'exploitation d'immenses quantités de données par des algorithmes d'apprentissage automatique permet de déceler des modèles statistiques et de formuler des prédictions. Cet ouvrage présente un état des lieux de la transition numérique de la justice, envisagée sous les deux formes principales qu'elle prend à l'heure actuelle : la dématérialisation des procédures, d'une part, et l'appropriation de l'intelligence artificielle par le recours aux algorithmes d'aide à la décision, d'autre part.\"\n",
      "Initiales ornées\n",
      "Description:  Freud, le temps de la neurologie : présentation et traduction des textes de 1884 à 1886 À l'heure où le projet psychanalytique est questionné dans ses fondements scientifiques et concurrencé dans son approche de la psyché humaine par le développement des neurosciences, Thierry Longé montre que le jeune Freud était engagé dans les courants scientifiques les plus novateurs de son époque. Il présente et restitue les textes, publiés entre 1884 et 1886, et jusque-là inédits en français, qui en témoignent. En effet, la psychanalyse est née dans ce terreau fertile d'une science neurologique en pleine mutation, et au moment même où les énigmes se déchiffraient les unes après les autres, permettant de concevoir un schéma cohérent de l'appareil neuro-cérébral. C'est en s'y adossant un temps pour mieux s'en détacher sans l'oublier jamais que Freud construit son propre appareil psychique. L'auteur redonne de la valeur à la continuité qu'on aurait volontiers tendance à disqualifier au profit de la rupture dans l'émergence de cette science nouvelle. Il insiste sur la dimension critique du geste freudien de penser avec et de penser contre.\n",
      "Initiales ornées\n",
      "Description:  Monde enchanté : chansons et imaginaires géographiques Le monde est enchanté par certaines chansons. Amsterdam, Penny Lane, Les lacs du Connemara, Göttingen sont désormais inséparables des mélodies de J. Brel, des Beatles, de M. Sardou et Barbara. Madonna nous emmène à La Isla Bonita ; avec Orelsan, Dans (s)a ville, on traîne.Voyage, voyage. Cet album invite à visiter les imaginaires géographiques véhiculés par trente-six chansons, pour la plupart très connues. Toutes associent des lieux à des rêves, des cauchemars, des émotions, et des valeurs. Il faut les prendre au sérieux : parce que les chansons nous affectent, modifient nos expériences et même les lieux que nous fréquentons.0Écouter les chansons géographiques, c'est peut-être regarder les cartes d'une autre oreille ?\n",
      "Initiales ornées\n",
      "Description:  Grosse, et alors  : connaître et combattre la grossophobie Bien qu'il semble concerner davantage les femmes, le phénomène de la grossophobie touche tout le monde. L'aversion envers les personnes grosses et les préjugés nocifs qu'on leur associe à tort sont à la base de discriminations volontaires ou non. Grosse, et alors? permet une meilleure sensibilisation aux enjeux liés à cette question. Du grand public aux autorités, sans oublier les émetteurs des nombreux messages auxquels nous sommes exposés quotidiennement, il est possible, en en prenant conscience, de diminuer les biais hostiles qui visent les personnes grosses. Lutter contre la grossophobie ne signifie pas faire l'apologie de l'obésité. C'est trouver une cohabitation proportionnelle des corps différents en taille et en silhouette, à l'image de la société.\n",
      "Initiales ornées\n",
      "Description:  Dans les prisons d'Angers : sous la Terreur, 1793-1794 1793-1794. La Terreur sévit dans tout le pays et particulièrement dans la Vendée militaire, en un déferlement d'actes de cruauté cuationné par les autorités de la Révolution. Si, depuis deux siècles, beaucoup a été écrit sur les affres de la guerre de Vendée, qui se souvient, dans la douceur de la capitale angevine, qu'Angers fut l'un des épicentres de cette féroce répression? Qui garde en mémoire le sort de ces milliers de prisonniers entassés dans des geôles alors établies dans des lieux familiers des Angevins d'aujourd'hui, puis laissés pour morts ou exécutés? 'Brigands de Vendeée', 'ennemis de la République' ou simples suspects: un système d'extermination impitoyable s'est abattu sur eux en quelques mois. Les mouroirs des prisons étant saturés, il fallut aux autorités locales imaginer des solutions autres que la guillotine, au rendement jugé insuffisant, pour se débarrasser de ces détenus encombrants. Et puis la Terreur s'arrêta, le silence se fit à Angers.. jusqu'à aujourd'hui.\n",
      "Initiales ornées\n",
      "Description:  Ce que les GAFAM font aux médias africains : enjeux socioéconomiques, éditoriaux et politiques de l'infomédiation Cet ouvrage s'intéresse aux relations qu'entretiennent les GAFAM (Google, Apple, Facebook, Amazon, Microsoft) et les médias en Afrique. Il aborde tout d'abord les réseaux sociaux numériques comme terrains d'amplification et de transformation de diffusion de l'information et questionne leur utilisation pour la diffusion de fausses informations et la remise en cause de la légitimité des médiateurs traditionnels de l'information. Il se consacre ensuite aux relations entretenues sur les plans éditorial, socioéconomique et sociotechnique par les plateformes et les médias, et y observe un certain opportunisme des médias et une domination des plateformes. Enfin, il s'intéresse au rôle joué par l'État à travers la régulation et les nouvelles lois qui intègrent les médias numériques mais mettent aussi en lumière les difficultés liées à l'implémentation d'un véritable cadre régulatoire\n",
      "Initiales ornées\n",
      "Description:  Une portion du présent : les normes et rituels sociaux comme sites d'intervention architecturale Comment l'architecture et l'urbanisme peuvent-ils mieux aborder les changements de nos réalités ? Comment les notions élargies de travail, d'obsession, d'agentivité, de propriété, de cycle de vie et de famille façonnent-elles nos modes de vie ? Une \"portion du présent\" décrit la situation actuelle et met en lumière le besoin d'une nouvelle spatialité pour abriter la formation de nouvelles relations sociales.\n",
      "Initiales ornées\n",
      "Description:  Jouer, rêver, inventer... : la créativité à l'oeuvre dans les lieux accueillant le jeune enfant Quelle place tient la créativité dans les structures d’accueil du jeune enfant ? Comment, et à quelles conditions s’exprime-t-elle, se décline-t-elle dans le quotidien des équipes ? Comment les professionnelles peuvent-elles inventer leurs modalités de travail et leurs « outils », dans une rencontre entre le cadre externe et leur inventivité propre ? Comment accompagnent-elles le déploiement de la créativité de l’enfant, à cet âge où il crée le monde par ses expériences ? Comment soutiennent-elles ces enfants qui semblent ne pas trouver d’élan créatif, ne pas jouer, ne pas inventer, imaginer ? Ainsi les autrices approfondissent le thème de la créativité au sens large, sans se restreindre au domaine de la création artistique. Elles s’appuient à la fois sur le développement de l’enfant et sur l’analyse du positionnement des professionnelles pour mieux comprendre certaines problématiques rencontrées au jour le jour dans les structures de la petite enfance. Alternant propos théoriques, vignettes cliniques et témoignages, elles rendent hommage à la création et à la créativité de ces artistes du quotidien qui accueillent les jeunes enfants mais aussi aux auteurs et cliniciens qui ont nourri leur pratique et leur réflexion\n",
      "Initiales ornées\n",
      "Description:  La décroissance et ses déclinaisons : pour sortir des clichés et des généralités Décroissance. Depuis que ce terme est entré dans le débat public il y a environ vingt ans, que d'idées reçues, de clichés et de malentendus. Chez les adversaires, mais aussi parfois chez les partisans de la décroissance. L'objet de ce livre, dans sa première partie, est de les cartographier et d'y répondre. Il convient ainsi d'assumer une définition de la décroissance au plus près de son sens ordinaire de « décrue » : il s'agit bien d'une diminution du domaine de l'économie au profit de celui de la «vie sociale », ce qui suppose de rompre avec tout un imaginaire porté par l'idéologie de la croissance. C'est pourquoi, dans la deuxieme partie, les auteurs proposent d'ouvrir seize axes de mise en pratique concrète de la décroissance, seize déclinaisons permettant de mieux appréhender ce qu'est, et ce que n'est pas, la décroissance. C'est alors tout un monde qui s'ouvre à des imaginaires et à des perspectives enthousiasmantes, faisant sortir la décroissance du temps des généralités, et permettant du même coup aux décroissants d'espérer explorer ces perspectives avec tous ces compagnons de route qui les défrichent déjà.\n",
      "Initiales ornées\n",
      "Description:  Mielvaque Si la Corrèze a déjà donné deux présidents de la République à la France, son député Mielvaque pourrait bien, grâce à ce livre, grimper au troisième rang des hommes politiques les plus célèbres du département. Élu, invalidé, réélu, Mielvaque déserte assidûment les bancs du Palais Bourbon et du conseil général. Car il a un souci bien plus préoccupant et pressant : la guerre hispano-américaine qui vient d'éclater à Cuba menace la fortune de sa femme, son éventuelle réélection, et même toute son existence. Ne pouvant rien contre le cours de l'Histoire, ruiné, il renoue avec ses crapuleries de jeunesse et est bientôt rattrapé par le scandale. Riche d'une iconographie abondante et souvent inédite, Mielvaque, Imposteur déchu de la IIIe République est le troisième tome de la trilogie qui retrace le parcours romanesque de ce personnage hors du commun et, dans un épilogue faustien, raconte la chute et la fin du personnage machiavélique et néanmoins si attachant qu'a été Michel Mielvaque.\n",
      "Initiales ornées\n",
      "Description:  L'EPS au ministère de l'Éducation nationale : 1981-2021 : transformations disciplinaires, mutations professionnelles, enjeux militants Le retour de l’EPS au sein du ministère de l’Éducation nationale en mai 1981 constitue un épisode souvent évoqué dans l’histoire de la discipline. Après quarante années passées dans une institution marquée par de nombreux changements, un bilan s’impose. Il s’agit d’analyser et de comprendre le processus d’intégration et ses effets sur le temps long, le rôle des enseignants d’EPS et des militants syndicaux dans les évolutions et transformations survenues. Ce livre est le résultat d’une journée d’études, de réflexions et de débats initiée par le SNEP qui s’est déroulée au département STAPS de l’Université de Créteil le 7 octobre 2021. Les articles qui le composent alternent les regards croisés d’acteurs et de chercheurs. Ils sont organisés selon six thématiques : la formation des enseignants, l’évolution de la leçon, la place du sport en EPS et de l’EPS dans le sport, l’exercice du métier, le statut des enseignants, le rôle du syndicalisme. Cette articulation de l’histoire et de la mémoire offre l’opportunité de proposer plusieurs grilles de lectures utiles afin de se saisir de la complémentarité des approches. Elle souligne le rôle majeur des acteurs sans lesquels l’action de toute institution reste artificielle. À sa lecture, on découvre au prisme de l’EPS, que ces quarante dernières années ont été faites d’engagements, de controverses, de débats qui ont nourri et accompagné une période de mutations sociales, scolaires et disciplinaires.\n",
      "Initiales ornées\n",
      "Description:  Les familles seigneuriales et le domaine de Valence-en-Brie : du Moyen âge à la Révolution de 1789 Monographie du village de Valence-en-Brie, présentation des seigneurs qui l'ont possédé, descriptif des fiefs qui le composaient\n",
      "Initiales ornées\n",
      "Description:  Algocratie : allons-nous donner le pouvoir aux algorithmes  Aujourd’hui, nos vies se retrouvent sous l’influence de nombreux algorithmes. Il y a ceux qui aident et qui conseillent : le GPS, les algorithmes de recommandation, les moteurs de recherche, les logiciels d’économie circulaire, les sites de rencontre. Nous nous y conformons sans aller outre, par confort, par facilité, par habitude ou lassitude. Mais il en est une deuxième catégorie, plus redoutable qui, elle, nous oblige et nous emprisonne dans ses lignes de code, inévitable et sans échappatoire possible : les systèmes d'inscription dans les établissements scolaires et universitaires, l’accès aux crédits, la fiscalité et la blockchain, et plus récemment toutes les applications qui nous ont permis d’affronter la crise du Covid. Face à cette défiance de la politique et de nos gouvernants qui s’installe partout, en réponse aux urgences et aux crises qui se multiplient, n’est-il pas temps de substituer l’algocratie à la démocratie ?\n",
      "Initiales ornées\n",
      "Description:  Désirs postcapitalistes Dans cette série de cours donnés avant sa disparition, Mark Fisher, théoricien critique britannique, auteur du Réalisme capitaliste et de Spectres de ma vie, commence par une question pour nous fondamentale : Voulons-nous vraiment ce que nous prétendons vouloir ? . Discutant avec ses élèves certaines des idées-phares de la pensée critique, il explore la relation entre le désir et le capitalisme, et se demande quelles puissances d'imagination et de relation restent à libérer à une époque où elles sont sans cesse re-programmées et canalisées par le développement personnel, la publicité et les industries technologiques. De l'émergence et de l'échec de la contre-culture dans les années 1970 à l'accélérationisme contemporain, en passant par les groupes d'auto-conscience féministe, ce livre met en perspective l'évolution d'un flux de positions, de programmes et d'actions pour mieux défendre la nécessité d'une transformation radicale de la société et de la culture.\n",
      "Initiales ornées\n",
      "Description:  Le coenseignement en pratique Présentation et analyse de tous les aspects du coenseignement, ses particularités et ses bénéfices, tant du point de vue des élèves que des professeurs. Ce guide explique comment mettre en place cette façon d'enseigner en abordant plusieurs points techniques : la forme, la posture des enseignants, la répartition des apprentissages, l'autorité, l'évaluation, entre autres.\n",
      "Initiales ornées\n",
      "Description:  Les jardins d'hiver : guide des plantes de véranda et d'intérieur Le seul livre de référence pour la culture et l'entretien de toutes sortes de plantes d'interieur, ecrit par l'un des meilleurs specialistes. plus de 700 plantes répertoriées. De superbes photographies couleurs. Des conseils pour la plantation, l'exposition, la chaleur, l'arrosage, la mutiplication, les compositions. Des exemples de jardins interieurs, avec tous les détails sur la construcion et aménagement de vérandas  pour vous permettre de réaliser, selon vos goûts, un merveilleux jardin d'hiver.\n",
      "Initiales ornées\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "example = df_valid100['descr'][98]\n",
    "print(\"Description: \", example)\n",
    "encodings = tokenizer.encode_plus(\n",
    "    example,\n",
    "    None,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    return_token_type_ids=True,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "\n",
    "preds = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for text in df_valid100[\"descr\"]:\n",
    "        print(\"Description: \", text)\n",
    "        encodings = tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt')\n",
    "        input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
    "        attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
    "        output = model(input_ids, attention_mask, token_type_ids)\n",
    "        final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
    "        print(df_valid100.columns[:-1].to_list()[int(np.argmax(final_output, axis=1))])\n",
    "        preds.append(final_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5087566375732422,\n",
       "  0.5124032497406006,\n",
       "  0.5132208466529846,\n",
       "  0.5153500437736511,\n",
       "  0.49287381768226624,\n",
       "  0.5066041350364685,\n",
       "  0.4946262538433075,\n",
       "  0.4659135639667511,\n",
       "  0.5077733397483826,\n",
       "  0.5057207345962524,\n",
       "  0.4824887812137604,\n",
       "  0.47844627499580383,\n",
       "  0.5210345983505249,\n",
       "  0.5154179334640503,\n",
       "  0.513576090335846,\n",
       "  0.5031613707542419,\n",
       "  0.5324522852897644,\n",
       "  0.5125325918197632,\n",
       "  0.46676015853881836,\n",
       "  0.510063886642456,\n",
       "  0.4849981665611267,\n",
       "  0.511141300201416,\n",
       "  0.4978821873664856,\n",
       "  0.4982639253139496,\n",
       "  0.4959043562412262,\n",
       "  0.5029359459877014,\n",
       "  0.5196188688278198,\n",
       "  0.49992048740386963,\n",
       "  0.500252366065979,\n",
       "  0.4984060227870941,\n",
       "  0.5226011872291565,\n",
       "  0.49930810928344727,\n",
       "  0.4976603090763092,\n",
       "  0.5147148966789246,\n",
       "  0.48331931233406067,\n",
       "  0.4911966919898987,\n",
       "  0.4955936372280121,\n",
       "  0.5148219466209412,\n",
       "  0.5056673884391785,\n",
       "  0.49778634309768677,\n",
       "  0.5052366256713867,\n",
       "  0.5104638934135437,\n",
       "  0.4688869118690491,\n",
       "  0.5139185190200806,\n",
       "  0.49960044026374817,\n",
       "  0.4939354658126831,\n",
       "  0.49325528740882874,\n",
       "  0.49511343240737915,\n",
       "  0.5143104791641235,\n",
       "  0.5068281292915344,\n",
       "  0.5058432817459106,\n",
       "  0.4993651807308197,\n",
       "  0.4959522485733032,\n",
       "  0.4823663830757141,\n",
       "  0.5024406909942627,\n",
       "  0.5067112445831299,\n",
       "  0.4948212206363678,\n",
       "  0.48981213569641113,\n",
       "  0.4929666221141815,\n",
       "  0.4871266782283783,\n",
       "  0.5117740035057068,\n",
       "  0.49435943365097046,\n",
       "  0.4870433211326599,\n",
       "  0.49478110671043396,\n",
       "  0.4850686192512512,\n",
       "  0.4959859848022461,\n",
       "  0.5053147077560425,\n",
       "  0.5309591889381409,\n",
       "  0.5053621530532837,\n",
       "  0.48137167096138,\n",
       "  0.5369501113891602,\n",
       "  0.5043532848358154,\n",
       "  0.48720160126686096,\n",
       "  0.4720573425292969,\n",
       "  0.4832819402217865,\n",
       "  0.49367350339889526,\n",
       "  0.5111327767372131,\n",
       "  0.470503568649292,\n",
       "  0.49992835521698,\n",
       "  0.4973750412464142,\n",
       "  0.5084748268127441,\n",
       "  0.518417239189148,\n",
       "  0.48818153142929077,\n",
       "  0.5147309303283691,\n",
       "  0.5295692682266235,\n",
       "  0.5163078904151917,\n",
       "  0.4845394790172577,\n",
       "  0.46748244762420654,\n",
       "  0.500739574432373,\n",
       "  0.47943949699401855,\n",
       "  0.5036281943321228,\n",
       "  0.47098731994628906,\n",
       "  0.5018774271011353,\n",
       "  0.5078710317611694,\n",
       "  0.49817562103271484,\n",
       "  0.49627596139907837,\n",
       "  0.5064526796340942,\n",
       "  0.4948817491531372,\n",
       "  0.5115848779678345,\n",
       "  0.5016884207725525,\n",
       "  0.5011688470840454,\n",
       "  0.5146439075469971,\n",
       "  0.5266464352607727,\n",
       "  0.504983127117157,\n",
       "  0.5245376229286194,\n",
       "  0.5225646495819092,\n",
       "  0.47779691219329834,\n",
       "  0.5226932764053345,\n",
       "  0.5163833498954773,\n",
       "  0.5137760639190674,\n",
       "  0.5152844190597534,\n",
       "  0.5072535276412964,\n",
       "  0.4927615523338318,\n",
       "  0.48801806569099426,\n",
       "  0.5061112642288208,\n",
       "  0.5134373307228088,\n",
       "  0.4947241544723511,\n",
       "  0.5127164125442505,\n",
       "  0.5106691718101501,\n",
       "  0.49872997403144836,\n",
       "  0.5092433094978333,\n",
       "  0.4844735860824585,\n",
       "  0.48310500383377075,\n",
       "  0.501546323299408,\n",
       "  0.4911194443702698,\n",
       "  0.5138619542121887,\n",
       "  0.49302539229393005,\n",
       "  0.4959818720817566,\n",
       "  0.4929981529712677,\n",
       "  0.4885401725769043,\n",
       "  0.4983983337879181,\n",
       "  0.48612308502197266,\n",
       "  0.5274454355239868,\n",
       "  0.49953505396842957,\n",
       "  0.5211701393127441,\n",
       "  0.5008673071861267,\n",
       "  0.48807600140571594,\n",
       "  0.4850790500640869,\n",
       "  0.5053513050079346,\n",
       "  0.5032821893692017,\n",
       "  0.507745623588562,\n",
       "  0.503257691860199,\n",
       "  0.48418888449668884,\n",
       "  0.495112806558609,\n",
       "  0.5297190546989441,\n",
       "  0.5127105712890625,\n",
       "  0.4947282373905182,\n",
       "  0.4805614650249481,\n",
       "  0.4840763807296753,\n",
       "  0.4845930337905884,\n",
       "  0.48940372467041016,\n",
       "  0.4853489398956299,\n",
       "  0.5206459164619446,\n",
       "  0.5025953650474548,\n",
       "  0.5118674039840698,\n",
       "  0.49594441056251526,\n",
       "  0.5016986131668091,\n",
       "  0.5002148747444153,\n",
       "  0.4835473895072937,\n",
       "  0.5033513903617859,\n",
       "  0.5051996111869812,\n",
       "  0.4777618646621704,\n",
       "  0.4981701970100403,\n",
       "  0.4914339780807495,\n",
       "  0.49098509550094604,\n",
       "  0.49273625016212463,\n",
       "  0.5091508030891418,\n",
       "  0.49437177181243896,\n",
       "  0.49346280097961426,\n",
       "  0.5321118235588074,\n",
       "  0.5063095092773438,\n",
       "  0.5075663924217224,\n",
       "  0.5139464139938354,\n",
       "  0.5063217878341675,\n",
       "  0.49035048484802246,\n",
       "  0.49999070167541504,\n",
       "  0.49973806738853455,\n",
       "  0.4995611011981964,\n",
       "  0.4954085350036621,\n",
       "  0.49259504675865173,\n",
       "  0.5062832236289978,\n",
       "  0.5290804505348206,\n",
       "  0.5016900897026062,\n",
       "  0.4954465627670288,\n",
       "  0.5019725561141968,\n",
       "  0.5189899206161499,\n",
       "  0.5009521842002869,\n",
       "  0.5251795649528503,\n",
       "  0.48009517788887024,\n",
       "  0.5207497477531433,\n",
       "  0.49622663855552673,\n",
       "  0.4924275577068329,\n",
       "  0.4949122965335846,\n",
       "  0.47421032190322876,\n",
       "  0.49463650584220886,\n",
       "  0.49163010716438293,\n",
       "  0.49211522936820984,\n",
       "  0.47528719902038574,\n",
       "  0.5151259303092957,\n",
       "  0.4943169951438904,\n",
       "  0.4828009307384491,\n",
       "  0.4920867085456848,\n",
       "  0.5143361687660217,\n",
       "  0.4950200021266937,\n",
       "  0.5371156930923462,\n",
       "  0.4838492274284363,\n",
       "  0.489995539188385,\n",
       "  0.47607266902923584,\n",
       "  0.5048415064811707,\n",
       "  0.5019040703773499,\n",
       "  0.5164185166358948,\n",
       "  0.4854855537414551,\n",
       "  0.5159750580787659,\n",
       "  0.5029987692832947,\n",
       "  0.4929845929145813,\n",
       "  0.5182839035987854,\n",
       "  0.48631563782691956,\n",
       "  0.4799438416957855,\n",
       "  0.502926766872406,\n",
       "  0.4869561791419983,\n",
       "  0.48421788215637207,\n",
       "  0.49455195665359497,\n",
       "  0.5181301236152649,\n",
       "  0.4881768822669983,\n",
       "  0.4932054579257965,\n",
       "  0.5080384016036987,\n",
       "  0.5136343836784363,\n",
       "  0.5005062818527222,\n",
       "  0.5045495629310608,\n",
       "  0.5158812403678894,\n",
       "  0.4826931953430176,\n",
       "  0.49995410442352295,\n",
       "  0.478167861700058,\n",
       "  0.492556631565094,\n",
       "  0.5272095203399658,\n",
       "  0.48518383502960205,\n",
       "  0.46972930431365967,\n",
       "  0.496235191822052,\n",
       "  0.5075498223304749,\n",
       "  0.4910440146923065,\n",
       "  0.5148934125900269,\n",
       "  0.5052369832992554,\n",
       "  0.5046550035476685,\n",
       "  0.4967053234577179,\n",
       "  0.5182404518127441,\n",
       "  0.5063005685806274,\n",
       "  0.4863921105861664,\n",
       "  0.5289561152458191,\n",
       "  0.4963168203830719,\n",
       "  0.49145084619522095,\n",
       "  0.49765583872795105,\n",
       "  0.49278005957603455,\n",
       "  0.4822236895561218,\n",
       "  0.5221323370933533,\n",
       "  0.5035375952720642,\n",
       "  0.5049230456352234,\n",
       "  0.4932878315448761,\n",
       "  0.5285325050354004,\n",
       "  0.5020022392272949,\n",
       "  0.5143759250640869,\n",
       "  0.5150218605995178,\n",
       "  0.5086150765419006,\n",
       "  0.5039128661155701,\n",
       "  0.4846532940864563,\n",
       "  0.5098966956138611,\n",
       "  0.5017368793487549,\n",
       "  0.5006254315376282,\n",
       "  0.4935424029827118,\n",
       "  0.5089465975761414,\n",
       "  0.5056064128875732,\n",
       "  0.5041800141334534,\n",
       "  0.5031512379646301,\n",
       "  0.527152419090271,\n",
       "  0.5116947293281555,\n",
       "  0.4945233166217804,\n",
       "  0.47256234288215637,\n",
       "  0.4926459789276123,\n",
       "  0.4930732548236847,\n",
       "  0.5052943825721741,\n",
       "  0.5047749876976013,\n",
       "  0.4934329092502594,\n",
       "  0.4966225326061249,\n",
       "  0.5151331424713135,\n",
       "  0.4927029013633728,\n",
       "  0.4898926913738251,\n",
       "  0.4870406985282898,\n",
       "  0.4837503135204315,\n",
       "  0.47814831137657166,\n",
       "  0.48075515031814575,\n",
       "  0.48803770542144775,\n",
       "  0.5173666477203369,\n",
       "  0.49497660994529724,\n",
       "  0.484867662191391,\n",
       "  0.4988401532173157,\n",
       "  0.49111294746398926,\n",
       "  0.4970138669013977,\n",
       "  0.491983026266098,\n",
       "  0.48620525002479553,\n",
       "  0.5206269025802612,\n",
       "  0.503351628780365,\n",
       "  0.5238497257232666,\n",
       "  0.49261489510536194,\n",
       "  0.47570210695266724,\n",
       "  0.4866020083427429,\n",
       "  0.5022224187850952,\n",
       "  0.4961634576320648,\n",
       "  0.5096256732940674,\n",
       "  0.48108887672424316,\n",
       "  0.49725469946861267,\n",
       "  0.5243019461631775,\n",
       "  0.512085497379303,\n",
       "  0.46761205792427063,\n",
       "  0.4900515377521515,\n",
       "  0.522896945476532,\n",
       "  0.49323809146881104,\n",
       "  0.4898501932621002,\n",
       "  0.5199354887008667,\n",
       "  0.49396929144859314,\n",
       "  0.5275231003761292,\n",
       "  0.5149370431900024,\n",
       "  0.5070807933807373,\n",
       "  0.502270519733429,\n",
       "  0.49326449632644653,\n",
       "  0.5146857500076294,\n",
       "  0.49791890382766724,\n",
       "  0.5023695230484009,\n",
       "  0.5005859732627869,\n",
       "  0.5090732574462891,\n",
       "  0.5067673921585083,\n",
       "  0.4869649410247803,\n",
       "  0.5023224353790283,\n",
       "  0.5006676316261292,\n",
       "  0.4705352187156677,\n",
       "  0.5134115815162659,\n",
       "  0.5032820105552673,\n",
       "  0.5002289414405823,\n",
       "  0.4764362871646881,\n",
       "  0.47719255089759827,\n",
       "  0.5258380174636841,\n",
       "  0.5012176632881165,\n",
       "  0.5171903371810913,\n",
       "  0.5182264447212219,\n",
       "  0.48712748289108276,\n",
       "  0.525102972984314,\n",
       "  0.5173326730728149,\n",
       "  0.5104987025260925,\n",
       "  0.4898916184902191,\n",
       "  0.49997031688690186,\n",
       "  0.5200944542884827,\n",
       "  0.5089031457901001,\n",
       "  0.46792763471603394,\n",
       "  0.4955061674118042,\n",
       "  0.49991488456726074,\n",
       "  0.4948970675468445,\n",
       "  0.4931464195251465,\n",
       "  0.4882235825061798,\n",
       "  0.5060496926307678,\n",
       "  0.523504912853241,\n",
       "  0.49669086933135986,\n",
       "  0.48990368843078613,\n",
       "  0.49729278683662415,\n",
       "  0.5253130197525024,\n",
       "  0.47855299711227417,\n",
       "  0.5000407099723816,\n",
       "  0.5089302062988281,\n",
       "  0.5062233805656433,\n",
       "  0.5151131749153137,\n",
       "  0.4983268082141876,\n",
       "  0.4855770766735077,\n",
       "  0.4740408658981323,\n",
       "  0.4777960479259491,\n",
       "  0.5229870080947876,\n",
       "  0.5109184980392456,\n",
       "  0.5139769315719604,\n",
       "  0.5042785406112671,\n",
       "  0.49565452337265015,\n",
       "  0.5163977146148682,\n",
       "  0.47892725467681885,\n",
       "  0.5042598843574524,\n",
       "  0.501235842704773,\n",
       "  0.4668023884296417,\n",
       "  0.4996385872364044,\n",
       "  0.5133171081542969,\n",
       "  0.5140406489372253,\n",
       "  0.5077359676361084,\n",
       "  0.5092175602912903,\n",
       "  0.49558520317077637,\n",
       "  0.4904440641403198,\n",
       "  0.48182058334350586,\n",
       "  0.4960068166255951,\n",
       "  0.4726872146129608,\n",
       "  0.4920886754989624,\n",
       "  0.49786341190338135,\n",
       "  0.4965602159500122,\n",
       "  0.5174376368522644,\n",
       "  0.5042157769203186,\n",
       "  0.470518559217453,\n",
       "  0.5106033682823181,\n",
       "  0.5149260759353638,\n",
       "  0.5068273544311523,\n",
       "  0.48813626170158386,\n",
       "  0.4804997146129608,\n",
       "  0.5254514813423157,\n",
       "  0.5361195802688599,\n",
       "  0.5080472826957703,\n",
       "  0.5056710243225098,\n",
       "  0.48112958669662476,\n",
       "  0.498083233833313,\n",
       "  0.4788055419921875,\n",
       "  0.5047394633293152,\n",
       "  0.4934249520301819,\n",
       "  0.5102509260177612,\n",
       "  0.47116634249687195,\n",
       "  0.5011581182479858,\n",
       "  0.4787254333496094,\n",
       "  0.5144146084785461,\n",
       "  0.49985942244529724,\n",
       "  0.47946661710739136,\n",
       "  0.49989286065101624,\n",
       "  0.48508399724960327,\n",
       "  0.5260853171348572,\n",
       "  0.513270914554596,\n",
       "  0.5044553875923157,\n",
       "  0.48210644721984863,\n",
       "  0.48656463623046875,\n",
       "  0.5062315464019775,\n",
       "  0.48575252294540405,\n",
       "  0.5012003779411316,\n",
       "  0.4935968816280365,\n",
       "  0.5142112970352173,\n",
       "  0.5100679397583008,\n",
       "  0.4977196156978607,\n",
       "  0.48030173778533936,\n",
       "  0.5153722167015076,\n",
       "  0.4921872913837433,\n",
       "  0.5317489504814148,\n",
       "  0.5254006385803223,\n",
       "  0.5127279162406921,\n",
       "  0.49764177203178406,\n",
       "  0.5134789347648621,\n",
       "  0.4798920452594757,\n",
       "  0.5155563950538635,\n",
       "  0.5042694211006165,\n",
       "  0.5152737498283386,\n",
       "  0.4961176812648773,\n",
       "  0.500928521156311,\n",
       "  0.501123309135437,\n",
       "  0.5289340019226074,\n",
       "  0.5132765173912048,\n",
       "  0.47722166776657104,\n",
       "  0.4917483925819397,\n",
       "  0.49983835220336914,\n",
       "  0.4813324511051178,\n",
       "  0.5053612589836121,\n",
       "  0.48204073309898376,\n",
       "  0.507897675037384,\n",
       "  0.4909360110759735,\n",
       "  0.508517861366272,\n",
       "  0.5242149233818054,\n",
       "  0.5215367078781128,\n",
       "  0.5042135715484619,\n",
       "  0.5135493278503418,\n",
       "  0.48066192865371704,\n",
       "  0.502091646194458,\n",
       "  0.49747177958488464,\n",
       "  0.5051390528678894,\n",
       "  0.5085902810096741,\n",
       "  0.49487194418907166,\n",
       "  0.508112907409668,\n",
       "  0.5048145651817322,\n",
       "  0.48109185695648193,\n",
       "  0.4924580454826355,\n",
       "  0.5068995356559753,\n",
       "  0.5054616332054138,\n",
       "  0.48969051241874695,\n",
       "  0.5004693865776062,\n",
       "  0.5308604836463928,\n",
       "  0.4842317998409271,\n",
       "  0.5009698867797852,\n",
       "  0.5011571645736694,\n",
       "  0.5278417468070984,\n",
       "  0.4951503872871399,\n",
       "  0.5100773572921753,\n",
       "  0.5098099112510681,\n",
       "  0.4947289228439331,\n",
       "  0.48792240023612976,\n",
       "  0.5057983994483948,\n",
       "  0.5259554982185364,\n",
       "  0.48636001348495483,\n",
       "  0.472878098487854,\n",
       "  0.5026323199272156,\n",
       "  0.48558393120765686,\n",
       "  0.4938027262687683,\n",
       "  0.48493829369544983,\n",
       "  0.4972141981124878,\n",
       "  0.4780665934085846,\n",
       "  0.5129688382148743,\n",
       "  0.4979216754436493,\n",
       "  0.4855550229549408,\n",
       "  0.5054444074630737,\n",
       "  0.5109032392501831,\n",
       "  0.5033839344978333,\n",
       "  0.48322099447250366,\n",
       "  0.4979722797870636,\n",
       "  0.5044213533401489,\n",
       "  0.5228449702262878,\n",
       "  0.49630284309387207,\n",
       "  0.5124635696411133,\n",
       "  0.520277202129364,\n",
       "  0.4980998933315277,\n",
       "  0.4790704548358917,\n",
       "  0.5129726529121399,\n",
       "  0.5004584789276123,\n",
       "  0.5119083523750305,\n",
       "  0.49216228723526,\n",
       "  0.5087165832519531,\n",
       "  0.5017675161361694,\n",
       "  0.5276716351509094,\n",
       "  0.5055530667304993,\n",
       "  0.4934186339378357,\n",
       "  0.5000230669975281,\n",
       "  0.5052028298377991,\n",
       "  0.49818336963653564,\n",
       "  0.5101026892662048,\n",
       "  0.5042451024055481,\n",
       "  0.48226189613342285,\n",
       "  0.47288239002227783,\n",
       "  0.49955621361732483,\n",
       "  0.4895194470882416,\n",
       "  0.4824332892894745,\n",
       "  0.5011560320854187,\n",
       "  0.48165568709373474,\n",
       "  0.4992046058177948,\n",
       "  0.4899064898490906,\n",
       "  0.486563503742218,\n",
       "  0.491806298494339,\n",
       "  0.4908970594406128,\n",
       "  0.5043519139289856,\n",
       "  0.5125483274459839,\n",
       "  0.5169487595558167,\n",
       "  0.4977574646472931,\n",
       "  0.5012890696525574,\n",
       "  0.48266878724098206,\n",
       "  0.4699375033378601,\n",
       "  0.5228012800216675,\n",
       "  0.5018571019172668,\n",
       "  0.5052937865257263,\n",
       "  0.47516754269599915,\n",
       "  0.5257139801979065,\n",
       "  0.4847595989704132,\n",
       "  0.5141605138778687,\n",
       "  0.49557337164878845,\n",
       "  0.48547422885894775,\n",
       "  0.4756176769733429,\n",
       "  0.5271333456039429,\n",
       "  0.5163240432739258,\n",
       "  0.49538055062294006,\n",
       "  0.4851278066635132,\n",
       "  0.4895336925983429,\n",
       "  0.5030596256256104,\n",
       "  0.4989018738269806,\n",
       "  0.5083711743354797,\n",
       "  0.5186458826065063,\n",
       "  0.5051829814910889,\n",
       "  0.4976237118244171,\n",
       "  0.5020116567611694,\n",
       "  0.504180371761322,\n",
       "  0.5026339888572693,\n",
       "  0.533272385597229,\n",
       "  0.4724791347980499,\n",
       "  0.518279492855072,\n",
       "  0.5192428231239319,\n",
       "  0.4814976751804352,\n",
       "  0.5142266750335693,\n",
       "  0.48267924785614014,\n",
       "  0.5181000828742981,\n",
       "  0.49324727058410645,\n",
       "  0.4977813959121704,\n",
       "  0.4834364056587219,\n",
       "  0.5183412432670593,\n",
       "  0.5015221238136292,\n",
       "  0.49296754598617554,\n",
       "  0.48760828375816345,\n",
       "  0.48420703411102295,\n",
       "  0.517407238483429,\n",
       "  0.5011246204376221,\n",
       "  0.5053957104682922,\n",
       "  0.5047036409378052,\n",
       "  0.4892843961715698,\n",
       "  0.4986989498138428,\n",
       "  0.48007258772850037,\n",
       "  0.49857616424560547,\n",
       "  0.48678216338157654,\n",
       "  0.5068246722221375,\n",
       "  0.48478761315345764,\n",
       "  0.5100381374359131,\n",
       "  0.4762319028377533,\n",
       "  0.5085452198982239,\n",
       "  0.5133203268051147,\n",
       "  0.4742756485939026,\n",
       "  0.48772484064102173,\n",
       "  0.48932957649230957,\n",
       "  0.5173998475074768,\n",
       "  0.5060520172119141,\n",
       "  0.5025262832641602,\n",
       "  0.49695104360580444,\n",
       "  0.4826933443546295,\n",
       "  0.48410409688949585,\n",
       "  0.4969468414783478,\n",
       "  0.49804437160491943,\n",
       "  0.5013060569763184,\n",
       "  0.49294623732566833,\n",
       "  0.5347554087638855,\n",
       "  0.48881107568740845,\n",
       "  0.5099167823791504,\n",
       "  0.49244004487991333,\n",
       "  0.502244234085083,\n",
       "  0.46667686104774475,\n",
       "  0.5113376975059509,\n",
       "  0.5221555233001709,\n",
       "  0.4893738031387329,\n",
       "  0.4905311167240143,\n",
       "  0.49251899123191833,\n",
       "  0.49830904603004456,\n",
       "  0.5062958002090454,\n",
       "  0.49428969621658325,\n",
       "  0.5129715800285339,\n",
       "  0.48647603392601013,\n",
       "  0.5030558705329895,\n",
       "  0.5203569531440735,\n",
       "  0.5029117465019226,\n",
       "  0.49321115016937256,\n",
       "  0.4665171504020691,\n",
       "  0.5260468125343323,\n",
       "  0.5163561105728149,\n",
       "  0.4961540699005127,\n",
       "  0.5036702752113342,\n",
       "  0.4940897226333618,\n",
       "  0.486028790473938,\n",
       "  0.5090532302856445,\n",
       "  0.5063493251800537,\n",
       "  0.48719820380210876,\n",
       "  0.4778797924518585,\n",
       "  0.48885437846183777,\n",
       "  0.48707225918769836,\n",
       "  0.4920988976955414,\n",
       "  0.48783671855926514,\n",
       "  0.4933322072029114,\n",
       "  0.4928451478481293,\n",
       "  0.484197199344635,\n",
       "  0.48411062359809875,\n",
       "  0.5056179761886597,\n",
       "  0.4762151539325714,\n",
       "  0.501370370388031,\n",
       "  0.48353126645088196,\n",
       "  0.4898410439491272,\n",
       "  0.4973658323287964,\n",
       "  0.4934600591659546,\n",
       "  0.5143875479698181,\n",
       "  0.4894583821296692,\n",
       "  0.5002128481864929,\n",
       "  0.472974956035614,\n",
       "  0.5034112930297852,\n",
       "  0.5209843516349792,\n",
       "  0.5207280516624451,\n",
       "  0.4952065050601959,\n",
       "  0.510972797870636,\n",
       "  0.4988116919994354,\n",
       "  0.493045449256897,\n",
       "  0.5266488790512085,\n",
       "  0.4731900990009308,\n",
       "  0.518301784992218,\n",
       "  0.519805371761322,\n",
       "  0.49767160415649414,\n",
       "  0.4827684164047241,\n",
       "  0.49992454051971436,\n",
       "  0.46550196409225464,\n",
       "  0.4812275171279907,\n",
       "  0.4854506254196167,\n",
       "  0.4980428218841553,\n",
       "  0.4976695775985718,\n",
       "  0.498232901096344,\n",
       "  0.49060454964637756,\n",
       "  0.49716079235076904,\n",
       "  0.5133664011955261,\n",
       "  0.5003010034561157,\n",
       "  0.5056422352790833,\n",
       "  0.5105915069580078,\n",
       "  0.47890177369117737,\n",
       "  0.4931679964065552,\n",
       "  0.4944164752960205,\n",
       "  0.48032841086387634,\n",
       "  0.5117329359054565,\n",
       "  0.5099700689315796,\n",
       "  0.49249064922332764,\n",
       "  0.4870523512363434,\n",
       "  0.5013552904129028,\n",
       "  0.5061774253845215,\n",
       "  0.46015575528144836,\n",
       "  0.5083951354026794,\n",
       "  0.5133918523788452,\n",
       "  0.4896933138370514,\n",
       "  0.5064927935600281,\n",
       "  0.49386027455329895,\n",
       "  0.4846739172935486,\n",
       "  0.5056552886962891,\n",
       "  0.5036082863807678,\n",
       "  0.5018978714942932,\n",
       "  0.4923989474773407,\n",
       "  0.5265154242515564,\n",
       "  0.5126936435699463,\n",
       "  0.48727574944496155,\n",
       "  0.49940747022628784,\n",
       "  0.495971143245697,\n",
       "  0.4932524263858795,\n",
       "  0.5147110223770142,\n",
       "  0.48629429936408997,\n",
       "  0.5086901187896729,\n",
       "  0.48056653141975403,\n",
       "  0.48591041564941406,\n",
       "  0.5355036854743958,\n",
       "  0.4914046823978424,\n",
       "  0.5166853070259094,\n",
       "  0.5156809091567993,\n",
       "  0.5015963912010193,\n",
       "  0.5058622360229492,\n",
       "  0.5069890022277832,\n",
       "  0.49759915471076965,\n",
       "  0.4811156988143921,\n",
       "  0.47566768527030945,\n",
       "  0.5012547373771667,\n",
       "  0.5209004878997803,\n",
       "  0.5160607099533081,\n",
       "  0.5081658959388733,\n",
       "  0.4775567948818207,\n",
       "  0.5084297060966492,\n",
       "  0.507527768611908,\n",
       "  0.471004456281662,\n",
       "  0.4966653883457184,\n",
       "  0.488693505525589,\n",
       "  0.5142806172370911,\n",
       "  0.5003066062927246,\n",
       "  0.4946173429489136,\n",
       "  0.4758043587207794,\n",
       "  0.4907875061035156,\n",
       "  0.5075893998146057,\n",
       "  0.5073213577270508,\n",
       "  0.4952303469181061,\n",
       "  0.5063421726226807,\n",
       "  0.48341062664985657,\n",
       "  0.48484006524086,\n",
       "  0.5015456676483154,\n",
       "  0.4842770993709564,\n",
       "  0.5022444128990173,\n",
       "  0.4956928491592407,\n",
       "  0.48982706665992737,\n",
       "  0.512060821056366,\n",
       "  0.4936849772930145,\n",
       "  0.4801467955112457,\n",
       "  0.47755223512649536,\n",
       "  0.5020445585250854,\n",
       "  0.4978038966655731,\n",
       "  0.4998956024646759,\n",
       "  0.4945356249809265,\n",
       "  0.49764519929885864,\n",
       "  0.48782315850257874,\n",
       "  0.5079621076583862,\n",
       "  0.5313909649848938,\n",
       "  0.490823358297348,\n",
       "  0.49897512793540955,\n",
       "  0.5245958566665649,\n",
       "  0.5042552947998047,\n",
       "  0.467273086309433,\n",
       "  0.5063073635101318,\n",
       "  0.488450288772583,\n",
       "  0.49189332127571106,\n",
       "  0.48460423946380615,\n",
       "  0.5348613858222961,\n",
       "  0.499551922082901,\n",
       "  0.5043155550956726,\n",
       "  0.5188080668449402,\n",
       "  0.4955326020717621,\n",
       "  0.4912484586238861,\n",
       "  0.5041966438293457,\n",
       "  0.468278169631958,\n",
       "  0.4959118068218231,\n",
       "  0.477071613073349,\n",
       "  0.5271790027618408,\n",
       "  0.504838764667511,\n",
       "  0.49889808893203735,\n",
       "  0.5086948275566101,\n",
       "  0.48451679944992065,\n",
       "  0.4932165741920471,\n",
       "  0.4855365455150604,\n",
       "  0.5325340032577515,\n",
       "  0.49514061212539673,\n",
       "  0.4944095313549042,\n",
       "  0.4853690564632416,\n",
       "  0.5071470141410828,\n",
       "  0.5088494420051575,\n",
       "  0.49553459882736206,\n",
       "  0.4993031919002533,\n",
       "  0.4883354604244232,\n",
       "  0.48996487259864807,\n",
       "  0.5160936713218689,\n",
       "  0.5090256929397583,\n",
       "  0.5020079612731934,\n",
       "  0.5011215209960938,\n",
       "  0.5010010600090027,\n",
       "  0.4972745478153229,\n",
       "  0.5342164039611816,\n",
       "  0.4654019773006439,\n",
       "  0.5025377869606018,\n",
       "  0.5031808614730835,\n",
       "  0.5321696400642395,\n",
       "  0.5068010687828064,\n",
       "  0.5187419056892395,\n",
       "  0.5003495216369629,\n",
       "  0.5013178586959839,\n",
       "  0.5136092305183411,\n",
       "  0.4901343882083893,\n",
       "  0.5029324889183044,\n",
       "  0.4935727119445801,\n",
       "  0.5012019872665405,\n",
       "  0.4785470962524414,\n",
       "  0.4879154860973358,\n",
       "  0.4975213408470154,\n",
       "  0.4857463240623474,\n",
       "  0.49338018894195557,\n",
       "  0.4739404618740082,\n",
       "  0.4775410294532776,\n",
       "  0.46735817193984985,\n",
       "  0.5093309879302979,\n",
       "  0.49930763244628906,\n",
       "  0.5026938319206238,\n",
       "  0.49821385741233826,\n",
       "  0.4984082877635956,\n",
       "  0.4705196022987366,\n",
       "  0.4932999014854431,\n",
       "  0.5176903009414673,\n",
       "  0.5069345235824585,\n",
       "  0.5211195349693298,\n",
       "  0.51225346326828,\n",
       "  0.5062623620033264,\n",
       "  0.4935430884361267,\n",
       "  0.49358829855918884,\n",
       "  0.4984641969203949,\n",
       "  0.5273775458335876,\n",
       "  0.5008898973464966,\n",
       "  0.49963250756263733,\n",
       "  0.4808288812637329,\n",
       "  0.5031066536903381,\n",
       "  0.5248230695724487,\n",
       "  0.5076457262039185,\n",
       "  0.48495808243751526,\n",
       "  0.5121051073074341,\n",
       "  0.5155290961265564,\n",
       "  0.4877403974533081,\n",
       "  0.5157305598258972,\n",
       "  0.5028515458106995,\n",
       "  0.4764348864555359,\n",
       "  0.48757869005203247,\n",
       "  0.47755903005599976,\n",
       "  0.5022624731063843,\n",
       "  0.5040038228034973,\n",
       "  0.5017545819282532,\n",
       "  0.48812001943588257,\n",
       "  0.5033313632011414,\n",
       "  0.5268041491508484,\n",
       "  0.5069385766983032,\n",
       "  0.4972059428691864,\n",
       "  0.4763355255126953,\n",
       "  0.48228463530540466,\n",
       "  0.5036390423774719,\n",
       "  0.49526485800743103,\n",
       "  0.49969691038131714,\n",
       "  0.503430187702179,\n",
       "  0.5028548836708069,\n",
       "  0.4920308291912079,\n",
       "  0.4890495240688324,\n",
       "  0.4935804605484009,\n",
       "  0.4655812382698059,\n",
       "  0.535988450050354,\n",
       "  0.5042820572853088,\n",
       "  0.5257773399353027,\n",
       "  0.5026942491531372,\n",
       "  0.5229277610778809,\n",
       "  0.5024526715278625,\n",
       "  0.5149148106575012,\n",
       "  0.47963955998420715,\n",
       "  0.4757862389087677,\n",
       "  0.4965163469314575,\n",
       "  0.5013585686683655,\n",
       "  0.5087013244628906,\n",
       "  0.4918877184391022,\n",
       "  0.47260594367980957,\n",
       "  0.48532983660697937,\n",
       "  0.49410638213157654,\n",
       "  0.4880577623844147,\n",
       "  0.49193283915519714,\n",
       "  0.49383318424224854,\n",
       "  0.533664345741272,\n",
       "  0.5225687623023987,\n",
       "  0.49968692660331726,\n",
       "  0.525625467300415,\n",
       "  0.49244052171707153,\n",
       "  0.49394047260284424,\n",
       "  0.4858812689781189,\n",
       "  0.49416959285736084,\n",
       "  0.4913635551929474,\n",
       "  0.5126683712005615,\n",
       "  0.5228525996208191,\n",
       "  0.499590128660202,\n",
       "  0.480800598859787,\n",
       "  0.5320424437522888,\n",
       "  0.5071738958358765,\n",
       "  0.4990217685699463,\n",
       "  0.5195015668869019,\n",
       "  0.4934389889240265,\n",
       "  0.5280536413192749,\n",
       "  0.5137731432914734,\n",
       "  0.5060678720474243,\n",
       "  0.4968433678150177,\n",
       "  0.5007333159446716,\n",
       "  0.4918670058250427,\n",
       "  0.49220970273017883,\n",
       "  0.49581438302993774,\n",
       "  0.4852880537509918,\n",
       "  0.46357589960098267,\n",
       "  0.5015238523483276,\n",
       "  0.5042322278022766,\n",
       "  0.5100773572921753,\n",
       "  0.5233719944953918,\n",
       "  0.5074933171272278,\n",
       "  0.5153239369392395,\n",
       "  0.489271342754364,\n",
       "  0.4833548963069916,\n",
       "  0.49553337693214417,\n",
       "  0.50724196434021,\n",
       "  0.5079983472824097,\n",
       "  0.49844488501548767,\n",
       "  0.4645003080368042,\n",
       "  0.5121008157730103,\n",
       "  0.5288023948669434,\n",
       "  0.5005297660827637,\n",
       "  0.4910110831260681,\n",
       "  0.5060988068580627,\n",
       "  0.5038303732872009,\n",
       "  0.49202054738998413,\n",
       "  0.5063251852989197,\n",
       "  0.5071163773536682,\n",
       "  0.4933673143386841,\n",
       "  0.5024245977401733,\n",
       "  0.493267297744751,\n",
       "  0.5229126811027527,\n",
       "  0.48765069246292114,\n",
       "  0.5101312398910522,\n",
       "  0.4886110723018646,\n",
       "  0.5263186693191528,\n",
       "  0.5018057823181152,\n",
       "  0.5137027502059937,\n",
       "  0.5151404738426208,\n",
       "  0.47249627113342285,\n",
       "  0.5223034620285034,\n",
       "  0.491660475730896,\n",
       "  0.5309017896652222,\n",
       "  0.4823739230632782,\n",
       "  0.5162985324859619,\n",
       "  0.5147486329078674,\n",
       "  0.48661139607429504,\n",
       "  0.4934875965118408,\n",
       "  0.5185744166374207,\n",
       "  0.49348148703575134,\n",
       "  0.48680606484413147,\n",
       "  0.5041224956512451,\n",
       "  0.4992940127849579,\n",
       "  0.5027166604995728,\n",
       "  0.4941719174385071,\n",
       "  0.5034539699554443,\n",
       "  0.4929571747779846,\n",
       "  0.47994184494018555,\n",
       "  0.5019242167472839,\n",
       "  0.5064136981964111,\n",
       "  0.5025798082351685,\n",
       "  0.48534253239631653,\n",
       "  0.5014483332633972,\n",
       "  0.4921215772628784,\n",
       "  0.5005684494972229,\n",
       "  0.5153322815895081,\n",
       "  0.5041378736495972,\n",
       "  0.5047436356544495,\n",
       "  0.5195755958557129,\n",
       "  0.5002426505088806,\n",
       "  0.5208621025085449,\n",
       "  0.5063173174858093,\n",
       "  0.46267884969711304,\n",
       "  0.4892481565475464,\n",
       "  0.5112070441246033,\n",
       "  0.47160235047340393,\n",
       "  0.5120375156402588,\n",
       "  ...]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, preds = trained_model(encodings[\"input_ids\"], encodings[\"attention_mask\"])\n",
    "preds = preds.flatten().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for idx, label in enumerate(label_cols):\n",
    "    if preds[idx] > 0.05:\n",
    "        predictions.append((label, round(preds[idx]*100, 2)))\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RameauLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, tokenizer: tokenizer, max_token_len: int = 128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        descr = \" \".join(data_row.descr.split())\n",
    "        labels = data_row[label_cols]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            descr,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            \"labels\": torch.FloatTensor(labels),\n",
    "            \"descr\": descr\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all datasets\n",
    "train_dataset = RameauLabelDataset(\n",
    "  data_train,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_LEN\n",
    ")\n",
    "val_dataset = RameauLabelDataset(\n",
    "  data_val,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_LEN\n",
    ")\n",
    "test_dataset = RameauLabelDataset(\n",
    "  df_test,\n",
    "  tokenizer,\n",
    "  max_token_len=MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility to load dataset in batch\n",
    "from torch.utils.data import DataLoader\n",
    "#Importing camembdert and AdamW Optimizer\n",
    "from transformers import CamembertForSequenceClassification, AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model declaration\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = CamembertForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels, lr, weight_decay, from_scratch=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if from_scratch:\n",
    "            # Si `from_scratch` est vrai, on charge uniquement la config (nombre de couches, hidden size, etc.) et pas les poids du modèle \n",
    "            config = AutoConfig.from_pretrained(\n",
    "                model_name, num_labels=num_labels\n",
    "            )\n",
    "            self.model = AutoModelForSequenceClassification.from_config(config)\n",
    "        else:\n",
    "            # Cette méthode permet de télécharger le bon modèle pré-entraîné directement depuis le Hub de HuggingFace sur lequel sont stockés de nombreux modèles\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name, num_labels=num_labels\n",
    "            )\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_labels = self.model.num_labels\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        logits = out.logits\n",
    "        # -------- MASKED --------\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits.view(-1, self.num_labels), batch[\"labels\"].view(-1))\n",
    "\n",
    "        # ------ END MASKED ------\n",
    "\n",
    "        self.log(\"train/loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        labels = batch[\"labels\"]\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        preds = torch.max(out.logits, -1).indices\n",
    "        # -------- MASKED --------\n",
    "        acc = (batch[\"labels\"] == preds).float().mean()\n",
    "        # ------ END MASKED ------\n",
    "        self.log(\"valid/acc\", acc)\n",
    "\n",
    "        f1 = f1_score(batch[\"labels\"].cpu().tolist(), preds.cpu().tolist(), average=\"macro\")\n",
    "        self.log(\"valid/f1\", f1)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \"\"\"La fonction predict step facilite la prédiction de données. Elle est \n",
    "        similaire à `validation_step`, sans le calcul des métriques.\n",
    "        \"\"\"\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        return torch.max(out.logits, -1).indices\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = LightningModel(model_name, n_labels, lr=3e-5, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"valid/acc\", mode=\"max\")\n",
    "\n",
    "camembert_trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"gpu\",\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor=\"valid/acc\", patience=4, mode=\"max\"),\n",
    "        model_checkpoint,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camembert_trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = LightningModel.load_from_checkpoint(checkpoint_path=model_checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_TO_LABEL = dataset[\"train\"].features[\"Label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, tokenizer, sentence):\n",
    "    tokenized_sentence = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    input_ids, attention_mask = tokenized_sentence.input_ids, tokenized_sentence.attention_mask\n",
    "\n",
    "    out = model(\n",
    "        input_ids=tokenized_sentence.input_ids,\n",
    "        attention_mask=tokenized_sentence.attention_mask\n",
    "    )\n",
    "\n",
    "    logits = out.logits\n",
    "\n",
    "    probas = torch.softmax(logits, -1).squeeze()\n",
    "\n",
    "    pred = torch.argmax(probas)\n",
    "\n",
    "    return ID_TO_LABEL[pred], probas[pred].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Bonjour, vous allez bien ?\"\n",
    "\n",
    "label_predicted, proba = get_preds(lightning_model.model, tokenizer, test_sentence)\n",
    "\n",
    "print(f\"Label: {label_predicted}, confidence: {proba:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def compute_metrics(p):\n",
    "    print(type(p))\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "from apex import amp\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8\n",
    "\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm boto3 requests regex sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "VALID_BATCH_SIZE = 16\n",
    "TEST_BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-05\n",
    "THRESHOLD = 0.2 # threshold for the sigmoid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "bertclassifier, li = BertForSequenceClassification.from_pretrained(BERT_MODEL_NAME, num_labels=n_labels,output_loading_info=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bertclassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name_param in enumerate(bertclassifier.named_parameters()):\n",
    "    print(i, name_param[0])\n",
    "    # we freeze first 21 layers (first encoder)\n",
    "    if i <= 20:\n",
    "        name_param[1].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(train_dataset[0], add_special_tokens=True, max_length = 512, truncation = True))\n",
    "bertclassifier.eval()\n",
    "bertclassifier(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the frozen layers \n",
    "print(list(bertclassifier.parameters())[20].requires_grad)\n",
    "\n",
    "# trainable layer\n",
    "print(list(bertclassifier.parameters())[-100].requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Module\n",
    "class RameauLabelDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_df, val_df, test_df, tokenizer, batch_size=8, max_token_len=128):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = RameauLabelDataset(\n",
    "            self.train_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "            \n",
    "        self.val_dataset = RameauLabelDataset(\n",
    "            self.val_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        self.test_dataset = RameauLabelDataset(\n",
    "            self.test_df,\n",
    "            self.tokenizer,\n",
    "            self.max_token_len\n",
    "        )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=4\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=1,\n",
    "            num_workers=4\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=1,\n",
    "            num_workers=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and set up the data_module\n",
    "data_module = RameauLabelDataModule(\n",
    "    data_train, \n",
    "    data_val, \n",
    "    df_test, \n",
    "    tokenizer,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len = MAX_LEN)\n",
    "\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0][0].shape)\n",
    "print(train_dataset[0][1].shape)\n",
    "print(train_dataset[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at few examples\n",
    "\n",
    "for i_batch, sample_batch in enumerate(train_dataset):\n",
    "    print(i_batch)\n",
    "    X, mask_X, y = sample_batch\n",
    "    print(X.shape)\n",
    "    print(mask_X.shape)\n",
    "    print(y.shape)\n",
    "    if i_batch == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # cuda for gpu acceleration\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = transformers.AdamW(bertclassifier.parameters(), lr = 1e-5)\n",
    "\n",
    "\n",
    "bertclassifier.to(device) # taking the model to gpu if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training epochs\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "train_metrics = {'acc': [], 'f1': []}\n",
    "test_metrics = {'acc': [], 'f1': []}\n",
    "\n",
    "# progress bar\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "for e in tqdm_notebook(range(epochs)):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    train_f1 = 0.0\n",
    "    batch_cnt = 0\n",
    "    \n",
    "    bertclassifier.train()\n",
    "    \n",
    "    print(f'epoch: {e+1}')\n",
    "    \n",
    "    for i_batch, (X, X_mask, y) in tqdm_notebook(enumerate(bbc_dataloader_train)):\n",
    "        X = X.to(device)\n",
    "        X_mask = X_mask.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, y_pred = bertclassifier(X, attention_mask = X_mask, labels = y)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(bertclassifier.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        y_pred = torch.argmax(y_pred, dim = -1)\n",
    "        \n",
    "        # update metrics\n",
    "        train_acc += accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n",
    "        train_f1 += f1_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average = 'weighted')\n",
    "        batch_cnt += 1\n",
    "    \n",
    "    print(f'train loss: {train_loss/batch_cnt}')\n",
    "    print(f'acc: {train_acc/batch_cnt}')\n",
    "    print(f'f1: {train_f1/batch_cnt}')\n",
    "    train_losses.append(train_loss/batch_cnt)\n",
    "    train_metrics['acc'].append(train_acc/batch_cnt)\n",
    "    train_metrics['f1'].append(train_f1/batch_cnt)\n",
    "        \n",
    "        \n",
    "    test_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    test_f1 = 0.0\n",
    "    batch_cnt = 0\n",
    "    \n",
    "    bertclassifier.eval()\n",
    "    with torch.no_grad():\n",
    "        for i_batch, (X, X_mask, y) in enumerate(bbc_dataloader_train):\n",
    "            X = X.to(device)\n",
    "            X_mask = X_mask.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = bertclassifier(X, attention_mask = X_mask, labels = y)[1] # in eval model we get the softmax output so, don't need to index\n",
    "\n",
    "            \n",
    "            y_pred = torch.argmax(y_pred, dim = -1)\n",
    "\n",
    "            # update metrics\n",
    "            test_acc += accuracy_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy())\n",
    "            test_f1 += f1_score(y.cpu().detach().numpy(), y_pred.cpu().detach().numpy(), average = 'weighted')\n",
    "            batch_cnt += 1\n",
    "            \n",
    "    test_metrics['acc'].append(test_acc/batch_cnt)\n",
    "    test_metrics['f1'].append(test_f1/batch_cnt)\n",
    "    print(f'test acc: {test_acc/batch_cnt}')\n",
    "    print(f'test f1: {test_f1/batch_cnt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the result\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_metrics['acc'], 'kx')\n",
    "plt.plot(train_metrics['f1'], 'r')\n",
    "plt.legend(['train acc', 'train f1'])\n",
    "plt.show()\n",
    "plt.plot(test_metrics['acc'], 'bx')\n",
    "plt.plot(test_metrics['f1'], 'y')\n",
    "plt.legend(['test acc', 'test f1'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torchmetrics import AUROC\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class RameauLabelClassifier(pl.LightningModule):\n",
    "  \n",
    "  # Set up the classifier\n",
    "  def __init__(self, config:dict):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "    self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n",
    "    self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "    self.classifier = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels'])\n",
    "    torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
    "    self.loss_func = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    self.dropout = nn.Dropout()\n",
    "    \n",
    "  def forward(self, input_ids, attention_mask, labels=None):\n",
    "    # model layer\n",
    "    output = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = torch.mean(output.last_hidden_state, 1)\n",
    "    # final logits\n",
    "    pooled_output = self.dropout(pooled_output)\n",
    "    pooled_output = self.hidden(pooled_output)\n",
    "    pooled_output = F.relu(pooled_output)\n",
    "    pooled_output = self.dropout(pooled_output)\n",
    "    logits = self.classifier(pooled_output)\n",
    "    # calculate loss\n",
    "    loss = 0\n",
    "    if labels is not None:\n",
    "      loss = self.loss_func(logits.view(-1, self.config['n_labels']), labels.view(-1, self.config['n_labels']))\n",
    "    return loss, logits\n",
    "\n",
    "  def training_step(self, batch, batch_index):\n",
    "    loss, outputs = self(**batch)\n",
    "    self.log(\"train loss \", loss, prog_bar = True, logger=True)\n",
    "    return {\"loss\":loss, \"predictions\":outputs, \"labels\": batch[\"labels\"]}\n",
    "\n",
    "  def validation_step(self, batch, batch_index):\n",
    "    loss, outputs = self(**batch)\n",
    "    self.log(\"validation loss \", loss, prog_bar = True, logger=True)\n",
    "    return {\"val_loss\": loss, \"predictions\":outputs, \"labels\": batch[\"labels\"]}\n",
    "\n",
    "  def predict_step(self, batch, batch_index):\n",
    "    loss, outputs = self(**batch)\n",
    "    return outputs\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'])\n",
    "    total_steps = self.config['train_size']/self.config['batch_size']\n",
    "    warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
    "    warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    return [optimizer],[scheduler]\n",
    "\n",
    "  def validation_epoch_end(self, outputs):\n",
    "    losses = []\n",
    "    for output in outputs:\n",
    "       loss = output['val_loss'].detach().cpu()\n",
    "       losses.append(loss)\n",
    "    avg_loss = torch.mean(torch.stack(losses))\n",
    "    self.log(\"avg_val_loss\", avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model_name': 'bert-base-multilingual-uncased',\n",
    "    'n_labels': len(label_cols),\n",
    "    'batch_size': 16,\n",
    "    'lr': 2.5e-6,\n",
    "    'warmup': 0.2, \n",
    "    'train_size': len(RameauLabelDataModule.train_dataloader()),\n",
    "    'weight_decay': 0.001,\n",
    "    'n_epochs': 4\n",
    "}\n",
    "\n",
    "model = RameauLabelClassifier(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data module\n",
    "data_module = data_module = RameauLabelDataModule(\n",
    "    data_train, \n",
    "    data_val, \n",
    "    df_test, \n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=config['batch_size'])\n",
    "data_module.setup()\n",
    "\n",
    "# model\n",
    "model = RameauLabelClassifier(config)\n",
    "\n",
    "# trainer and fit\n",
    "trainer = pl.Trainer(max_epochs=config[\"n_epochs\"], accelerator=\"auto\", enable_progress_bar=True)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./lightning_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to convert list of comments into predictions for each comment\n",
    "def classify_raw_comments(model, dm):\n",
    "  predictions = trainer.predict(model, datamodule=dm)\n",
    "  flattened_predictions = np.stack([torch.sigmoid(torch.Tensor(p)) for batch in predictions for p in batch])\n",
    "  return flattened_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, label in enumerate(labels):\n",
    "  fpr, tpr, _ = metrics.roc_curve(\n",
    "      true_labels[:,i].astype(int), predictions[:, i])\n",
    "  auc = metrics.roc_auc_score(\n",
    "      true_labels[:,i].astype(int), predictions[:, i])\n",
    "  plt.plot(fpr, tpr, label='%s %g' % (attribute, auc))\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('BERT Trained on Rameau Dataset - AUC ROC')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abes_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "79d7ff32004ac4c5bc1812f118fca289ef6cc0cea24529fb05e42e57e2fccd5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
